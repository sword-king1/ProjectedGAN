Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     1           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.4     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_8.5     18874368    11264    [8, 2048, 8, 8]     float32 
synthesis.feat_8.6     1           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.7     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.8     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    1           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.4    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_16.5    4718592     5632     [8, 1024, 16, 16]   float32 
synthesis.feat_16.6    1           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.7    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.8    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.4    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_32.5    1179648     2816     [8, 512, 32, 32]    float32 
synthesis.feat_32.6    1           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.7    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.8    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1           -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.4    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.5    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.6    1           -        [8, 512, 64, 64]    float32 
synthesis.feat_64.7    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.8    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   1           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.4   -           -        [8, 128, 128, 128]  float32 
synthesis.feat_128.5   294912      1408     [8, 256, 128, 128]  float32 
synthesis.feat_128.6   1           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.7   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.8   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   1           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.4   -           -        [8, 64, 256, 256]   float32 
synthesis.feat_256.5   73728       704      [8, 128, 256, 256]  float32 
synthesis.feat_256.6   1           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.7   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.8   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  105684175   134595   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[228, 229, 229,  ..., 209, 209, 210],
          [228, 229, 229,  ..., 207, 208, 209],
          [228, 229, 229,  ..., 205, 207, 208],
          ...,
          [110, 109, 101,  ..., 101, 150, 126],
          [ 85, 121,  99,  ..., 116, 125, 106],
          [ 99, 107, 121,  ..., 129,  99,  92]],

         [[219, 220, 220,  ..., 200, 200, 201],
          [219, 220, 220,  ..., 198, 199, 200],
          [219, 220, 220,  ..., 196, 198, 199],
          ...,
          [ 90,  84,  71,  ...,  81, 123,  99],
          [ 68,  96,  75,  ...,  95, 102,  85],
          [ 87,  82,  85,  ..., 108,  76,  72]],

         [[214, 215, 215,  ..., 191, 191, 192],
          [214, 215, 215,  ..., 189, 190, 191],
          [214, 215, 215,  ..., 187, 189, 190],
          ...,
          [ 33,  22,  16,  ...,  10,  55,  40],
          [  9,  24,   3,  ...,  28,  35,  26],
          [ 22,   6,  28,  ...,  45,  11,  15]]],


        [[[130, 255, 250,  ..., 251, 255, 134],
          [130, 255, 250,  ..., 251, 255, 134],
          [130, 255, 250,  ..., 251, 255, 134],
          ...,
          [135, 255, 251,  ..., 251, 255, 134],
          [135, 255, 251,  ..., 251, 255, 134],
          [136, 255, 251,  ..., 251, 255, 134]],

         [[137, 255, 248,  ..., 251, 255, 134],
          [137, 255, 248,  ..., 251, 255, 134],
          [137, 255, 248,  ..., 251, 255, 134],
          ...,
          [132, 255, 251,  ..., 251, 255, 134],
          [132, 255, 251,  ..., 251, 255, 134],
          [132, 255, 251,  ..., 251, 255, 134]],

         [[136, 255, 251,  ..., 249, 255, 134],
          [136, 255, 251,  ..., 249, 255, 134],
          [136, 255, 251,  ..., 249, 255, 134],
          ...,
          [139, 255, 247,  ..., 251, 255, 134],
          [139, 255, 247,  ..., 251, 255, 134],
          [139, 255, 247,  ..., 251, 255, 134]]],


        [[[ 82,  82,  82,  ..., 110, 112, 114],
          [ 84,  84,  84,  ..., 108, 111, 113],
          [ 86,  85,  85,  ..., 110, 111, 113],
          ...,
          [  8,  10,   9,  ...,  19,  17,  17],
          [ 10,   9,  10,  ...,  19,  19,  19],
          [ 13,   9,   6,  ...,  16,  19,  18]],

         [[110, 110, 110,  ..., 135, 135, 137],
          [112, 112, 112,  ..., 134, 133, 136],
          [114, 113, 113,  ..., 135, 134, 136],
          ...,
          [ 10,  12,  10,  ...,  27,  25,  25],
          [ 12,  12,  11,  ...,  28,  26,  27],
          [ 16,  12,   8,  ...,  27,  28,  28]],

         [[131, 131, 131,  ..., 153, 151, 153],
          [133, 133, 133,  ..., 152, 149, 152],
          [135, 134, 134,  ..., 153, 150, 152],
          ...,
          [  7,   9,   7,  ...,   1,   1,   1],
          [  9,   8,   9,  ...,   1,   3,   3],
          [  7,   5,   5,  ...,   0,   4,   3]]],


        ...,


        [[[ 96,  97,  98,  ...,  72,  71,  73],
          [ 97,  97,  98,  ...,  72,  72,  72],
          [ 97,  97,  97,  ...,  73,  72,  71],
          ...,
          [104, 100,  94,  ..., 142, 149, 115],
          [ 84, 117,  99,  ..., 116, 119, 124],
          [ 64,  99,  98,  ...,  82, 107, 134]],

         [[115, 116, 117,  ...,  99,  99,  99],
          [116, 116, 117,  ...,  99,  99,  98],
          [116, 116, 116,  ..., 100,  98,  99],
          ...,
          [ 58,  45,  50,  ..., 105, 110,  80],
          [ 44,  67,  49,  ...,  90,  85,  90],
          [ 31,  62,  48,  ...,  58,  78,  98]],

         [[155, 156, 157,  ..., 142, 142, 142],
          [156, 156, 157,  ..., 142, 142, 142],
          [156, 156, 156,  ..., 143, 141, 141],
          ...,
          [ 17,   7,   9,  ...,  43,  35,   5],
          [  6,  20,   8,  ...,  23,  22,  33],
          [  6,  17,   7,  ...,  18,  24,  57]]],


        [[[113, 113, 113,  ..., 131, 132, 133],
          [115, 115, 115,  ..., 133, 134, 135],
          [117, 117, 117,  ..., 135, 135, 137],
          ...,
          [163, 153, 139,  ..., 178, 183, 183],
          [190, 170, 163,  ..., 189, 166, 163],
          [182, 183, 188,  ..., 148, 150, 153]],

         [[158, 158, 158,  ..., 167, 168, 169],
          [160, 160, 160,  ..., 169, 170, 171],
          [162, 162, 162,  ..., 171, 172, 173],
          ...,
          [127, 117, 103,  ..., 141, 146, 147],
          [153, 134, 127,  ..., 148, 127, 124],
          [145, 148, 153,  ..., 109, 110, 112]],

         [[215, 215, 215,  ..., 219, 220, 221],
          [217, 217, 217,  ..., 220, 221, 222],
          [219, 219, 219,  ..., 221, 221, 223],
          ...,
          [ 90,  82,  68,  ..., 101, 108, 110],
          [116,  98,  91,  ..., 109,  90,  88],
          [108, 110, 114,  ...,  70,  73,  78]]],


        [[[232, 215, 228,  ...,  15,  17,  18],
          [249, 249, 247,  ...,  10,  16,  17],
          [253, 253, 253,  ...,   9,  13,  15],
          ...,
          [ 51,  33,  19,  ...,  39,  40,  42],
          [ 50,  26,  42,  ...,  36,  32,  31],
          [ 24,  53,  35,  ...,  30,  12,  13]],

         [[234, 218, 231,  ...,  79,  78,  75],
          [249, 250, 248,  ...,  77,  77,  75],
          [253, 253, 253,  ...,  76,  76,  76],
          ...,
          [ 53,  35,  21,  ...,  49,  46,  47],
          [ 53,  28,  44,  ...,  49,  44,  44],
          [ 26,  55,  37,  ...,  46,  31,  32]],

         [[237, 221, 233,  ..., 132, 132, 129],
          [251, 252, 249,  ..., 128, 131, 129],
          [253, 253, 253,  ..., 128, 130, 129],
          ...,
          [ 42,  22,   9,  ...,  30,  27,  27],
          [ 40,  15,  31,  ...,  34,  28,  26],
          [ 13,  42,  24,  ...,  35,  18,  17]]]], dtype=torch.uint8)
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 417, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 87, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
