Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 417, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 87, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 417, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 87, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
phase in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 417, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 87, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
phase.module.requires_grad_(False)
# Update weights.
# Accumulate gradients.
phase in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 418, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 87, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
#phase.module.requires_grad_(False)
# Update weights.
# Accumulate gradients.
phase in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 418, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 87, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
#loss:accumulate_gradients in Gboth
#loss:accumulate_gradients in Gboth
#loss:accumulate_gradients in Gboth
#loss:accumulate_gradients in Gboth
#loss:accumulate_gradients in Gboth
#loss:accumulate_gradients in Gboth
#loss:accumulate_gradients in Gboth
#phase.module.requires_grad_(False)
# Update weights.
# Accumulate gradients.
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 418, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 89, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Gboth
#phase.module.requires_grad_(False)
# Update weights.
# Accumulate gradients.
time : loss.accumulate_gradients 0
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 420, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 89, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.
time : loss.accumulate_gradients 1
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
loss:endGmain
time : loss.accumulate_gradients 2
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 3
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 4
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 5
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 6
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 7
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 8
#loss:accumulate_gradients in Gboth
loss:endGmain
#phase.module.requires_grad_(False)
# Update weights.
# Accumulate gradients.
time : loss.accumulate_gradients 1
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 420, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 90, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.
time : loss.accumulate_gradients 1
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
loss:endGmain
time : loss.accumulate_gradients 2
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 3
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 4
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 5
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 6
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 7
#loss:accumulate_gradients in Gboth
loss:endGmain
time : loss.accumulate_gradients 8
#loss:accumulate_gradients in Gboth
loss:endGmain
#phase.module.requires_grad_(False)
# Update weights.
#Gstep()
# Accumulate gradients.
time : loss.accumulate_gradients 1
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 420, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 90, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 420, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 90, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 427, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 90, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 207, in training_loop
    opt_0 = dnnlib.util.construct_class_by_name(params=module.discriminator.mini_discs["0"].parameters(),**opt_kwargs)  # subclass of torch.optim.Optimizer
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1265, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'Generator' object has no attribute 'discriminator'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 207, in training_loop
    opt_0 = dnnlib.util.construct_class_by_name(params=module.discriminator.mini_discs["0"].parameters(),**opt_kwargs)  # subclass of torch.optim.Optimizer
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1265, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'Generator' object has no attribute 'discriminator'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 207, in training_loop
    opt_0 = dnnlib.util.construct_class_by_name(params=D.discriminator.mini_discs["0"].parameters())  # subclass of torch.optim.Optimizer
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/dnnlib/util.py", line 303, in construct_class_by_name
    return call_func_by_name(*args, func_name=class_name, **kwargs)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/dnnlib/util.py", line 295, in call_func_by_name
    assert func_name is not None
AssertionError
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 440, in training_loop
    D.module.feature_network.requires_grad_(False)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1265, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'ProjectedDiscriminator' object has no attribute 'module'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 90, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1227, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1227, device='cuda:0', grad_fn=<MeanBackward0>)
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:143: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1231, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1231, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 72, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:145: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 73, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:145: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 72, in forbid_accumulation_hook
    print("p,p.grad_sample==", p, p.grad_sample)
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:146: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 71, in forbid_accumulation_hook
    print("p,p.grad_sample==", p, p.grad_sample)
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:146: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-6.7629e-01, -4.0627e-01, -7.8531e-01,  3.3895e-01, -1.4127e+00,
         -3.5403e-01,  5.7704e-01,  5.6012e-01, -2.2621e-01, -4.1798e-01,
          6.4434e-01,  2.0544e-01, -9.1146e-01, -7.5527e-01, -6.1189e-01,
         -5.1799e-01, -9.2543e-01, -5.3375e-01, -1.9263e+00, -5.8883e-01,
         -4.8600e-01, -1.4632e+00, -1.1792e+00, -1.6270e+00, -3.0942e-02,
         -2.9638e-01,  2.5526e-01, -6.8390e-02, -5.3978e-01, -5.2902e-01,
          9.2071e-02,  6.9523e-01,  4.8733e-02, -3.3712e-01, -4.3887e-01,
         -3.4958e-01,  3.2872e-01,  1.0521e-01,  1.0214e-01, -3.3139e-01,
         -1.2593e+00,  2.1136e-01, -1.0376e+00, -8.5037e-01,  7.1457e-01,
          1.5599e-01, -5.0370e-01, -2.3636e-02, -6.5791e-02, -1.0312e+00,
         -2.4161e-01,  1.5383e-01,  1.4885e-01,  4.2441e-02,  8.2213e-01,
          8.5723e-01,  1.4101e-01,  8.8083e-02,  4.1061e-01,  5.3332e-01,
          3.3519e-01, -3.0344e-01,  3.6018e-01, -4.8814e-01, -5.7127e-02,
         -1.0512e-01, -5.1564e-01, -1.7798e-01,  2.2688e-02,  7.1509e-01,
          3.8189e-01, -1.2310e+00, -1.4784e+00,  3.0207e-01,  1.4155e+00,
          1.6602e-02,  4.7183e-01,  3.7201e-01,  5.1918e-01,  6.0899e-01,
          5.5749e-02,  7.0681e-01,  3.0680e-01,  3.8355e-01,  6.0704e-01,
          4.9511e-01,  6.4674e-01,  2.1027e-01,  2.5805e-01,  3.7330e-01,
          5.7756e-01,  5.0348e-01,  4.0516e-01,  7.4009e-01,  5.9263e-01,
         -3.8994e-01, -6.8146e-01, -2.8829e-01,  2.2852e-01, -1.9280e-01],
        [-2.3913e-01,  1.8784e-01, -3.4780e-01, -4.7148e-01, -2.4332e-01,
         -5.6237e-01, -1.0981e+00, -5.2099e-01,  1.1218e-01,  6.0430e-01,
         -3.7797e-01, -1.4706e-01, -9.0936e-01, -8.4351e-02, -1.0933e+00,
          1.2784e-01,  6.7201e-01, -3.6674e-02,  5.4304e-02, -5.8825e-02,
         -6.3121e-01, -8.3511e-01, -5.0440e-01, -1.3701e+00, -9.7931e-01,
          6.6572e-02,  1.4510e-01, -9.4125e-01, -2.7755e-01, -1.0244e+00,
          3.7264e-03,  2.8723e-01, -2.8693e-01, -1.2972e+00, -1.8586e+00,
         -1.8805e+00, -9.8598e-01, -5.5446e-01, -1.2197e+00, -1.7505e+00,
         -4.6161e-01, -1.2678e-01, -8.9318e-01, -1.6280e+00, -1.9276e+00,
         -3.5124e-01,  5.4130e-01, -7.8365e-02, -3.9908e-01, -7.9744e-01,
          7.1245e-01,  6.1014e-02,  4.7648e-01,  1.3900e+00,  7.8050e-01,
          1.1257e+00,  6.1271e-01,  7.5728e-01,  4.4527e-01,  5.3162e-01,
          1.4714e+00,  8.8987e-01,  8.3890e-01,  8.2270e-01,  6.7324e-01,
         -8.2606e-01,  7.5277e-01, -3.1366e-01, -2.4640e-01, -1.2444e-01,
         -1.2945e-01,  3.5515e-01, -1.8300e-01, -5.7057e-01, -5.3250e-01,
          7.4840e-01,  7.7752e-01,  6.9376e-01,  7.3468e-01,  4.1640e-01,
          5.5840e-01,  4.6899e-01,  1.1833e-01,  1.7960e-01, -6.0023e-02,
          1.0489e+00,  1.0111e+00,  3.0347e-01, -7.8031e-02, -4.5198e-01,
          1.2305e+00,  1.0342e+00,  3.3719e-01,  4.8171e-02, -3.2223e-01,
          9.2241e-02,  3.4847e-01, -5.0836e-02, -2.7310e-01, -5.6326e-01],
        [-6.0520e-01, -7.6581e-01, -1.8566e-01, -6.1869e-01,  2.2285e-01,
         -5.7777e-01, -1.4071e+00, -7.7457e-01, -1.3385e+00, -7.6557e-01,
         -2.7672e-01, -3.7347e-01, -1.0680e+00, -1.1448e+00, -5.1495e-01,
         -1.1797e+00, -1.9552e-01, -5.2020e-01, -1.1823e+00, -1.7981e-02,
         -6.0842e-01, -3.9819e-01, -5.4661e-03, -7.4554e-01, -7.9904e-01,
         -4.3822e-01,  1.5354e-01, -8.4353e-02, -5.9068e-01, -1.0446e+00,
          8.1564e-02, -3.8943e-01, -6.4730e-01, -8.9013e-01, -5.1908e-01,
          3.7185e-01, -7.6817e-03, -3.7534e-01, -1.2979e+00,  7.7192e-01,
          5.4285e-01, -3.8361e-01,  1.7904e-01, -5.1716e-01, -6.3301e-01,
         -3.8649e-01,  8.1772e-01, -6.6261e-01, -5.6724e-01, -1.1195e+00,
         -4.8471e-02,  4.8014e-01,  3.2281e-01, -2.0362e-01, -8.0626e-01,
          2.7603e-01,  3.0283e-01, -2.2672e-01, -9.1979e-01,  2.8580e-01,
         -2.5239e-01, -2.7121e-01,  4.6543e-01,  2.8272e-01, -5.6185e-01,
          5.6029e-01,  5.2190e-01,  7.5978e-01,  7.2986e-01, -5.7560e-01,
          9.1326e-01,  4.1626e-01,  8.5601e-01,  6.9005e-01,  4.0083e-02,
         -4.6509e-01, -5.9163e-01, -7.9781e-01, -5.5132e-01, -5.6474e-01,
         -8.2951e-01, -9.7964e-01, -7.2636e-01, -4.9387e-01, -4.2327e-01,
         -2.7928e-02, -1.6502e-01, -5.2401e-02,  1.4365e-01, -8.5315e-02,
          4.6453e-01,  3.9301e-01,  5.1418e-01,  5.4665e-01,  5.6325e-02,
          1.9246e-01,  3.4119e-02,  3.2758e-01,  4.0456e-01, -1.2777e-01],
        [ 3.9909e-01, -4.7757e-04, -9.7051e-01, -1.7005e+00, -1.0456e+00,
         -6.6082e-01, -5.8610e-01,  7.1686e-01, -1.4844e-01, -7.1895e-01,
          4.1951e-01, -4.6253e-01, -1.6921e+00, -9.0619e-01, -6.1585e-01,
         -8.1921e-01, -1.7248e-01, -9.9150e-01, -8.2626e-01, -1.2917e+00,
         -9.8420e-01, -6.0330e-01, -1.2169e+00, -6.7952e-01, -1.4487e-01,
         -1.9427e-01,  1.8703e-01,  1.7014e-01, -1.6225e-01, -6.9198e-01,
         -1.3844e+00, -1.7470e+00, -1.0916e+00,  4.8032e-02, -9.1091e-01,
          1.2862e-01, -9.5527e-01, -9.8609e-01, -6.1018e-01, -1.3216e+00,
         -1.1279e+00, -8.3806e-01, -2.8283e-01,  7.4155e-01,  2.1557e-02,
         -5.6992e-01, -6.1766e-01, -7.8310e-01,  4.5750e-02, -1.6669e-01,
          5.3177e-01,  2.3461e-01,  1.1359e+00,  8.7341e-01,  2.9871e-01,
          4.6814e-01, -1.0866e+00, -4.5649e-01, -4.2970e-01,  1.7685e-01,
          1.2868e+00, -4.4111e-01, -4.2292e-01, -8.5689e-03,  4.5728e-01,
          2.6221e-01, -5.2368e-01,  3.0590e-01, -4.6831e-01, -1.7490e-01,
         -5.3202e-01, -3.8981e-01,  1.1450e-01,  1.1315e-01,  6.3588e-01,
          5.7665e-01,  2.9878e-01,  9.9598e-01,  1.1522e+00,  9.1847e-01,
          6.0121e-03, -1.4977e-01,  6.8392e-01,  1.1265e+00,  6.8297e-01,
         -5.6335e-01, -4.9830e-01,  2.2696e-01,  7.6891e-01,  3.3043e-01,
         -5.2827e-01, -4.7643e-01,  2.2581e-01,  4.1159e-01,  1.6191e-01,
         -8.4234e-01, -2.4676e-01,  4.7415e-01,  4.8969e-01,  3.5307e-01],
        [-4.4973e-01, -1.1776e+00, -1.3391e+00, -9.5897e-01, -9.1396e-01,
         -4.0107e-01, -2.6912e-01, -8.9436e-01, -9.0778e-01, -4.9035e-01,
         -6.5728e-01, -6.6919e-01, -6.4301e-01, -1.1234e+00,  1.8756e-01,
         -3.9848e-01, -1.4162e+00, -4.4931e-01, -1.4445e+00,  1.0070e-02,
          2.9023e-01,  7.0735e-01,  2.2648e-01, -3.0936e-01, -3.2578e-01,
         -3.6711e-01,  5.1836e-01,  9.7122e-02,  6.1115e-01,  5.1414e-01,
         -9.2254e-01,  8.2762e-01, -6.9886e-01,  9.4624e-02, -7.1178e-01,
         -7.3803e-01, -2.1232e-01, -1.8536e-01,  3.7114e-02,  1.6182e-01,
         -7.6356e-01, -4.8827e-01, -7.1353e-01,  2.1955e-01, -1.8742e-01,
          2.3926e-01,  2.3496e-01, -1.5084e-01,  5.2522e-03,  3.3928e-01,
          3.9726e-01,  1.2483e+00,  2.3109e-01,  6.3423e-01, -2.2523e-01,
          6.8449e-01,  1.8143e+00,  4.5837e-01,  1.9907e+00,  6.2147e-01,
          4.2755e-01, -1.6142e-01,  1.8581e-01,  2.1414e+00,  5.7491e-01,
          2.5351e-01,  4.6476e-01,  7.2367e-01,  9.8484e-01,  1.4103e+00,
          4.5132e-01,  2.5013e-01,  2.0321e-01,  1.2305e+00,  4.6195e-01,
          7.2350e-01,  6.0921e-01,  7.9352e-01,  6.3421e-01,  6.7564e-01,
          2.1184e-01,  6.2694e-01,  9.1144e-01,  7.3508e-01,  6.8829e-01,
         -1.8787e-01,  5.6437e-01,  6.3283e-01,  5.8727e-01,  2.5100e-01,
         -4.9790e-01,  4.2123e-01,  8.0347e-01,  7.6156e-01,  2.2216e-01,
         -1.3483e-01,  5.6858e-01,  5.3909e-01,  1.1832e-01, -2.1436e-02],
        [-8.3599e-01, -8.0165e-01, -8.1975e-01, -2.1917e-01, -7.9529e-01,
         -7.0788e-01, -5.5284e-01, -3.2106e-01,  6.8419e-01,  1.1911e+00,
         -2.2085e-01,  1.0451e+00, -1.0734e+00, -3.4130e-01,  3.3095e-01,
         -1.3380e+00, -9.4915e-01, -9.1207e-01,  3.2314e-02, -1.4572e+00,
         -1.3922e+00, -1.4229e+00, -1.1367e+00, -9.6353e-01, -7.6166e-01,
         -1.4124e+00, -2.8538e-03, -9.6314e-01, -1.1877e+00, -8.4975e-01,
         -9.4014e-01, -7.1089e-01, -9.4525e-01, -1.7763e+00, -5.9107e-01,
          2.1705e-01, -4.9436e-01, -3.8580e-01, -1.0046e+00, -4.5153e-01,
          2.0857e-01,  1.2313e+00,  3.7748e-01, -1.0836e-01, -1.0154e-02,
         -1.0486e+00,  2.1446e-01, -9.2595e-01, -3.6622e-01, -8.7070e-01,
          4.9114e-01,  4.8765e-01,  4.7797e-01,  3.5892e-01,  1.0538e+00,
          3.7636e-01,  1.5494e-01,  1.9176e-01,  4.5139e-01,  1.0332e+00,
          6.4526e-01,  1.7110e-01,  3.3914e-02, -2.6641e-01,  2.7879e-01,
          1.6658e-01, -3.8589e-01,  5.1193e-01, -1.2419e-01, -5.9999e-01,
         -3.9398e-01,  4.0364e-01, -4.3473e-01,  3.3005e-02,  3.0369e-01,
          6.9066e-01,  1.1519e+00,  1.2476e+00,  1.2954e+00,  1.0676e+00,
         -3.8889e-01, -1.0894e-01,  1.6797e-01,  6.4684e-01,  5.0924e-01,
         -1.3162e+00, -8.6009e-01, -4.3032e-01,  1.3203e-01,  1.0881e-01,
         -1.5251e+00, -9.5169e-01, -3.3559e-01,  3.2152e-02, -2.9012e-01,
         -1.4432e+00, -1.1238e+00, -2.5596e-01,  1.9813e-01,  7.8456e-02],
        [-1.6223e+00, -1.1017e+00, -1.1151e+00, -8.8888e-01, -1.0197e+00,
         -6.1774e-02, -1.8580e-01, -3.6500e-01, -1.2461e-02, -6.2182e-01,
         -1.7994e-01, -8.8278e-01, -2.9547e-01, -1.0998e+00, -8.1011e-01,
         -1.8608e+00,  2.3888e-01, -4.8221e-01,  1.5537e-01,  2.5905e-02,
         -5.4020e-01, -2.0343e-02, -9.7207e-01, -9.3905e-01, -7.4945e-01,
          1.2620e-01, -1.9693e-01, -1.3377e+00, -8.2561e-01,  3.2517e-01,
          3.5146e-01,  1.8708e-01,  2.2238e-01,  1.2801e-01, -5.0508e-01,
         -6.3579e-01,  1.5173e-01,  6.0989e-02,  3.0411e-01, -1.6368e+00,
         -1.6801e+00, -6.5258e-02, -5.5551e-01, -4.4661e-01, -1.9740e+00,
         -2.3239e-01, -1.0370e-01, -1.1748e+00, -8.3159e-01, -1.3943e-01,
         -4.1212e-01,  2.8794e-01,  6.7287e-01,  3.2349e-02,  3.6994e-01,
          4.7863e-01, -2.6328e-01,  3.7504e-01,  4.9257e-01,  1.0508e+00,
          5.2918e-01, -5.5079e-01,  1.4028e-01, -6.2532e-01,  2.2010e-01,
         -1.4736e-01,  9.0616e-01, -1.2148e-01,  2.2444e-01,  3.8017e-01,
          6.0777e-01,  8.3268e-01,  2.8145e-01, -1.9868e-01, -2.1437e-01,
          3.4311e-01,  6.8535e-01,  6.5387e-01,  7.3135e-01,  7.0109e-01,
          5.4454e-01,  2.7098e-01,  2.7322e-01,  6.1909e-01,  7.1250e-01,
          3.1924e-01,  2.2630e-01,  3.6803e-01,  7.3370e-01,  7.1224e-01,
         -5.8012e-02,  8.6326e-02,  2.2224e-01,  5.0881e-01,  3.3477e-01,
         -3.2888e-02,  3.4603e-02, -6.2309e-02, -2.0097e-02,  2.6565e-01],
        [-4.9378e-01, -4.3192e-01, -8.4806e-01, -3.6984e-01, -7.9320e-01,
         -1.0591e+00, -2.6109e-01, -2.0611e-01, -9.8003e-01, -1.0348e+00,
         -1.5335e+00, -8.1668e-01, -4.7126e-01, -7.2730e-01, -1.1106e+00,
         -1.1724e+00, -6.9271e-01, -7.3183e-01, -1.5181e-01,  1.8507e-01,
         -2.8728e-01, -1.3174e-01, -3.9225e-01, -2.4291e-01,  3.6737e-01,
         -5.8126e-01,  2.3015e-01,  6.3973e-01,  2.4371e-01,  6.1200e-02,
         -7.7343e-01,  6.2641e-01,  8.4266e-01,  1.5222e-03, -2.9706e-01,
         -2.1661e-01, -1.4598e+00, -1.5926e+00, -3.7800e-01, -7.3357e-01,
          1.4898e-01, -1.0340e+00, -1.1046e+00, -1.3547e+00, -1.9019e-01,
         -3.1680e-01, -6.2010e-01, -1.0969e+00, -6.4116e-01,  3.6058e-01,
          7.1252e-01,  1.3251e+00,  1.4150e+00,  4.4391e-01,  3.8609e-02,
         -5.1666e-01,  5.1815e-01,  9.7669e-02,  1.3519e-02, -1.3389e-01,
          8.3861e-03, -1.2439e-01,  1.9763e-01, -2.5220e-01,  4.9017e-01,
          2.9054e-02,  9.7755e-01, -7.8705e-01, -5.5552e-01, -3.1549e-01,
         -6.0776e-01, -1.9514e-01, -3.5131e-01, -6.4920e-02,  2.5362e-01,
          2.9163e-01,  6.1558e-01,  6.0759e-01,  4.5213e-01,  7.9426e-01,
         -4.3446e-01, -1.7307e-01, -3.4990e-01, -3.8649e-01, -2.1247e-01,
         -1.1261e+00, -9.6415e-01, -7.6582e-01, -7.7700e-01, -8.7690e-01,
         -6.4142e-01, -1.6547e-01, -2.0783e-01, -3.4889e-01, -8.0873e-01,
         -3.1144e-01,  4.9127e-01,  8.0143e-01,  9.1835e-01,  1.2292e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.6763, 1.4063, 1.7853, 0.6610, 2.4127, 1.3540, 0.4230, 0.4399, 1.2262,
         1.4180, 0.3557, 0.7946, 1.9115, 1.7553, 1.6119, 1.5180, 1.9254, 1.5337,
         2.9263, 1.5888, 1.4860, 2.4632, 2.1792, 2.6270, 1.0309, 1.2964, 0.7447,
         1.0684, 1.5398, 1.5290, 0.9079, 0.3048, 0.9513, 1.3371, 1.4389, 1.3496,
         0.6713, 0.8948, 0.8979, 1.3314, 2.2593, 0.7886, 2.0376, 1.8504, 0.2854,
         0.8440, 1.5037, 1.0236, 1.0658, 2.0312, 1.2416, 0.8462, 0.8511, 0.9576,
         0.1779, 0.1428, 0.8590, 0.9119, 0.5894, 0.4667, 0.6648, 1.3034, 0.6398,
         1.4881, 1.0571, 1.1051, 1.5156, 1.1780, 0.9773, 0.2849, 0.6181, 2.2310,
         2.4784, 0.6979, 0.0000, 0.9834, 0.5282, 0.6280, 0.4808, 0.3910, 0.9443,
         0.2932, 0.6932, 0.6165, 0.3930, 0.5049, 0.3533, 0.7897, 0.7420, 0.6267,
         0.4224, 0.4965, 0.5948, 0.2599, 0.4074, 1.3899, 1.6815, 1.2883, 0.7715,
         1.1928],
        [1.2391, 0.8122, 1.3478, 1.4715, 1.2433, 1.5624, 2.0981, 1.5210, 0.8878,
         0.3957, 1.3780, 1.1471, 1.9094, 1.0844, 2.0933, 0.8722, 0.3280, 1.0367,
         0.9457, 1.0588, 1.6312, 1.8351, 1.5044, 2.3701, 1.9793, 0.9334, 0.8549,
         1.9413, 1.2776, 2.0244, 0.9963, 0.7128, 1.2869, 2.2972, 2.8586, 2.8805,
         1.9860, 1.5545, 2.2197, 2.7505, 1.4616, 1.1268, 1.8932, 2.6280, 2.9276,
         1.3512, 0.4587, 1.0784, 1.3991, 1.7974, 0.2875, 0.9390, 0.5235, 0.0000,
         0.2195, 0.0000, 0.3873, 0.2427, 0.5547, 0.4684, 0.0000, 0.1101, 0.1611,
         0.1773, 0.3268, 1.8261, 0.2472, 1.3137, 1.2464, 1.1244, 1.1295, 0.6449,
         1.1830, 1.5706, 1.5325, 0.2516, 0.2225, 0.3062, 0.2653, 0.5836, 0.4416,
         0.5310, 0.8817, 0.8204, 1.0600, 0.0000, 0.0000, 0.6965, 1.0780, 1.4520,
         0.0000, 0.0000, 0.6628, 0.9518, 1.3222, 0.9078, 0.6515, 1.0508, 1.2731,
         1.5633],
        [1.6052, 1.7658, 1.1857, 1.6187, 0.7772, 1.5778, 2.4071, 1.7746, 2.3385,
         1.7656, 1.2767, 1.3735, 2.0680, 2.1448, 1.5149, 2.1797, 1.1955, 1.5202,
         2.1823, 1.0180, 1.6084, 1.3982, 1.0055, 1.7455, 1.7990, 1.4382, 0.8465,
         1.0844, 1.5907, 2.0446, 0.9184, 1.3894, 1.6473, 1.8901, 1.5191, 0.6282,
         1.0077, 1.3753, 2.2979, 0.2281, 0.4571, 1.3836, 0.8210, 1.5172, 1.6330,
         1.3865, 0.1823, 1.6626, 1.5672, 2.1195, 1.0485, 0.5199, 0.6772, 1.2036,
         1.8063, 0.7240, 0.6972, 1.2267, 1.9198, 0.7142, 1.2524, 1.2712, 0.5346,
         0.7173, 1.5618, 0.4397, 0.4781, 0.2402, 0.2701, 1.5756, 0.0867, 0.5837,
         0.1440, 0.3100, 0.9599, 1.4651, 1.5916, 1.7978, 1.5513, 1.5647, 1.8295,
         1.9796, 1.7264, 1.4939, 1.4233, 1.0279, 1.1650, 1.0524, 0.8564, 1.0853,
         0.5355, 0.6070, 0.4858, 0.4533, 0.9437, 0.8075, 0.9659, 0.6724, 0.5954,
         1.1278],
        [0.6009, 1.0005, 1.9705, 2.7005, 2.0456, 1.6608, 1.5861, 0.2831, 1.1484,
         1.7190, 0.5805, 1.4625, 2.6921, 1.9062, 1.6159, 1.8192, 1.1725, 1.9915,
         1.8263, 2.2917, 1.9842, 1.6033, 2.2169, 1.6795, 1.1449, 1.1943, 0.8130,
         0.8299, 1.1623, 1.6920, 2.3844, 2.7470, 2.0916, 0.9520, 1.9109, 0.8714,
         1.9553, 1.9861, 1.6102, 2.3216, 2.1279, 1.8381, 1.2828, 0.2584, 0.9784,
         1.5699, 1.6177, 1.7831, 0.9542, 1.1667, 0.4682, 0.7654, 0.0000, 0.1266,
         0.7013, 0.5319, 2.0866, 1.4565, 1.4297, 0.8231, 0.0000, 1.4411, 1.4229,
         1.0086, 0.5427, 0.7378, 1.5237, 0.6941, 1.4683, 1.1749, 1.5320, 1.3898,
         0.8855, 0.8868, 0.3641, 0.4233, 0.7012, 0.0040, 0.0000, 0.0815, 0.9940,
         1.1498, 0.3161, 0.0000, 0.3170, 1.5634, 1.4983, 0.7730, 0.2311, 0.6696,
         1.5283, 1.4764, 0.7742, 0.5884, 0.8381, 1.8423, 1.2468, 0.5258, 0.5103,
         0.6469],
        [1.4497, 2.1776, 2.3391, 1.9590, 1.9140, 1.4011, 1.2691, 1.8944, 1.9078,
         1.4904, 1.6573, 1.6692, 1.6430, 2.1234, 0.8124, 1.3985, 2.4162, 1.4493,
         2.4445, 0.9899, 0.7098, 0.2927, 0.7735, 1.3094, 1.3258, 1.3671, 0.4816,
         0.9029, 0.3889, 0.4859, 1.9225, 0.1724, 1.6989, 0.9054, 1.7118, 1.7380,
         1.2123, 1.1854, 0.9629, 0.8382, 1.7636, 1.4883, 1.7135, 0.7804, 1.1874,
         0.7607, 0.7650, 1.1508, 0.9947, 0.6607, 0.6027, 0.0000, 0.7689, 0.3658,
         1.2252, 0.3155, 0.0000, 0.5416, 0.0000, 0.3785, 0.5725, 1.1614, 0.8142,
         0.0000, 0.4251, 0.7465, 0.5352, 0.2763, 0.0152, 0.0000, 0.5487, 0.7499,
         0.7968, 0.0000, 0.5381, 0.2765, 0.3908, 0.2065, 0.3658, 0.3244, 0.7882,
         0.3731, 0.0886, 0.2649, 0.3117, 1.1879, 0.4356, 0.3672, 0.4127, 0.7490,
         1.4979, 0.5788, 0.1965, 0.2384, 0.7778, 1.1348, 0.4314, 0.4609, 0.8817,
         1.0214],
        [1.8360, 1.8017, 1.8198, 1.2192, 1.7953, 1.7079, 1.5528, 1.3211, 0.3158,
         0.0000, 1.2208, 0.0000, 2.0734, 1.3413, 0.6691, 2.3380, 1.9492, 1.9121,
         0.9677, 2.4572, 2.3922, 2.4229, 2.1367, 1.9635, 1.7617, 2.4124, 1.0029,
         1.9631, 2.1877, 1.8498, 1.9401, 1.7109, 1.9452, 2.7763, 1.5911, 0.7829,
         1.4944, 1.3858, 2.0046, 1.4515, 0.7914, 0.0000, 0.6225, 1.1084, 1.0102,
         2.0486, 0.7855, 1.9260, 1.3662, 1.8707, 0.5089, 0.5123, 0.5220, 0.6411,
         0.0000, 0.6236, 0.8451, 0.8082, 0.5486, 0.0000, 0.3547, 0.8289, 0.9661,
         1.2664, 0.7212, 0.8334, 1.3859, 0.4881, 1.1242, 1.6000, 1.3940, 0.5964,
         1.4347, 0.9670, 0.6963, 0.3093, 0.0000, 0.0000, 0.0000, 0.0000, 1.3889,
         1.1089, 0.8320, 0.3532, 0.4908, 2.3162, 1.8601, 1.4303, 0.8680, 0.8912,
         2.5251, 1.9517, 1.3356, 0.9678, 1.2901, 2.4432, 2.1238, 1.2560, 0.8019,
         0.9215],
        [2.6223, 2.1017, 2.1151, 1.8889, 2.0197, 1.0618, 1.1858, 1.3650, 1.0125,
         1.6218, 1.1799, 1.8828, 1.2955, 2.0998, 1.8101, 2.8608, 0.7611, 1.4822,
         0.8446, 0.9741, 1.5402, 1.0203, 1.9721, 1.9390, 1.7494, 0.8738, 1.1969,
         2.3377, 1.8256, 0.6748, 0.6485, 0.8129, 0.7776, 0.8720, 1.5051, 1.6358,
         0.8483, 0.9390, 0.6959, 2.6368, 2.6801, 1.0653, 1.5555, 1.4466, 2.9740,
         1.2324, 1.1037, 2.1748, 1.8316, 1.1394, 1.4121, 0.7121, 0.3271, 0.9677,
         0.6301, 0.5214, 1.2633, 0.6250, 0.5074, 0.0000, 0.4708, 1.5508, 0.8597,
         1.6253, 0.7799, 1.1474, 0.0938, 1.1215, 0.7756, 0.6198, 0.3922, 0.1673,
         0.7185, 1.1987, 1.2144, 0.6569, 0.3146, 0.3461, 0.2687, 0.2989, 0.4555,
         0.7290, 0.7268, 0.3809, 0.2875, 0.6808, 0.7737, 0.6320, 0.2663, 0.2878,
         1.0580, 0.9137, 0.7778, 0.4912, 0.6652, 1.0329, 0.9654, 1.0623, 1.0201,
         0.7343],
        [1.4938, 1.4319, 1.8481, 1.3698, 1.7932, 2.0591, 1.2611, 1.2061, 1.9800,
         2.0348, 2.5335, 1.8167, 1.4713, 1.7273, 2.1106, 2.1724, 1.6927, 1.7318,
         1.1518, 0.8149, 1.2873, 1.1317, 1.3923, 1.2429, 0.6326, 1.5813, 0.7699,
         0.3603, 0.7563, 0.9388, 1.7734, 0.3736, 0.1573, 0.9985, 1.2971, 1.2166,
         2.4598, 2.5926, 1.3780, 1.7336, 0.8510, 2.0340, 2.1046, 2.3547, 1.1902,
         1.3168, 1.6201, 2.0969, 1.6412, 0.6394, 0.2875, 0.0000, 0.0000, 0.5561,
         0.9614, 1.5167, 0.4818, 0.9023, 0.9865, 1.1339, 0.9916, 1.1244, 0.8024,
         1.2522, 0.5098, 0.9709, 0.0225, 1.7870, 1.5555, 1.3155, 1.6078, 1.1951,
         1.3513, 1.0649, 0.7464, 0.7084, 0.3844, 0.3924, 0.5479, 0.2057, 1.4345,
         1.1731, 1.3499, 1.3865, 1.2125, 2.1261, 1.9641, 1.7658, 1.7770, 1.8769,
         1.6414, 1.1655, 1.2078, 1.3489, 1.8087, 1.3114, 0.5087, 0.1986, 0.0817,
         0.8771]], device='cuda:0', grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1438, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1124, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1124, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-1.8912e-01, -2.1873e-01, -2.4613e-01, -7.7000e-01,  2.1502e-01,
         -1.9964e+00, -3.8880e-01,  5.9650e-01, -1.3166e+00, -6.5653e-01,
         -1.2415e+00, -1.9745e+00, -1.0852e+00, -9.9162e-01, -1.9498e-01,
         -1.0078e+00, -1.0876e-02,  6.6768e-01, -5.0314e-01, -2.6139e-01,
         -1.8041e+00, -4.3107e-01, -1.2774e-01, -8.6260e-01, -3.0705e-01,
         -8.9544e-01, -2.2317e-01,  1.9575e-03, -1.2979e+00, -8.7120e-01,
         -8.1913e-01, -1.0685e+00, -7.8496e-01, -7.4750e-01, -4.2535e-01,
         -4.0737e-02, -4.9055e-01, -3.1910e-01,  3.1417e-01,  4.5823e-01,
          8.9796e-01, -1.0549e-01, -3.6190e-01,  9.1472e-01,  5.1182e-02,
         -5.9563e-01,  4.7894e-01, -1.5779e-01,  4.8375e-02, -4.3576e-02,
          8.6232e-01,  1.4281e+00,  5.3387e-01, -1.0105e+00, -1.0730e-01,
          1.0411e+00, -2.4372e-02, -2.4478e-01,  8.7473e-01,  2.5289e-01,
          7.6109e-01,  5.3119e-01, -8.9059e-01, -7.0184e-01,  5.6823e-01,
          1.5450e-01, -5.2425e-01, -5.3024e-01,  1.5869e-01,  1.8202e-01,
         -1.1078e-01, -8.4417e-02,  1.7533e-01, -5.8464e-01,  5.5657e-01,
          6.8698e-01,  1.0513e-01, -5.0348e-02, -3.4759e-01, -2.8338e-01,
          4.1772e-01,  2.7145e-01,  4.1181e-01,  1.3684e-01, -1.2974e-01,
         -3.3750e-02,  1.0282e-01,  5.5292e-01,  4.5223e-01,  1.5140e-01,
         -1.9816e-01, -1.7864e-01, -9.7251e-02,  1.6977e-01,  5.4405e-01,
         -1.6958e-01,  3.2562e-01,  1.8035e-01, -3.2855e-02,  2.6631e-01],
        [-5.0002e-02,  3.4042e-01,  2.3798e-02,  6.7141e-02, -8.9891e-01,
         -9.0072e-01, -9.7082e-02, -5.6282e-01, -8.2728e-01, -3.6447e-01,
         -8.3913e-01, -8.1635e-02, -1.1468e+00, -4.8131e-01, -5.6389e-01,
         -1.0410e+00, -5.3089e-01, -2.1875e-02, -5.1805e-01,  8.8251e-01,
         -3.1026e-01, -9.0421e-01, -3.0584e-01, -2.5585e+00, -5.6249e-01,
          4.9715e-03,  3.0344e-01, -5.9295e-01, -7.6891e-01, -6.8108e-01,
         -4.5379e-01,  1.6631e-01, -3.6982e-01, -1.5973e+00, -9.1169e-01,
         -5.3456e-01, -1.5627e-01,  3.5657e-01, -2.8156e-01,  1.0398e+00,
         -3.4690e-01, -6.3446e-01, -8.2431e-01, -1.2821e+00, -2.1339e+00,
          1.0829e+00, -7.2827e-01, -6.7458e-02, -1.2216e+00, -2.5358e-01,
         -1.8907e-01, -2.0320e-02, -1.1247e-01, -2.9816e-01,  1.8055e-02,
          3.0693e-01, -3.4782e-01, -3.4756e-01,  5.4937e-01,  2.9716e-01,
          3.4391e-01,  8.2505e-01,  2.1883e-01,  7.5171e-01,  9.3845e-01,
         -3.7975e-01, -1.3551e+00, -3.0412e-01, -4.8984e-01, -1.3078e-01,
         -3.4496e-02,  2.9789e-02, -4.3140e-01,  4.5545e-01,  4.3566e-01,
          9.8517e-01,  1.2451e+00,  8.3433e-01,  5.3658e-01,  3.6142e-01,
          6.4342e-01,  5.9368e-01,  1.2871e-01, -2.2766e-01, -4.3869e-01,
         -4.5221e-01, -1.7364e-01, -4.1237e-01, -4.9640e-01, -6.0481e-01,
         -3.8488e-01, -3.0475e-01, -3.9885e-01, -3.3690e-01, -4.0826e-01,
          2.8207e-01,  3.1161e-01,  1.7378e-01, -1.7088e-01, -3.0360e-01],
        [-5.8160e-01, -1.9250e-01, -9.8080e-02, -3.8183e-01, -3.5724e-01,
         -1.1221e+00, -1.8460e-01, -5.0072e-01, -5.0022e-01, -9.0780e-01,
         -9.2907e-01, -1.5376e-02,  6.3464e-02, -4.1488e-02, -3.7068e-01,
         -3.3769e-01, -2.2570e-02, -9.2214e-01, -2.0328e-01, -1.5647e-01,
         -5.7118e-01, -1.5146e+00, -9.7484e-01, -2.0776e+00, -4.8264e-01,
         -1.4803e-01, -1.7915e+00, -8.4641e-02, -4.8702e-01, -7.5923e-01,
         -4.6557e-01, -4.6927e-01, -1.4559e+00, -1.5675e-02, -5.1783e-01,
         -6.5438e-01, -1.2231e+00, -1.2951e+00, -5.7495e-01, -1.2233e+00,
         -1.3765e+00, -7.3932e-01,  7.0288e-01,  5.7671e-01, -4.5417e-01,
         -4.2737e-01, -3.1554e-01, -8.8696e-01, -8.3394e-01, -1.4554e-01,
          1.4371e+00, -3.8939e-01, -1.0642e-01,  1.7479e-01, -7.3359e-02,
         -2.8497e-02,  7.8154e-01,  3.0996e-01,  3.3192e-01, -2.0389e-01,
          6.7298e-01,  1.6705e-01, -1.1342e-01,  7.0081e-01,  1.3455e-01,
          5.0472e-01, -3.7541e-01, -2.2360e-01, -1.7570e-01, -2.9201e-01,
          3.1893e-01, -6.9607e-01, -4.0997e-01,  2.8512e-01, -2.5064e-02,
          6.2969e-01,  1.0154e+00,  5.7997e-01, -3.3270e-01, -2.7087e-01,
          3.7023e-01,  2.7047e-01, -1.2732e-01, -2.7315e-01, -2.1619e-02,
          2.7427e-01, -1.2307e-02, -2.3753e-01, -2.6806e-01, -3.5884e-01,
          4.1788e-02,  1.0649e-02, -1.8260e-01, -3.6850e-01, -5.0553e-01,
         -3.7379e-01, -1.0435e-01, -2.8090e-01, -8.9372e-02,  9.8906e-02],
        [ 6.4751e-01, -3.9704e-01, -1.3054e+00, -1.6632e+00, -9.1151e-01,
          6.2201e-01, -7.7377e-01, -5.2066e-01, -5.9166e-02, -2.1177e-01,
         -2.2147e-01, -6.7030e-02, -8.3364e-01, -1.8135e-01,  8.6393e-02,
         -1.3601e-01, -4.2658e-01,  1.0903e-01, -8.1500e-01,  1.0464e+00,
         -7.3880e-01, -7.4525e-01, -1.7042e+00, -1.2311e+00, -2.5158e-02,
         -2.5710e-01, -1.3639e+00, -5.3598e-01, -2.3101e-01, -9.7422e-02,
         -1.1613e+00, -6.6051e-01, -1.8947e+00, -6.1596e-01, -9.2402e-01,
         -1.5915e+00, -1.8856e+00, -4.4679e-01, -8.4478e-01, -1.2342e-01,
         -6.8744e-02, -4.0286e-01,  6.2982e-01, -3.1442e-01, -5.3254e-01,
          5.8854e-01, -3.8776e-01, -9.4102e-01, -3.3146e-02,  7.1701e-01,
          5.6802e-01,  7.9271e-01,  3.1910e-01,  5.2593e-01,  3.1038e-01,
          1.2362e+00,  2.4748e-01,  4.9204e-01, -4.9834e-01, -2.1500e-01,
         -1.2493e+00, -1.2762e+00, -6.8354e-01,  1.5759e-01, -1.2824e-01,
         -4.8535e-01, -7.7038e-01, -1.3227e-02, -9.9420e-01, -1.7700e+00,
         -7.6923e-01, -6.0978e-01, -1.9514e-01, -4.0287e-01, -3.1044e-02,
          5.9629e-02, -3.2426e-01, -5.4410e-01, -4.1260e-02,  4.8639e-01,
         -1.8172e-01, -1.8296e-01, -1.2813e-01,  1.8750e-01,  1.5393e-01,
         -6.3769e-01, -4.8429e-01, -2.4522e-01,  8.4915e-02, -6.7974e-02,
         -1.9758e-01, -2.0732e-01, -5.1888e-01, -2.7952e-01, -2.0366e-01,
          3.9580e-01,  3.0112e-01, -7.5916e-03,  4.3492e-02, -3.9561e-02],
        [ 4.3767e-02,  9.2693e-02, -7.5240e-04,  3.1726e-01,  5.8087e-01,
         -1.3181e+00, -1.2282e+00,  3.1723e-01, -8.9357e-01,  7.5548e-02,
         -1.6932e+00, -9.1411e-01, -3.6242e-01,  6.5711e-02, -4.1360e-01,
         -4.9406e-01,  1.4996e-01, -3.4955e-01,  1.3503e+00, -2.3354e-01,
         -1.5317e+00, -1.0276e+00, -3.8853e-01, -1.2640e+00, -5.5049e-01,
         -8.0378e-01, -1.0990e+00, -8.3469e-01, -8.1340e-01,  1.4327e-01,
         -1.4024e+00, -1.0857e+00, -1.4574e+00, -8.8436e-01, -9.5809e-01,
         -4.9334e-01, -2.8032e-01,  5.7120e-01,  4.0773e-01, -4.0279e-01,
         -6.0403e-02, -7.0133e-01, -5.4837e-01, -6.2708e-01, -1.0736e+00,
         -2.1281e-01, -2.0699e-01,  5.0281e-01, -9.9250e-02, -3.4178e-01,
          7.1802e-03,  8.5067e-01,  2.9297e-01,  6.6337e-01,  5.9273e-01,
         -9.7072e-01, -6.2455e-01, -3.5123e-01, -6.7795e-01, -4.8754e-01,
          7.7914e-01, -5.3201e-01, -1.1298e+00, -2.7726e-01, -1.0150e+00,
          5.3544e-01, -2.6170e-01, -1.5478e-01, -9.2556e-01, -4.9962e-01,
         -2.3369e-01,  3.1060e-01,  2.2545e-02, -2.8995e-01, -1.0214e+00,
          2.5947e-01, -2.9928e-02, -3.0725e-01, -4.7453e-01, -3.1097e-01,
          9.0531e-02, -8.4755e-02, -3.0950e-01,  2.7500e-02,  4.3578e-01,
          2.0950e-01,  6.2835e-02,  1.7320e-01,  6.4865e-01,  1.2823e+00,
          2.4871e-01, -7.7556e-02,  2.7810e-01,  8.2553e-01,  1.1100e+00,
          7.1304e-02, -6.5509e-02,  1.5968e-01,  3.6649e-01,  6.4932e-01],
        [ 6.7657e-02, -9.6602e-01, -4.5985e-01, -6.4488e-01, -4.9194e-01,
         -1.0676e+00, -4.9697e-01,  3.0292e-01, -2.0384e-01, -2.5400e-01,
         -5.9205e-01, -9.6246e-01, -4.6883e-01, -1.2931e+00, -2.6557e-01,
         -9.9302e-02, -1.3901e+00, -1.1422e+00, -2.5110e-01,  8.4564e-02,
         -1.0193e+00, -6.3887e-01, -2.0077e+00, -1.1064e+00, -5.8743e-01,
         -1.9321e-02,  2.4207e-01, -3.7290e-01, -4.7855e-01,  1.4607e-01,
         -9.6670e-01, -2.0985e-01, -9.0272e-01, -9.9085e-01, -2.3758e-01,
         -1.9763e+00, -2.0688e+00, -1.0898e+00, -9.7516e-01, -1.0882e+00,
         -6.9105e-01, -6.0437e-01, -7.7697e-01, -9.9479e-01, -5.2255e-01,
         -1.1707e-01,  2.5956e-01, -7.9163e-01, -1.3558e+00,  1.8110e-01,
          9.5920e-01,  1.2948e-01,  4.9636e-01,  2.5790e-01,  4.0062e-01,
         -6.8689e-02,  3.3579e-01,  5.2178e-01, -9.9119e-02,  4.9181e-01,
          8.6322e-01,  4.6276e-01, -2.8188e-01,  6.3828e-02,  1.0966e-01,
          7.4942e-01,  1.7176e-01, -1.3498e-01,  7.3286e-01,  1.8955e-01,
         -1.2460e+00, -5.0111e-01, -4.4696e-01, -5.9801e-01, -6.4553e-01,
          6.2393e-01,  7.3887e-01,  7.8448e-01,  6.0962e-01,  2.8501e-01,
          6.9655e-01,  5.8322e-01,  5.7005e-01,  4.2852e-01,  9.2279e-02,
          1.5977e-01,  1.9259e-01,  8.3725e-02,  1.2190e-01,  1.5978e-01,
         -1.4833e-01, -2.5259e-01, -3.4579e-01, -1.9817e-01, -2.0565e-01,
         -2.7575e-01, -2.7983e-01, -6.5657e-01, -9.0587e-01, -4.6923e-01],
        [ 2.3961e-01,  2.5319e-01, -1.3279e+00, -1.3184e-01, -4.9151e-01,
          5.9705e-01, -2.8685e-01, -7.2040e-02, -6.4982e-01, -5.6220e-01,
         -1.8696e+00, -1.4094e+00, -2.2447e-01, -4.4093e-01, -1.4678e-01,
          6.8329e-01,  5.3266e-01, -4.8071e-01, -1.0095e+00, -7.1655e-01,
          2.5969e-02, -4.8638e-01, -9.3589e-01, -4.2960e-01, -2.1522e-01,
          1.1072e+00, -1.9504e-01, -2.4586e-01, -5.8777e-02, -9.1499e-01,
         -6.7347e-02,  9.6832e-02,  3.5743e-01, -6.3980e-01, -2.9119e-01,
          4.8555e-01,  3.3115e-01,  7.5869e-01,  1.9807e-01, -7.2347e-01,
         -8.0705e-01, -8.0892e-01, -9.1379e-01,  4.5194e-01, -1.6810e-01,
         -5.0150e-01, -1.4430e-01, -2.7644e-01, -2.2915e-01,  2.1575e-01,
          5.9358e-02, -9.1324e-01, -4.7431e-01,  1.2238e+00,  6.0433e-01,
          9.2302e-01, -1.8889e-01, -2.9680e-01,  1.0756e+00,  9.1940e-01,
          3.2536e-01,  1.0008e+00,  1.4469e+00,  4.0599e-01,  4.2730e-01,
          1.0805e+00, -3.5371e-01,  5.0051e-01,  7.0944e-01,  5.8012e-01,
          7.7833e-01,  1.1829e+00, -3.1713e-01,  1.2393e+00,  7.0479e-01,
         -6.8370e-01, -8.4275e-01, -5.4923e-01, -1.7546e-01,  1.8714e-01,
         -6.2443e-01, -8.5219e-01, -6.1912e-01, -3.1990e-01,  4.1609e-01,
         -1.1019e-01, -5.3011e-01, -4.9683e-01, -3.5088e-01,  4.1325e-01,
          3.2801e-01, -1.4440e-01, -6.4778e-01, -1.0154e+00, -2.6041e-01,
          2.3240e-03, -3.7692e-01, -8.5243e-01, -1.6619e+00, -1.1138e+00],
        [-1.2818e+00,  9.2784e-01, -6.4452e-02, -5.8772e-01,  4.4425e-02,
         -9.1456e-01,  1.1331e+00, -4.7037e-01, -3.4996e-01, -1.2244e-02,
         -1.5982e+00, -8.7129e-01, -3.5395e-01,  3.9784e-01, -3.4913e-01,
         -5.7199e-01, -4.2347e-01,  2.1880e-01, -4.4571e-01, -1.5266e-01,
         -1.0318e+00, -3.1911e-01, -1.5708e+00,  2.6631e-01, -8.9311e-01,
         -9.9146e-01,  2.7134e-01, -4.6518e-01, -9.6167e-01, -1.8642e+00,
         -1.0418e+00, -2.4129e-01,  8.6804e-02, -1.1884e+00, -6.5867e-01,
          5.3723e-02, -3.7645e-01, -3.3506e-01, -2.8734e-01,  3.2893e-01,
          2.7881e-01, -3.6289e-01, -4.1485e-01, -4.6797e-01, -1.9758e-01,
          4.9012e-01,  1.4455e-01,  1.1013e-01, -2.2566e-01,  1.2224e-01,
          4.2588e-01, -1.0322e-01,  1.2777e-01, -2.4896e-01,  8.2889e-01,
          8.0342e-01, -2.6907e-01, -5.5556e-01,  9.8731e-02,  3.2141e-01,
          6.3706e-01,  1.0598e-01, -3.2021e-01, -5.9164e-02, -1.4649e-01,
          9.4830e-01,  9.3524e-01, -3.5189e-02, -1.9086e-01,  6.6262e-01,
         -6.2672e-02, -4.8877e-01, -9.4631e-01, -9.2788e-01, -7.2292e-01,
         -1.7591e-01, -7.0653e-02, -1.9847e-01, -2.7154e-01,  3.1179e-02,
          4.4782e-02,  4.2739e-02, -2.4899e-02,  9.0799e-02,  3.7407e-01,
          2.4569e-01,  5.4937e-01,  5.5690e-01,  8.0444e-01,  8.3054e-01,
         -3.1450e-01,  3.4278e-02,  1.6006e-01,  3.7164e-01,  1.0092e-01,
         -1.1590e+00, -8.6609e-01, -5.8813e-01, -6.0118e-01, -8.8276e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.1891, 1.2187, 1.2461, 1.7700, 0.7850, 2.9964, 1.3888, 0.4035, 2.3166,
         1.6565, 2.2415, 2.9745, 2.0852, 1.9916, 1.1950, 2.0078, 1.0109, 0.3323,
         1.5031, 1.2614, 2.8041, 1.4311, 1.1277, 1.8626, 1.3071, 1.8954, 1.2232,
         0.9980, 2.2979, 1.8712, 1.8191, 2.0685, 1.7850, 1.7475, 1.4254, 1.0407,
         1.4906, 1.3191, 0.6858, 0.5418, 0.1020, 1.1055, 1.3619, 0.0853, 0.9488,
         1.5956, 0.5211, 1.1578, 0.9516, 1.0436, 0.1377, 0.0000, 0.4661, 2.0105,
         1.1073, 0.0000, 1.0244, 1.2448, 0.1253, 0.7471, 0.2389, 0.4688, 1.8906,
         1.7018, 0.4318, 0.8455, 1.5242, 1.5302, 0.8413, 0.8180, 1.1108, 1.0844,
         0.8247, 1.5846, 0.4434, 0.3130, 0.8949, 1.0503, 1.3476, 1.2834, 0.5823,
         0.7285, 0.5882, 0.8632, 1.1297, 1.0337, 0.8972, 0.4471, 0.5478, 0.8486,
         1.1982, 1.1786, 1.0973, 0.8302, 0.4560, 1.1696, 0.6744, 0.8196, 1.0329,
         0.7337],
        [1.0500, 0.6596, 0.9762, 0.9329, 1.8989, 1.9007, 1.0971, 1.5628, 1.8273,
         1.3645, 1.8391, 1.0816, 2.1468, 1.4813, 1.5639, 2.0410, 1.5309, 1.0219,
         1.5180, 0.1175, 1.3103, 1.9042, 1.3058, 3.5585, 1.5625, 0.9950, 0.6966,
         1.5930, 1.7689, 1.6811, 1.4538, 0.8337, 1.3698, 2.5973, 1.9117, 1.5346,
         1.1563, 0.6434, 1.2816, 0.0000, 1.3469, 1.6345, 1.8243, 2.2821, 3.1339,
         0.0000, 1.7283, 1.0675, 2.2216, 1.2536, 1.1891, 1.0203, 1.1125, 1.2982,
         0.9819, 0.6931, 1.3478, 1.3476, 0.4506, 0.7028, 0.6561, 0.1749, 0.7812,
         0.2483, 0.0616, 1.3797, 2.3551, 1.3041, 1.4898, 1.1308, 1.0345, 0.9702,
         1.4314, 0.5445, 0.5643, 0.0148, 0.0000, 0.1657, 0.4634, 0.6386, 0.3566,
         0.4063, 0.8713, 1.2277, 1.4387, 1.4522, 1.1736, 1.4124, 1.4964, 1.6048,
         1.3849, 1.3048, 1.3989, 1.3369, 1.4083, 0.7179, 0.6884, 0.8262, 1.1709,
         1.3036],
        [1.5816, 1.1925, 1.0981, 1.3818, 1.3572, 2.1221, 1.1846, 1.5007, 1.5002,
         1.9078, 1.9291, 1.0154, 0.9365, 1.0415, 1.3707, 1.3377, 1.0226, 1.9221,
         1.2033, 1.1565, 1.5712, 2.5146, 1.9748, 3.0776, 1.4826, 1.1480, 2.7915,
         1.0846, 1.4870, 1.7592, 1.4656, 1.4693, 2.4559, 1.0157, 1.5178, 1.6544,
         2.2231, 2.2951, 1.5750, 2.2233, 2.3765, 1.7393, 0.2971, 0.4233, 1.4542,
         1.4274, 1.3155, 1.8870, 1.8339, 1.1455, 0.0000, 1.3894, 1.1064, 0.8252,
         1.0734, 1.0285, 0.2185, 0.6900, 0.6681, 1.2039, 0.3270, 0.8330, 1.1134,
         0.2992, 0.8655, 0.4953, 1.3754, 1.2236, 1.1757, 1.2920, 0.6811, 1.6961,
         1.4100, 0.7149, 1.0251, 0.3703, 0.0000, 0.4200, 1.3327, 1.2709, 0.6298,
         0.7295, 1.1273, 1.2732, 1.0216, 0.7257, 1.0123, 1.2375, 1.2681, 1.3588,
         0.9582, 0.9894, 1.1826, 1.3685, 1.5055, 1.3738, 1.1043, 1.2809, 1.0894,
         0.9011],
        [0.3525, 1.3970, 2.3054, 2.6632, 1.9115, 0.3780, 1.7738, 1.5207, 1.0592,
         1.2118, 1.2215, 1.0670, 1.8336, 1.1814, 0.9136, 1.1360, 1.4266, 0.8910,
         1.8150, 0.0000, 1.7388, 1.7453, 2.7042, 2.2311, 1.0252, 1.2571, 2.3639,
         1.5360, 1.2310, 1.0974, 2.1613, 1.6605, 2.8947, 1.6160, 1.9240, 2.5915,
         2.8856, 1.4468, 1.8448, 1.1234, 1.0687, 1.4029, 0.3702, 1.3144, 1.5325,
         0.4115, 1.3878, 1.9410, 1.0331, 0.2830, 0.4320, 0.2073, 0.6809, 0.4741,
         0.6896, 0.0000, 0.7525, 0.5080, 1.4983, 1.2150, 2.2493, 2.2762, 1.6835,
         0.8424, 1.1282, 1.4854, 1.7704, 1.0132, 1.9942, 2.7700, 1.7692, 1.6098,
         1.1951, 1.4029, 1.0310, 0.9404, 1.3243, 1.5441, 1.0413, 0.5136, 1.1817,
         1.1830, 1.1281, 0.8125, 0.8461, 1.6377, 1.4843, 1.2452, 0.9151, 1.0680,
         1.1976, 1.2073, 1.5189, 1.2795, 1.2037, 0.6042, 0.6989, 1.0076, 0.9565,
         1.0396],
        [0.9562, 0.9073, 1.0008, 0.6827, 0.4191, 2.3181, 2.2282, 0.6828, 1.8936,
         0.9245, 2.6932, 1.9141, 1.3624, 0.9343, 1.4136, 1.4941, 0.8500, 1.3495,
         0.0000, 1.2335, 2.5317, 2.0276, 1.3885, 2.2640, 1.5505, 1.8038, 2.0990,
         1.8347, 1.8134, 0.8567, 2.4024, 2.0857, 2.4574, 1.8844, 1.9581, 1.4933,
         1.2803, 0.4288, 0.5923, 1.4028, 1.0604, 1.7013, 1.5484, 1.6271, 2.0736,
         1.2128, 1.2070, 0.4972, 1.0992, 1.3418, 0.9928, 0.1493, 0.7070, 0.3366,
         0.4073, 1.9707, 1.6246, 1.3512, 1.6779, 1.4875, 0.2209, 1.5320, 2.1298,
         1.2773, 2.0150, 0.4646, 1.2617, 1.1548, 1.9256, 1.4996, 1.2337, 0.6894,
         0.9775, 1.2899, 2.0214, 0.7405, 1.0299, 1.3073, 1.4745, 1.3110, 0.9095,
         1.0848, 1.3095, 0.9725, 0.5642, 0.7905, 0.9372, 0.8268, 0.3513, 0.0000,
         0.7513, 1.0776, 0.7219, 0.1745, 0.0000, 0.9287, 1.0655, 0.8403, 0.6335,
         0.3507],
        [0.9323, 1.9660, 1.4599, 1.6449, 1.4919, 2.0676, 1.4970, 0.6971, 1.2038,
         1.2540, 1.5921, 1.9625, 1.4688, 2.2931, 1.2656, 1.0993, 2.3901, 2.1422,
         1.2511, 0.9154, 2.0193, 1.6389, 3.0077, 2.1064, 1.5874, 1.0193, 0.7579,
         1.3729, 1.4786, 0.8539, 1.9667, 1.2098, 1.9027, 1.9908, 1.2376, 2.9763,
         3.0688, 2.0898, 1.9752, 2.0882, 1.6910, 1.6044, 1.7770, 1.9948, 1.5225,
         1.1171, 0.7404, 1.7916, 2.3558, 0.8189, 0.0408, 0.8705, 0.5036, 0.7421,
         0.5994, 1.0687, 0.6642, 0.4782, 1.0991, 0.5082, 0.1368, 0.5372, 1.2819,
         0.9362, 0.8903, 0.2506, 0.8282, 1.1350, 0.2671, 0.8104, 2.2460, 1.5011,
         1.4470, 1.5980, 1.6455, 0.3761, 0.2611, 0.2155, 0.3904, 0.7150, 0.3035,
         0.4168, 0.4300, 0.5715, 0.9077, 0.8402, 0.8074, 0.9163, 0.8781, 0.8402,
         1.1483, 1.2526, 1.3458, 1.1982, 1.2056, 1.2758, 1.2798, 1.6566, 1.9059,
         1.4692],
        [0.7604, 0.7468, 2.3279, 1.1318, 1.4915, 0.4030, 1.2869, 1.0720, 1.6498,
         1.5622, 2.8696, 2.4094, 1.2245, 1.4409, 1.1468, 0.3167, 0.4673, 1.4807,
         2.0095, 1.7165, 0.9740, 1.4864, 1.9359, 1.4296, 1.2152, 0.0000, 1.1950,
         1.2459, 1.0588, 1.9150, 1.0673, 0.9032, 0.6426, 1.6398, 1.2912, 0.5145,
         0.6688, 0.2413, 0.8019, 1.7235, 1.8071, 1.8089, 1.9138, 0.5481, 1.1681,
         1.5015, 1.1443, 1.2764, 1.2291, 0.7842, 0.9406, 1.9132, 1.4743, 0.0000,
         0.3957, 0.0770, 1.1889, 1.2968, 0.0000, 0.0806, 0.6746, 0.0000, 0.0000,
         0.5940, 0.5727, 0.0000, 1.3537, 0.4995, 0.2906, 0.4199, 0.2217, 0.0000,
         1.3171, 0.0000, 0.2952, 1.6837, 1.8427, 1.5492, 1.1755, 0.8129, 1.6244,
         1.8522, 1.6191, 1.3199, 0.5839, 1.1102, 1.5301, 1.4968, 1.3509, 0.5867,
         0.6720, 1.1444, 1.6478, 2.0154, 1.2604, 0.9977, 1.3769, 1.8524, 2.6619,
         2.1138],
        [2.2818, 0.0722, 1.0645, 1.5877, 0.9556, 1.9146, 0.0000, 1.4704, 1.3500,
         1.0122, 2.5982, 1.8713, 1.3540, 0.6022, 1.3491, 1.5720, 1.4235, 0.7812,
         1.4457, 1.1527, 2.0318, 1.3191, 2.5708, 0.7337, 1.8931, 1.9915, 0.7287,
         1.4652, 1.9617, 2.8642, 2.0418, 1.2413, 0.9132, 2.1884, 1.6587, 0.9463,
         1.3764, 1.3351, 1.2873, 0.6711, 0.7212, 1.3629, 1.4149, 1.4680, 1.1976,
         0.5099, 0.8554, 0.8899, 1.2257, 0.8778, 0.5741, 1.1032, 0.8722, 1.2490,
         0.1711, 0.1966, 1.2691, 1.5556, 0.9013, 0.6786, 0.3629, 0.8940, 1.3202,
         1.0592, 1.1465, 0.0517, 0.0648, 1.0352, 1.1909, 0.3374, 1.0627, 1.4888,
         1.9463, 1.9279, 1.7229, 1.1759, 1.0707, 1.1985, 1.2715, 0.9688, 0.9552,
         0.9573, 1.0249, 0.9092, 0.6259, 0.7543, 0.4506, 0.4431, 0.1956, 0.1695,
         1.3145, 0.9657, 0.8399, 0.6284, 0.8991, 2.1590, 1.8661, 1.5881, 1.6012,
         1.8828]], device='cuda:0', grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2200, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1086, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1086, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-2.1191e-01,  2.1823e-01, -2.8274e-01, -7.4193e-01, -3.8815e-01,
         -3.4541e-01,  1.6358e-01, -8.0648e-01, -1.0658e+00, -1.6423e-01,
         -2.5178e-01, -2.1432e-01, -4.5777e-01, -1.1155e+00, -5.7204e-01,
          3.9868e-01, -8.8694e-01, -2.7667e-01, -4.2111e-01, -5.0351e-01,
         -5.1597e-02, -7.8648e-01, -1.1074e+00, -1.3082e+00, -7.2945e-01,
         -8.0579e-01, -1.5309e+00, -1.5241e+00, -1.6324e+00, -1.3104e+00,
          5.7400e-02,  2.1437e-02, -5.5769e-01, -1.2624e+00, -9.6297e-01,
         -6.2254e-01, -6.6673e-01, -1.4293e+00, -4.0030e-01, -1.2867e+00,
          4.8668e-01, -1.9844e-01,  7.6288e-01, -4.3964e-02, -2.2126e-01,
          3.3526e-01, -1.0210e+00, -1.0462e-02, -1.3315e+00, -3.8370e-01,
          4.2883e-01,  4.9782e-01,  3.8527e-01,  7.1650e-01,  1.1243e+00,
          6.1922e-01, -1.5219e-01,  3.2642e-01,  1.3288e+00,  5.2498e-01,
         -7.1801e-02, -2.1793e-01, -3.1468e-01,  3.7813e-02,  6.0213e-02,
          2.1707e-01,  6.5981e-01, -5.7288e-01,  6.3366e-01,  6.2040e-01,
         -1.8878e+00,  5.5989e-02, -3.3118e-01, -2.7137e-01, -2.9953e-01,
          4.6323e-01,  9.3370e-01,  1.2429e+00,  1.3929e+00,  1.3329e+00,
         -7.2778e-01, -5.6491e-01, -4.1024e-01, -2.1808e-01,  2.2924e-01,
         -8.8014e-01, -1.0393e+00, -1.3779e+00, -1.1079e+00, -4.1814e-01,
         -5.5644e-01, -7.3474e-01, -1.2196e+00, -1.3514e+00, -5.1304e-01,
         -5.4004e-01, -5.6208e-01, -9.2423e-01, -8.3445e-01,  6.0125e-01],
        [-1.4278e+00, -4.4313e-01,  2.2284e-01, -1.9696e-01, -6.0260e-02,
         -5.2229e-01, -7.4410e-01,  1.3591e-01, -7.2361e-01, -3.7946e-01,
         -4.9156e-01, -2.2697e+00, -3.9782e-01, -1.0800e+00,  4.9285e-01,
         -8.4230e-01,  5.3654e-01, -3.9673e-01, -1.7434e+00,  8.9914e-03,
         -1.2534e+00, -6.7591e-01, -3.9810e-01, -3.6944e-01,  6.6464e-02,
         -3.2513e-01, -5.0588e-01, -3.7404e-01, -3.1804e-02, -6.6660e-01,
         -1.4464e+00, -5.8239e-01, -1.7392e+00, -1.7897e+00, -1.7580e+00,
         -1.4948e+00, -1.5688e+00, -3.0127e-02, -2.5508e-01, -8.7674e-01,
         -3.2036e-01, -8.3633e-01, -5.8415e-01, -6.1477e-01, -4.1136e-01,
         -4.9883e-02, -4.9808e-01, -9.2680e-01, -5.5701e-01,  3.4676e-01,
          4.9145e-01,  6.5704e-01,  8.3425e-01,  6.7557e-01,  8.8487e-01,
          6.1575e-02, -8.3914e-01,  4.1908e-01,  7.6672e-02,  1.0359e+00,
          3.3496e-01, -1.2406e-01, -6.5934e-01,  6.7734e-01,  7.7809e-01,
         -9.7860e-01,  5.3098e-01,  2.3331e-02, -1.4900e+00, -4.4225e-01,
         -6.2704e-01, -1.0517e+00, -5.1985e-01, -1.1021e-01,  3.5838e-01,
         -1.7767e-01,  4.2486e-01,  1.0136e+00,  7.7124e-01,  3.0643e-01,
         -2.2161e-01, -2.6193e-01,  1.4082e-01,  5.1503e-01,  4.8863e-01,
         -2.3450e-01, -8.4642e-01, -1.1531e+00, -7.5403e-01, -3.2495e-01,
         -9.0277e-02, -4.9835e-01, -5.8173e-01, -5.0580e-01, -3.9674e-01,
         -7.2758e-01, -3.6044e-01, -4.5880e-02, -4.9200e-02, -1.1003e-01],
        [-1.9602e-01, -9.5503e-01, -7.5794e-01,  1.5884e-01, -1.7133e+00,
         -5.1071e-01, -2.1928e-01, -7.8887e-01, -8.6715e-01,  3.9251e-01,
         -1.7652e-01, -1.5950e+00, -1.4914e+00, -1.7196e+00,  2.7683e-01,
         -2.7351e-01,  4.5192e-02,  4.0628e-01, -1.1857e+00, -3.0828e-01,
         -6.3468e-01, -1.6963e-01, -7.0542e-01, -2.0793e+00, -1.2994e+00,
         -9.8951e-01, -1.3323e+00, -9.5301e-01, -1.1045e+00, -1.4235e+00,
         -6.9814e-01,  2.7989e-01, -2.8642e-01,  8.2866e-01, -1.6190e+00,
         -4.8906e-01, -1.6479e-01, -8.8462e-01, -5.7831e-01, -1.0439e+00,
         -2.5495e-01, -4.2194e-01, -9.5878e-01, -7.7054e-02,  3.5111e-01,
         -2.8367e-01,  1.8330e-01,  1.1415e+00, -1.2341e-01,  8.8839e-02,
          2.2362e-03, -1.0119e+00,  4.6039e-01,  1.2545e-01, -5.2688e-01,
          7.8940e-01,  9.1474e-01, -8.1405e-02,  1.8018e+00,  1.3600e+00,
         -2.5493e-01, -1.7560e-01, -1.7880e-01,  6.5686e-01,  8.6002e-01,
          6.9073e-01, -3.3176e-02,  1.4601e-02, -4.0127e-01, -6.3930e-01,
          3.4087e-01,  8.6750e-01, -3.3542e-01, -7.8593e-02,  6.3383e-01,
          9.5540e-01,  1.0293e+00,  1.1264e+00,  9.6785e-01,  1.0527e+00,
          6.4543e-01,  7.8465e-01,  7.5434e-01,  3.5448e-01,  3.6516e-01,
          4.2852e-01,  5.0011e-01,  7.6080e-01,  7.9932e-01,  4.7392e-01,
         -2.6769e-01, -2.8537e-01,  2.6290e-02,  3.7256e-01,  3.2885e-01,
         -2.5488e-01, -2.0127e-01, -6.1994e-02,  2.2078e-01,  2.3913e-01],
        [-6.6028e-01,  1.5795e-01, -9.2201e-01, -1.1673e+00, -6.6639e-01,
          2.7502e-01,  4.6655e-01,  4.0000e-01,  2.9744e-01, -1.5235e+00,
         -3.3816e-01, -1.5169e-01, -3.9624e-01, -3.2997e-01,  2.7023e-01,
         -7.0467e-01, -9.1839e-01, -1.0419e+00, -1.6545e-01,  1.5926e-01,
         -2.0589e-01, -3.6603e-01, -5.4665e-01,  5.6309e-02, -1.5904e-01,
         -7.9535e-01, -1.1909e+00, -4.2619e-01, -4.9051e-01,  9.1376e-03,
         -1.3210e+00, -2.8804e-02, -1.1711e+00, -1.4564e+00, -2.2103e-01,
         -6.1116e-01,  1.2551e-01,  4.5563e-01,  2.6795e-01, -2.2038e-01,
         -3.5827e-01, -5.1596e-01, -1.0533e+00, -6.9076e-01, -3.9889e-01,
         -9.4460e-01,  2.3463e-01, -3.7609e-01, -7.5334e-01, -3.8949e-01,
          5.6565e-01,  5.4857e-01,  1.5535e+00,  1.0097e+00,  5.6358e-01,
          8.0246e-01, -3.2261e-01, -1.3561e-01,  1.5317e-01,  1.6482e+00,
         -1.1959e-02,  1.0027e-01, -4.0760e-01, -9.6089e-01, -3.7424e-01,
         -2.2803e-01, -4.4590e-01, -1.4307e-01,  2.6494e-01, -9.3807e-02,
         -3.6656e-01, -6.3540e-01, -1.9445e-01, -9.0975e-01, -6.0576e-01,
         -1.0043e-01, -4.9141e-01, -1.8467e-01,  5.3470e-02,  5.0755e-01,
          5.0664e-02, -6.2381e-01, -7.4144e-01, -4.2554e-01,  3.3848e-02,
          4.7760e-01, -5.1011e-01, -6.9231e-01, -3.1596e-01, -9.7901e-02,
          4.5855e-01, -4.0213e-01, -4.8885e-01,  4.8480e-02, -1.3889e-01,
         -1.7228e-01, -6.7976e-01, -3.7844e-01,  2.9255e-02, -2.4829e-01],
        [-4.1359e-01,  6.1505e-01, -4.6084e-01, -4.6194e-01, -3.7203e-01,
          6.6742e-01, -2.9907e-01,  1.3081e-01,  2.3813e-01, -4.4068e-01,
         -1.2501e+00, -1.4630e+00, -4.1164e-01,  2.6487e-02, -5.9168e-01,
         -5.4394e-01, -1.2676e+00, -6.9544e-01, -7.0853e-01, -6.4016e-01,
         -7.7308e-02,  1.3231e-01, -4.1039e-01, -2.0386e-01,  1.7457e-01,
         -4.1281e-01, -1.6652e-01, -3.4014e-01, -1.0277e+00,  2.5540e-01,
         -7.7921e-01, -4.4861e-01, -5.5993e-01, -1.6169e-03,  9.0522e-02,
         -1.5965e+00, -8.2904e-01, -2.0718e+00, -5.1391e-01, -7.9852e-01,
         -1.7249e+00, -3.1303e-01,  5.2456e-02, -1.1710e+00, -6.8679e-01,
         -1.4915e+00, -1.0097e+00,  2.1017e-01, -1.1003e+00,  2.5117e-01,
          4.9784e-01,  4.8763e-01,  2.6823e-01,  4.0100e-01,  9.6801e-01,
          2.9580e-01,  1.1672e+00, -1.6583e-01, -6.4449e-01, -6.2926e-02,
          1.3752e+00, -1.1338e-01,  5.9078e-01,  1.3512e-01,  6.5780e-01,
          5.7338e-01, -3.9030e-01, -5.9211e-01,  6.7321e-02,  1.9945e-01,
          6.4887e-01, -7.6946e-01,  4.9431e-02, -3.8196e-01,  6.0811e-01,
         -1.4404e-02, -2.2932e-01, -3.7876e-02,  2.2890e-01,  2.6897e-01,
         -2.4947e-01, -4.7954e-01, -2.9737e-01, -7.6016e-02,  2.0316e-01,
         -4.6326e-01, -7.7970e-01, -8.2879e-01, -4.5047e-01, -1.2016e-02,
         -3.3713e-01, -1.1478e+00, -1.5844e+00, -1.2872e+00, -5.8328e-01,
          2.9894e-01, -2.4486e-01, -8.5041e-01, -1.2225e+00, -7.4161e-01],
        [-7.2538e-01, -3.2664e-01, -1.6538e+00, -5.0846e-01, -1.5037e+00,
         -2.4065e+00, -1.4766e+00, -1.7602e-01, -6.6050e-01, -1.0941e+00,
         -1.5735e+00, -1.9153e+00, -6.4695e-01, -2.9237e-01, -6.3940e-01,
         -2.8205e-01, -1.4962e-01,  1.8379e-01,  5.0290e-01, -4.0674e-02,
         -1.7155e+00, -9.9787e-01, -3.8032e-01, -9.7462e-01, -1.5125e+00,
          6.9497e-01, -1.1508e-01, -1.1208e+00, -1.2007e+00, -2.5526e-01,
         -1.1716e-01, -9.0842e-01,  2.0162e-01, -3.9948e-01, -6.1205e-01,
         -4.9653e-01, -4.0436e-01, -7.4499e-01, -7.8900e-01, -9.8988e-01,
          2.9064e-01, -7.2489e-01, -7.6538e-01, -5.7961e-01,  5.8206e-01,
         -1.2067e+00, -6.2453e-01,  6.3321e-02, -7.2808e-01, -1.4547e+00,
          6.9143e-01, -4.2477e-01, -2.6831e-01,  5.7836e-01,  7.0687e-01,
          7.4193e-01, -9.7385e-02, -4.0203e-01, -8.3950e-01,  2.2146e-01,
          8.3063e-01, -3.4641e-01,  4.5639e-01,  8.2985e-02,  6.9935e-01,
         -7.4478e-01,  5.8555e-01,  4.9528e-01, -4.3766e-01,  8.9079e-01,
          1.3252e+00, -5.5113e-02, -1.3972e-02, -2.8165e-01,  5.1036e-01,
          2.6759e-02, -3.9863e-01, -3.1751e-01, -3.0146e-01, -7.0728e-01,
          2.9183e-01, -1.4908e-01, -3.4815e-01, -7.6527e-02, -2.3628e-01,
          3.3062e-01,  3.7881e-02, -1.5266e-01,  1.0521e-01,  8.4147e-03,
          3.4710e-02,  1.2888e-01,  1.8215e-01,  4.0409e-01,  5.5913e-01,
          5.9418e-02,  2.5400e-01,  4.0252e-02, -2.1284e-01,  2.3883e-03],
        [-7.4936e-01,  2.7870e-02,  6.6268e-01,  5.4820e-01,  1.2943e-02,
         -7.6588e-01, -6.8695e-01, -1.3575e-01, -7.2240e-01, -3.5770e-01,
         -9.2495e-01, -5.1405e-01, -4.9045e-01, -1.8094e+00, -1.3220e+00,
         -1.1026e+00, -9.0684e-01, -2.1074e-01, -1.0101e+00, -1.3590e+00,
         -6.1269e-01, -5.3967e-01, -4.0383e-01, -1.0100e+00, -1.4923e-01,
         -1.7269e-01, -1.5742e-01, -2.2151e-01, -9.6369e-01,  1.6416e-02,
         -7.1969e-01, -7.8897e-01, -8.7348e-01, -2.1221e-01,  4.5066e-01,
         -6.4661e-01, -9.9344e-01, -2.9622e-01, -7.9118e-01, -9.3064e-01,
         -7.9110e-01, -6.7452e-01, -5.6974e-01, -9.7369e-01, -1.9542e+00,
         -4.2447e-02, -2.9105e-02, -1.1827e+00, -1.8441e-01, -3.8009e-01,
          9.2414e-01,  6.6031e-01,  1.1207e+00, -7.3509e-02,  6.2267e-01,
          2.2073e+00,  9.9871e-01,  1.8483e-01, -8.7691e-01,  7.7143e-01,
          3.0965e-02, -4.0118e-01, -5.8339e-01, -5.4101e-01, -2.6712e-01,
          6.5293e-01,  6.9453e-01,  2.9540e-01,  3.1253e-01,  7.7398e-01,
          4.9352e-01,  1.6260e-02,  9.4941e-01, -2.2665e-01,  4.6733e-01,
         -2.6852e-01,  8.9520e-02,  4.7814e-01,  8.2799e-01,  7.9865e-01,
         -3.2084e-01, -2.0434e-01,  1.3221e-01,  4.4632e-01,  3.7238e-01,
         -7.4318e-02,  4.1329e-02,  6.4501e-02,  7.5770e-02,  6.7013e-01,
          1.0086e-01,  4.2527e-01,  7.1841e-01,  7.0804e-01,  1.6035e+00,
         -1.0215e-01,  3.3297e-01,  3.7126e-01,  8.3665e-01,  1.9048e+00],
        [-7.9638e-01,  5.1682e-01,  5.4238e-01, -6.8896e-01,  5.0398e-01,
          5.5231e-01, -8.1114e-01, -6.7363e-03,  1.1398e-01, -2.5664e-01,
         -6.1771e-01, -1.2667e+00, -9.9095e-01, -1.2848e+00,  1.6723e-02,
         -7.1309e-01, -1.2581e+00, -1.7808e+00, -7.9662e-01,  3.6308e-01,
         -4.5836e-01, -1.3012e+00, -1.4521e+00, -1.0724e+00, -1.9560e-01,
         -2.1780e+00, -1.0049e-01, -1.6425e+00, -1.6716e+00, -1.5061e+00,
          1.3435e-03, -9.9708e-01, -9.2300e-01, -6.2010e-01, -3.3803e-01,
         -1.0610e+00, -4.6466e-01,  2.5375e-01, -2.9608e-01, -5.9368e-01,
          3.7458e-01, -5.4737e-01,  2.2019e-01, -3.6358e-01, -1.0424e+00,
          5.7229e-01, -2.0938e-01, -4.1312e-01, -3.1326e-01, -7.0119e-01,
         -2.5432e-01,  2.5163e-01,  6.4157e-01, -1.6820e-02,  8.3505e-02,
         -6.0401e-01,  6.9726e-01, -1.1059e+00, -1.9272e-01,  9.6606e-01,
         -5.4160e-01, -7.5900e-01,  2.8655e-01, -4.6706e-01, -1.0848e+00,
         -6.9616e-01,  2.6834e-01,  1.2651e+00, -2.9998e-01, -2.7305e-01,
         -4.6665e-01,  2.2890e-01,  2.3824e-01,  5.5948e-01,  6.2840e-02,
         -2.1993e-02, -1.7425e-01, -7.0528e-02,  6.3075e-03,  2.5154e-02,
          1.5325e-01,  2.6011e-01, -4.6053e-02, -9.6850e-02,  2.1899e-01,
          3.0992e-01,  7.2616e-01,  4.1990e-01,  2.2428e-01,  5.6469e-01,
          5.7004e-01,  9.0587e-01,  5.7644e-01,  3.7005e-01,  6.3265e-01,
          6.3356e-02,  4.2479e-01,  4.2812e-01,  3.9986e-01,  2.1774e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.2119e+00, 7.8177e-01, 1.2827e+00, 1.7419e+00, 1.3881e+00, 1.3454e+00,
         8.3642e-01, 1.8065e+00, 2.0658e+00, 1.1642e+00, 1.2518e+00, 1.2143e+00,
         1.4578e+00, 2.1155e+00, 1.5720e+00, 6.0132e-01, 1.8869e+00, 1.2767e+00,
         1.4211e+00, 1.5035e+00, 1.0516e+00, 1.7865e+00, 2.1074e+00, 2.3082e+00,
         1.7295e+00, 1.8058e+00, 2.5309e+00, 2.5241e+00, 2.6324e+00, 2.3104e+00,
         9.4260e-01, 9.7856e-01, 1.5577e+00, 2.2624e+00, 1.9630e+00, 1.6225e+00,
         1.6667e+00, 2.4293e+00, 1.4003e+00, 2.2867e+00, 5.1332e-01, 1.1984e+00,
         2.3712e-01, 1.0440e+00, 1.2213e+00, 6.6474e-01, 2.0210e+00, 1.0105e+00,
         2.3315e+00, 1.3837e+00, 5.7117e-01, 5.0218e-01, 6.1473e-01, 2.8350e-01,
         0.0000e+00, 3.8078e-01, 1.1522e+00, 6.7358e-01, 0.0000e+00, 4.7502e-01,
         1.0718e+00, 1.2179e+00, 1.3147e+00, 9.6219e-01, 9.3979e-01, 7.8293e-01,
         3.4019e-01, 1.5729e+00, 3.6634e-01, 3.7960e-01, 2.8878e+00, 9.4401e-01,
         1.3312e+00, 1.2714e+00, 1.2995e+00, 5.3677e-01, 6.6301e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00, 1.7278e+00, 1.5649e+00, 1.4102e+00, 1.2181e+00,
         7.7076e-01, 1.8801e+00, 2.0393e+00, 2.3779e+00, 2.1079e+00, 1.4181e+00,
         1.5564e+00, 1.7347e+00, 2.2196e+00, 2.3514e+00, 1.5130e+00, 1.5400e+00,
         1.5621e+00, 1.9242e+00, 1.8344e+00, 3.9875e-01],
        [2.4278e+00, 1.4431e+00, 7.7716e-01, 1.1970e+00, 1.0603e+00, 1.5223e+00,
         1.7441e+00, 8.6409e-01, 1.7236e+00, 1.3795e+00, 1.4916e+00, 3.2697e+00,
         1.3978e+00, 2.0800e+00, 5.0715e-01, 1.8423e+00, 4.6346e-01, 1.3967e+00,
         2.7434e+00, 9.9101e-01, 2.2534e+00, 1.6759e+00, 1.3981e+00, 1.3694e+00,
         9.3354e-01, 1.3251e+00, 1.5059e+00, 1.3740e+00, 1.0318e+00, 1.6666e+00,
         2.4464e+00, 1.5824e+00, 2.7392e+00, 2.7897e+00, 2.7580e+00, 2.4948e+00,
         2.5688e+00, 1.0301e+00, 1.2551e+00, 1.8767e+00, 1.3204e+00, 1.8363e+00,
         1.5841e+00, 1.6148e+00, 1.4114e+00, 1.0499e+00, 1.4981e+00, 1.9268e+00,
         1.5570e+00, 6.5324e-01, 5.0855e-01, 3.4296e-01, 1.6575e-01, 3.2443e-01,
         1.1513e-01, 9.3842e-01, 1.8391e+00, 5.8092e-01, 9.2333e-01, 0.0000e+00,
         6.6504e-01, 1.1241e+00, 1.6593e+00, 3.2266e-01, 2.2191e-01, 1.9786e+00,
         4.6902e-01, 9.7667e-01, 2.4900e+00, 1.4422e+00, 1.6270e+00, 2.0517e+00,
         1.5198e+00, 1.1102e+00, 6.4162e-01, 1.1777e+00, 5.7514e-01, 0.0000e+00,
         2.2876e-01, 6.9357e-01, 1.2216e+00, 1.2619e+00, 8.5918e-01, 4.8497e-01,
         5.1137e-01, 1.2345e+00, 1.8464e+00, 2.1531e+00, 1.7540e+00, 1.3249e+00,
         1.0903e+00, 1.4983e+00, 1.5817e+00, 1.5058e+00, 1.3967e+00, 1.7276e+00,
         1.3604e+00, 1.0459e+00, 1.0492e+00, 1.1100e+00],
        [1.1960e+00, 1.9550e+00, 1.7579e+00, 8.4116e-01, 2.7133e+00, 1.5107e+00,
         1.2193e+00, 1.7889e+00, 1.8671e+00, 6.0749e-01, 1.1765e+00, 2.5950e+00,
         2.4914e+00, 2.7196e+00, 7.2317e-01, 1.2735e+00, 9.5481e-01, 5.9372e-01,
         2.1857e+00, 1.3083e+00, 1.6347e+00, 1.1696e+00, 1.7054e+00, 3.0793e+00,
         2.2994e+00, 1.9895e+00, 2.3323e+00, 1.9530e+00, 2.1045e+00, 2.4235e+00,
         1.6981e+00, 7.2011e-01, 1.2864e+00, 1.7134e-01, 2.6190e+00, 1.4891e+00,
         1.1648e+00, 1.8846e+00, 1.5783e+00, 2.0439e+00, 1.2549e+00, 1.4219e+00,
         1.9588e+00, 1.0771e+00, 6.4889e-01, 1.2837e+00, 8.1670e-01, 0.0000e+00,
         1.1234e+00, 9.1116e-01, 9.9776e-01, 2.0119e+00, 5.3961e-01, 8.7455e-01,
         1.5269e+00, 2.1060e-01, 8.5260e-02, 1.0814e+00, 0.0000e+00, 0.0000e+00,
         1.2549e+00, 1.1756e+00, 1.1788e+00, 3.4314e-01, 1.3998e-01, 3.0927e-01,
         1.0332e+00, 9.8540e-01, 1.4013e+00, 1.6393e+00, 6.5913e-01, 1.3250e-01,
         1.3354e+00, 1.0786e+00, 3.6617e-01, 4.4604e-02, 0.0000e+00, 0.0000e+00,
         3.2147e-02, 0.0000e+00, 3.5457e-01, 2.1535e-01, 2.4566e-01, 6.4552e-01,
         6.3484e-01, 5.7148e-01, 4.9989e-01, 2.3920e-01, 2.0068e-01, 5.2608e-01,
         1.2677e+00, 1.2854e+00, 9.7371e-01, 6.2744e-01, 6.7115e-01, 1.2549e+00,
         1.2013e+00, 1.0620e+00, 7.7922e-01, 7.6087e-01],
        [1.6603e+00, 8.4205e-01, 1.9220e+00, 2.1673e+00, 1.6664e+00, 7.2498e-01,
         5.3345e-01, 6.0000e-01, 7.0256e-01, 2.5235e+00, 1.3382e+00, 1.1517e+00,
         1.3962e+00, 1.3300e+00, 7.2977e-01, 1.7047e+00, 1.9184e+00, 2.0419e+00,
         1.1655e+00, 8.4074e-01, 1.2059e+00, 1.3660e+00, 1.5467e+00, 9.4369e-01,
         1.1590e+00, 1.7953e+00, 2.1909e+00, 1.4262e+00, 1.4905e+00, 9.9086e-01,
         2.3210e+00, 1.0288e+00, 2.1711e+00, 2.4564e+00, 1.2210e+00, 1.6112e+00,
         8.7449e-01, 5.4437e-01, 7.3205e-01, 1.2204e+00, 1.3583e+00, 1.5160e+00,
         2.0533e+00, 1.6908e+00, 1.3989e+00, 1.9446e+00, 7.6537e-01, 1.3761e+00,
         1.7533e+00, 1.3895e+00, 4.3435e-01, 4.5143e-01, 0.0000e+00, 0.0000e+00,
         4.3642e-01, 1.9754e-01, 1.3226e+00, 1.1356e+00, 8.4683e-01, 0.0000e+00,
         1.0120e+00, 8.9973e-01, 1.4076e+00, 1.9609e+00, 1.3742e+00, 1.2280e+00,
         1.4459e+00, 1.1431e+00, 7.3506e-01, 1.0938e+00, 1.3666e+00, 1.6354e+00,
         1.1944e+00, 1.9098e+00, 1.6058e+00, 1.1004e+00, 1.4914e+00, 1.1847e+00,
         9.4653e-01, 4.9245e-01, 9.4934e-01, 1.6238e+00, 1.7414e+00, 1.4255e+00,
         9.6615e-01, 5.2240e-01, 1.5101e+00, 1.6923e+00, 1.3160e+00, 1.0979e+00,
         5.4145e-01, 1.4021e+00, 1.4888e+00, 9.5152e-01, 1.1389e+00, 1.1723e+00,
         1.6798e+00, 1.3784e+00, 9.7075e-01, 1.2483e+00],
        [1.4136e+00, 3.8495e-01, 1.4608e+00, 1.4619e+00, 1.3720e+00, 3.3258e-01,
         1.2991e+00, 8.6919e-01, 7.6187e-01, 1.4407e+00, 2.2501e+00, 2.4630e+00,
         1.4116e+00, 9.7351e-01, 1.5917e+00, 1.5439e+00, 2.2676e+00, 1.6954e+00,
         1.7085e+00, 1.6402e+00, 1.0773e+00, 8.6769e-01, 1.4104e+00, 1.2039e+00,
         8.2543e-01, 1.4128e+00, 1.1665e+00, 1.3401e+00, 2.0277e+00, 7.4460e-01,
         1.7792e+00, 1.4486e+00, 1.5599e+00, 1.0016e+00, 9.0948e-01, 2.5965e+00,
         1.8290e+00, 3.0718e+00, 1.5139e+00, 1.7985e+00, 2.7249e+00, 1.3130e+00,
         9.4754e-01, 2.1710e+00, 1.6868e+00, 2.4915e+00, 2.0097e+00, 7.8983e-01,
         2.1003e+00, 7.4883e-01, 5.0216e-01, 5.1237e-01, 7.3177e-01, 5.9900e-01,
         3.1991e-02, 7.0420e-01, 0.0000e+00, 1.1658e+00, 1.6445e+00, 1.0629e+00,
         0.0000e+00, 1.1134e+00, 4.0922e-01, 8.6488e-01, 3.4220e-01, 4.2662e-01,
         1.3903e+00, 1.5921e+00, 9.3268e-01, 8.0055e-01, 3.5113e-01, 1.7695e+00,
         9.5057e-01, 1.3820e+00, 3.9189e-01, 1.0144e+00, 1.2293e+00, 1.0379e+00,
         7.7110e-01, 7.3103e-01, 1.2495e+00, 1.4795e+00, 1.2974e+00, 1.0760e+00,
         7.9684e-01, 1.4633e+00, 1.7797e+00, 1.8288e+00, 1.4505e+00, 1.0120e+00,
         1.3371e+00, 2.1478e+00, 2.5844e+00, 2.2872e+00, 1.5833e+00, 7.0106e-01,
         1.2449e+00, 1.8504e+00, 2.2225e+00, 1.7416e+00],
        [1.7254e+00, 1.3266e+00, 2.6538e+00, 1.5085e+00, 2.5037e+00, 3.4065e+00,
         2.4766e+00, 1.1760e+00, 1.6605e+00, 2.0941e+00, 2.5735e+00, 2.9153e+00,
         1.6470e+00, 1.2924e+00, 1.6394e+00, 1.2821e+00, 1.1496e+00, 8.1621e-01,
         4.9710e-01, 1.0407e+00, 2.7155e+00, 1.9979e+00, 1.3803e+00, 1.9746e+00,
         2.5125e+00, 3.0503e-01, 1.1151e+00, 2.1208e+00, 2.2007e+00, 1.2553e+00,
         1.1172e+00, 1.9084e+00, 7.9838e-01, 1.3995e+00, 1.6120e+00, 1.4965e+00,
         1.4044e+00, 1.7450e+00, 1.7890e+00, 1.9899e+00, 7.0936e-01, 1.7249e+00,
         1.7654e+00, 1.5796e+00, 4.1794e-01, 2.2067e+00, 1.6245e+00, 9.3668e-01,
         1.7281e+00, 2.4547e+00, 3.0857e-01, 1.4248e+00, 1.2683e+00, 4.2164e-01,
         2.9313e-01, 2.5807e-01, 1.0974e+00, 1.4020e+00, 1.8395e+00, 7.7854e-01,
         1.6937e-01, 1.3464e+00, 5.4361e-01, 9.1702e-01, 3.0065e-01, 1.7448e+00,
         4.1445e-01, 5.0472e-01, 1.4377e+00, 1.0921e-01, 0.0000e+00, 1.0551e+00,
         1.0140e+00, 1.2817e+00, 4.8964e-01, 9.7324e-01, 1.3986e+00, 1.3175e+00,
         1.3015e+00, 1.7073e+00, 7.0817e-01, 1.1491e+00, 1.3481e+00, 1.0765e+00,
         1.2363e+00, 6.6938e-01, 9.6212e-01, 1.1527e+00, 8.9479e-01, 9.9159e-01,
         9.6529e-01, 8.7112e-01, 8.1785e-01, 5.9591e-01, 4.4087e-01, 9.4058e-01,
         7.4600e-01, 9.5975e-01, 1.2128e+00, 9.9761e-01],
        [1.7494e+00, 9.7213e-01, 3.3732e-01, 4.5180e-01, 9.8706e-01, 1.7659e+00,
         1.6869e+00, 1.1358e+00, 1.7224e+00, 1.3577e+00, 1.9249e+00, 1.5140e+00,
         1.4904e+00, 2.8094e+00, 2.3220e+00, 2.1026e+00, 1.9068e+00, 1.2107e+00,
         2.0101e+00, 2.3590e+00, 1.6127e+00, 1.5397e+00, 1.4038e+00, 2.0100e+00,
         1.1492e+00, 1.1727e+00, 1.1574e+00, 1.2215e+00, 1.9637e+00, 9.8358e-01,
         1.7197e+00, 1.7890e+00, 1.8735e+00, 1.2122e+00, 5.4934e-01, 1.6466e+00,
         1.9934e+00, 1.2962e+00, 1.7912e+00, 1.9306e+00, 1.7911e+00, 1.6745e+00,
         1.5697e+00, 1.9737e+00, 2.9542e+00, 1.0424e+00, 1.0291e+00, 2.1827e+00,
         1.1844e+00, 1.3801e+00, 7.5860e-02, 3.3969e-01, 0.0000e+00, 1.0735e+00,
         3.7733e-01, 0.0000e+00, 1.2885e-03, 8.1517e-01, 1.8769e+00, 2.2857e-01,
         9.6904e-01, 1.4012e+00, 1.5834e+00, 1.5410e+00, 1.2671e+00, 3.4707e-01,
         3.0547e-01, 7.0460e-01, 6.8747e-01, 2.2602e-01, 5.0648e-01, 9.8374e-01,
         5.0593e-02, 1.2266e+00, 5.3267e-01, 1.2685e+00, 9.1048e-01, 5.2186e-01,
         1.7201e-01, 2.0135e-01, 1.3208e+00, 1.2043e+00, 8.6779e-01, 5.5368e-01,
         6.2762e-01, 1.0743e+00, 9.5867e-01, 9.3550e-01, 9.2423e-01, 3.2987e-01,
         8.9914e-01, 5.7473e-01, 2.8159e-01, 2.9196e-01, 0.0000e+00, 1.1022e+00,
         6.6703e-01, 6.2874e-01, 1.6335e-01, 0.0000e+00],
        [1.7964e+00, 4.8318e-01, 4.5762e-01, 1.6890e+00, 4.9602e-01, 4.4769e-01,
         1.8111e+00, 1.0067e+00, 8.8602e-01, 1.2566e+00, 1.6177e+00, 2.2667e+00,
         1.9910e+00, 2.2848e+00, 9.8328e-01, 1.7131e+00, 2.2581e+00, 2.7808e+00,
         1.7966e+00, 6.3692e-01, 1.4584e+00, 2.3012e+00, 2.4521e+00, 2.0724e+00,
         1.1956e+00, 3.1780e+00, 1.1005e+00, 2.6425e+00, 2.6716e+00, 2.5061e+00,
         9.9866e-01, 1.9971e+00, 1.9230e+00, 1.6201e+00, 1.3380e+00, 2.0610e+00,
         1.4647e+00, 7.4625e-01, 1.2961e+00, 1.5937e+00, 6.2542e-01, 1.5474e+00,
         7.7981e-01, 1.3636e+00, 2.0424e+00, 4.2771e-01, 1.2094e+00, 1.4131e+00,
         1.3133e+00, 1.7012e+00, 1.2543e+00, 7.4837e-01, 3.5843e-01, 1.0168e+00,
         9.1650e-01, 1.6040e+00, 3.0274e-01, 2.1059e+00, 1.1927e+00, 3.3935e-02,
         1.5416e+00, 1.7590e+00, 7.1345e-01, 1.4671e+00, 2.0848e+00, 1.6962e+00,
         7.3166e-01, 0.0000e+00, 1.3000e+00, 1.2731e+00, 1.4667e+00, 7.7110e-01,
         7.6176e-01, 4.4052e-01, 9.3716e-01, 1.0220e+00, 1.1742e+00, 1.0705e+00,
         9.9369e-01, 9.7485e-01, 8.4675e-01, 7.3989e-01, 1.0461e+00, 1.0968e+00,
         7.8101e-01, 6.9008e-01, 2.7384e-01, 5.8010e-01, 7.7572e-01, 4.3531e-01,
         4.2996e-01, 9.4130e-02, 4.2356e-01, 6.2995e-01, 3.6735e-01, 9.3664e-01,
         5.7521e-01, 5.7188e-01, 6.0014e-01, 7.8226e-01]], device='cuda:0',
       grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2466, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1034, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1034, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-2.2522e-01, -7.7123e-01, -1.2822e+00,  6.0868e-01, -5.7354e-01,
          1.9950e-01, -1.5572e-01, -1.2342e-01,  4.7258e-01, -7.5180e-01,
          7.4219e-02, -3.8570e-01, -7.9273e-01,  5.2673e-01,  1.0021e-01,
         -1.1166e+00, -1.2582e+00, -1.0546e+00, -8.2977e-01, -7.4722e-01,
         -2.8468e-01, -1.2426e+00, -1.8645e+00, -2.1204e-01, -3.9965e-01,
         -9.7526e-01, -3.3714e-01, -6.9455e-01, -9.3326e-01, -5.7189e-01,
         -6.1263e-01, -2.8399e-01, -1.5237e+00, -9.0899e-01, -9.4727e-01,
         -2.4450e-01,  6.3943e-01, -5.3991e-01, -7.9341e-01, -1.4677e+00,
         -8.8842e-02, -6.6079e-01, -9.3525e-01, -8.0862e-01, -1.7714e+00,
         -5.5457e-01, -4.6640e-01, -1.1909e+00, -3.1252e-01, -1.1485e+00,
          1.8010e-01,  1.2136e-01,  1.3605e+00,  2.1636e-01,  2.2735e-01,
         -6.1105e-01, -4.6589e-01, -6.2346e-01, -6.4051e-01,  2.9830e-01,
          4.0557e-01, -9.6949e-02, -3.7141e-01, -7.8913e-01, -2.3870e-01,
          4.3360e-01, -2.2136e-01,  4.9801e-01,  3.0135e-01,  9.9934e-01,
          2.6782e-01, -1.1193e-01, -8.4105e-01, -4.8849e-01,  1.0014e+00,
          1.1483e+00,  1.1413e+00,  9.5232e-01,  6.8511e-01,  3.1727e-01,
          1.1507e+00,  1.0018e+00,  8.4466e-01,  7.3183e-01,  3.4909e-01,
          3.6404e-01,  1.7604e-01,  1.2794e-01,  4.1800e-02,  1.3164e-01,
         -3.1432e-01, -7.7208e-01, -1.0165e+00, -8.8368e-01, -8.1404e-02,
         -3.5384e-02, -3.4980e-01, -4.8174e-01, -3.8443e-01, -1.3601e-01],
        [-5.7992e-01, -8.4427e-01, -2.4332e-01, -5.1529e-01, -9.0952e-01,
         -1.0526e+00, -9.6610e-01, -8.7004e-01, -2.6002e-01, -1.2981e+00,
         -1.0798e+00, -7.2673e-01, -1.9258e+00, -4.7536e-04,  1.7201e-01,
          7.0648e-01, -1.6548e+00, -1.4202e+00, -5.9105e-01,  2.1278e-02,
          3.3533e-01,  4.2258e-01, -3.6385e-01, -4.6179e-01,  7.7179e-01,
         -5.3694e-01, -2.3311e-01, -1.2051e-02, -2.4571e-01, -6.9289e-02,
         -7.5207e-01, -2.1902e-01, -8.9040e-02, -1.2152e+00, -3.9482e-01,
         -4.3549e-01,  3.8505e-03, -8.1884e-01, -4.0612e-01,  2.9363e-01,
         -2.3354e-01, -5.5807e-01,  5.1385e-01,  1.3119e+00, -3.2118e-01,
         -3.2336e-01, -2.1493e-01, -9.5764e-01, -1.6028e+00, -9.5227e-01,
          8.2379e-01,  1.1199e+00,  8.1640e-01,  6.9969e-01, -1.4982e-02,
          7.3903e-02, -8.1931e-02,  7.1215e-01,  5.4988e-02,  1.9626e-01,
          4.7303e-01,  2.0132e-01, -5.1956e-01,  1.2059e+00,  1.4252e+00,
         -4.1038e-01, -2.1962e-01, -2.2985e-02,  3.2787e-01, -3.3019e-01,
          5.2515e-01, -9.1836e-02,  7.4358e-01, -6.4746e-01, -2.7051e-01,
          6.9385e-01,  6.6121e-01,  7.9249e-01,  8.8542e-01,  5.0655e-01,
          4.8015e-01,  4.9827e-01,  8.0846e-01,  8.5996e-01,  2.3975e-01,
          4.0130e-01,  3.1116e-02, -4.6734e-01, -3.3782e-01, -7.2745e-01,
         -4.4438e-02, -8.9404e-01, -1.7005e+00, -1.6038e+00, -1.5765e+00,
         -2.7732e-01, -9.4464e-01, -1.6145e+00, -9.6495e-01, -9.2793e-01],
        [ 1.0090e-01,  6.9203e-02, -1.1975e+00, -1.1491e+00, -1.3130e+00,
         -3.2058e-01, -5.8405e-01,  4.1494e-01,  1.4024e-02, -7.0561e-01,
          5.1774e-01, -1.6865e-01, -8.7285e-01, -2.4879e-01, -3.7615e-01,
         -3.8678e-01, -1.1666e+00, -9.5209e-01, -9.3989e-01, -1.6134e+00,
         -2.5012e-01,  7.8821e-02, -7.1112e-01,  5.6887e-02,  1.0985e-01,
         -7.2581e-01, -9.9221e-01, -8.7399e-01,  2.8490e-01,  2.6454e-02,
         -7.9912e-01, -4.0885e-01,  2.8842e-01,  3.1499e-01, -4.7190e-02,
         -1.2281e+00, -2.5301e-01, -1.8117e-01, -5.6789e-01,  7.6337e-01,
         -6.9595e-01, -6.1055e-01, -3.3591e-02, -4.9581e-01, -4.4501e-01,
         -1.3164e-01, -1.2387e-01, -8.3121e-01, -4.1179e-01, -1.9813e-01,
          5.2023e-01,  2.5193e-01,  1.6449e-01, -8.8396e-02,  2.8212e-01,
          8.4705e-01,  2.1954e-01,  8.1869e-01,  7.8632e-01,  7.7148e-01,
          3.7115e-01,  9.0917e-01,  4.7860e-01,  2.5594e-01,  5.2442e-01,
         -1.3858e-01,  2.3756e-01, -4.9357e-01, -1.5086e-01, -6.8644e-01,
         -1.7612e-01,  3.9976e-01,  7.0464e-02,  1.5302e-02,  1.8773e-01,
         -3.4607e-02,  4.3008e-02,  6.9725e-01,  8.2770e-01,  5.6538e-01,
         -4.5595e-01, -2.3661e-01,  3.4825e-01,  6.2972e-01,  7.2180e-01,
         -2.5129e-01,  5.0557e-02,  7.1096e-01,  9.4414e-01,  1.3449e+00,
         -2.6132e-01,  3.3785e-02,  3.6260e-01,  6.2519e-01,  1.1509e+00,
         -6.8915e-01, -6.3436e-01, -3.8691e-01, -8.3346e-02,  5.3852e-01],
        [-1.4524e+00, -9.2776e-01, -2.9722e-01, -5.6155e-01, -6.3001e-01,
         -5.3486e-01, -5.9415e-02,  3.0371e-01,  1.0603e-01, -3.6488e-01,
         -5.3436e-01,  2.4370e-02, -6.3571e-01, -1.1989e+00, -4.9363e-01,
         -1.2639e+00, -9.7024e-01, -9.4450e-01, -6.3403e-02, -1.0692e+00,
         -2.8229e-01, -3.7552e-01, -4.1187e-01, -9.3573e-01, -2.7530e-01,
          2.0507e-01,  4.1979e-01,  1.3988e-01,  9.9002e-01,  4.1736e-01,
         -1.0213e+00, -9.9128e-01, -1.1727e+00, -3.3487e-01, -9.9211e-01,
         -6.9441e-01, -1.0531e+00, -1.2249e+00, -1.2129e+00, -3.3682e-01,
         -1.3620e+00, -2.0412e+00, -2.2095e+00, -8.1495e-01, -7.3134e-01,
          4.5397e-01,  7.5355e-01, -6.0585e-01, -6.1185e-03,  3.5858e-01,
          1.0566e+00,  1.0104e+00,  1.8553e-01, -3.8345e-02,  2.3696e-01,
          6.0776e-01,  6.1727e-01,  5.9959e-01,  5.2385e-01,  9.1349e-01,
         -4.6988e-01,  1.5223e-01,  5.8891e-01, -1.2498e-01, -4.6342e-01,
          1.5765e-01, -8.9012e-01,  3.6718e-01,  1.7816e-01, -4.0856e-01,
         -2.2510e-01, -3.5506e-01, -4.0606e-01, -6.0584e-01,  1.1457e-02,
          1.3437e-01, -7.2434e-03,  4.2793e-02, -4.3738e-01, -7.8285e-01,
          3.9761e-01,  4.5867e-01,  2.4057e-01, -6.1907e-01, -1.0008e+00,
          2.4305e-02,  6.9722e-02, -2.8504e-02, -4.9503e-01, -7.8491e-01,
         -3.3163e-01, -3.9972e-02, -1.1748e-01, -6.7083e-01, -9.0089e-01,
         -6.9382e-01, -5.8842e-01, -5.7840e-01, -4.2852e-01, -4.3248e-01],
        [ 3.9630e-01, -3.6514e-01,  4.4691e-01,  1.8122e-01, -8.3847e-01,
         -5.8539e-01, -1.2003e+00, -2.4542e-01,  1.2481e-01,  3.3883e-01,
         -3.4751e-01, -1.6114e-01, -7.4477e-01, -4.2642e-03,  3.7401e-01,
         -7.1234e-01, -5.3003e-01, -1.1764e+00, -1.8613e+00,  1.9987e-01,
         -6.9008e-01, -1.0607e+00, -1.2753e+00, -4.0644e-01, -7.4829e-01,
         -7.0392e-01, -8.7680e-01, -1.3694e-01, -1.3051e+00, -7.0459e-01,
         -1.0495e-01, -2.5750e-01, -5.2136e-01, -1.2512e+00, -7.3771e-01,
          1.1001e-01, -1.1275e+00, -2.7610e-01,  3.1846e-01, -6.8464e-01,
         -6.2571e-01, -7.8795e-01, -1.5064e+00,  1.7383e-01, -7.3315e-01,
         -7.2315e-01, -6.6471e-01, -5.6416e-02, -3.1971e-01, -6.9668e-02,
          2.0829e-01, -1.3392e-01, -6.9735e-02,  7.1235e-01,  5.7211e-02,
         -1.9429e-01, -8.9910e-01, -4.2677e-01, -9.5110e-01,  3.7283e-01,
         -9.9712e-02,  1.6308e-01,  1.7431e-01, -7.5532e-01, -1.4109e-01,
         -4.7071e-01, -3.2814e-01, -1.6319e-01, -1.1565e-01, -6.6369e-01,
          2.0167e-01, -8.4506e-01, -1.1547e+00,  1.6476e-02,  3.8301e-02,
         -5.9483e-01, -3.6633e-01, -3.8354e-01, -2.2740e-01, -4.5984e-01,
         -9.4696e-02, -1.4353e-02, -4.3465e-01, -4.1961e-01, -9.8580e-01,
          1.7994e-01, -2.2535e-01, -8.9221e-01, -1.0956e+00, -1.1858e+00,
          9.5655e-01,  2.5783e-01,  4.5380e-02,  7.0192e-02, -1.6393e-01,
          1.1694e+00,  1.2172e+00,  1.1630e+00,  1.0510e+00,  1.3059e-01],
        [-5.0701e-01,  1.4517e-01, -5.1606e-01,  1.7636e-01,  9.9910e-01,
          1.7638e-01, -1.0707e-01, -4.7127e-01, -1.5188e+00, -3.0054e-01,
         -7.8486e-01, -1.7144e-01,  1.9125e-01,  2.0791e-03,  5.4684e-01,
         -7.5718e-01, -1.0235e+00, -8.3526e-01, -1.1293e+00,  6.0402e-02,
         -8.0068e-01,  1.1017e-01, -7.3878e-01, -2.0886e-03,  8.4452e-01,
         -6.6486e-01, -1.1061e-01, -7.3605e-01, -6.1279e-01, -9.8118e-01,
         -7.2873e-02,  3.0227e-01, -5.2204e-01, -4.0553e-01, -3.6055e-02,
         -4.2841e-01, -2.1198e-01, -7.4989e-01, -2.8154e-01, -1.0016e+00,
         -6.2542e-01, -8.0284e-01, -1.7452e+00, -9.5821e-01, -8.7166e-01,
         -9.6569e-01, -1.3591e+00, -4.0658e-01, -1.4398e+00, -5.2718e-01,
         -6.1333e-01,  4.4181e-01,  1.0663e+00,  8.1104e-01,  1.0543e+00,
          2.8825e-01,  7.0120e-02, -2.0164e-02,  3.7920e-01, -7.8807e-01,
         -3.5800e-01, -6.5786e-01,  9.2599e-01,  3.5947e-01,  1.1129e+00,
          2.3659e-01,  4.7878e-01,  2.9461e-01, -2.2932e-01, -7.9300e-02,
         -2.4580e-01, -1.3815e+00,  7.5794e-02,  7.9748e-01,  7.4583e-01,
         -4.7723e-01, -6.2762e-01, -6.7950e-01, -4.1097e-01, -9.7652e-02,
         -3.6005e-01, -4.2358e-01, -3.2584e-01,  6.2348e-03,  2.0102e-01,
         -3.7779e-01, -2.2505e-01,  1.6816e-01,  5.5532e-01,  5.0362e-01,
          4.4376e-02,  2.4537e-01,  7.8805e-01,  1.1146e+00,  5.1574e-01,
         -1.2268e-01,  1.2727e-01,  4.9407e-01,  8.8609e-01,  2.3011e-01],
        [-5.2819e-01, -3.1025e-01, -2.5270e-01, -4.4155e-02, -2.8442e-01,
         -9.1192e-01,  1.5578e-01, -1.2330e-01, -7.1029e-01, -4.4220e-01,
         -6.8246e-02,  1.9271e-02, -7.4509e-01, -9.8779e-01, -7.8005e-01,
         -1.2109e+00, -5.5750e-01, -1.2438e+00, -1.6892e+00, -1.8444e-01,
         -1.9693e+00, -3.5465e-01, -8.0291e-01, -8.3644e-01,  1.3017e-01,
         -4.4019e-01, -3.7947e-01, -6.5601e-01,  6.9390e-03,  6.0756e-01,
         -2.7033e-01,  5.8260e-01, -6.9402e-02, -3.1464e-01, -6.8202e-01,
         -3.3974e-01, -5.2252e-01, -2.4553e-01, -2.5868e-01, -1.5515e+00,
         -9.2404e-01,  1.9763e-01, -1.6407e-01, -3.8920e-01, -8.1599e-01,
         -5.8819e-01, -3.8011e-01, -4.4254e-01, -9.7797e-01, -2.3218e-02,
          1.2009e+00, -3.8978e-02,  1.0880e-01,  3.6557e-01, -4.5192e-02,
          9.8606e-01, -3.4118e-01, -2.8530e-01,  3.6504e-01,  1.8553e-01,
          8.0680e-02,  2.5967e-03, -3.9188e-02,  6.3891e-01,  7.2911e-01,
         -7.3518e-01, -2.5410e-01,  2.0089e-01,  1.5202e-01, -5.0759e-01,
          8.7766e-01,  8.3124e-01,  4.6099e-01, -3.0719e-01,  1.1925e-01,
         -2.8458e-01,  4.1157e-04, -2.5410e-01,  1.7199e-01,  2.7552e-01,
         -2.1850e-01,  4.7840e-02, -4.0183e-01,  2.2114e-01,  3.3657e-01,
          4.1371e-01,  9.2063e-01,  3.7852e-01,  3.1305e-01,  3.1516e-01,
          1.0029e+00,  1.6365e+00,  8.4915e-01,  7.0052e-01,  5.3726e-01,
          6.2757e-01,  1.2253e+00,  8.1786e-01,  7.3416e-01,  9.2430e-01],
        [ 2.7092e-01,  1.2083e-01, -4.2318e-02,  4.2143e-01, -5.4818e-01,
          1.3462e-01, -6.9839e-03, -9.2890e-01, -2.7977e-01, -8.3122e-02,
         -6.0034e-01, -8.6224e-01, -4.5398e-01,  6.4230e-01, -6.1109e-01,
         -1.3076e+00, -1.1993e+00, -1.6115e+00, -5.6400e-01, -1.4330e+00,
          2.1635e-01, -8.6249e-01,  1.7556e-01, -1.1113e+00,  9.3302e-02,
         -3.1658e-01, -9.8442e-01, -1.1031e+00, -1.4371e+00, -1.0063e+00,
         -1.4150e+00, -8.3582e-01, -5.4856e-01, -8.4934e-01,  1.0268e-01,
         -1.3892e+00,  4.1577e-01, -3.8356e-01, -1.0371e+00, -3.2053e-01,
         -1.3933e+00,  2.3875e-03,  2.8546e-01, -6.2977e-01, -1.7568e+00,
          5.4228e-01, -2.3079e-01,  3.5220e-01, -2.8378e-01,  7.0997e-01,
         -3.0120e-01, -5.1091e-02,  4.8418e-01,  2.0549e-01,  5.3307e-01,
         -2.0692e-01,  7.8401e-02, -4.3287e-01,  5.3196e-01,  4.2325e-01,
          1.1947e-01, -2.1555e-02, -1.4684e-01,  2.3385e-01,  9.0929e-01,
          3.3153e-01, -3.3219e-01,  2.8329e-01, -4.1315e-01,  5.4093e-01,
         -1.3533e-01,  3.7059e-01, -7.4088e-01, -5.9059e-01,  4.5769e-01,
          5.9262e-01,  1.3608e+00,  1.4911e+00,  6.2283e-01, -1.7097e-01,
          2.7734e-01,  1.3281e+00,  1.3819e+00,  3.9907e-01, -7.1308e-01,
         -5.9579e-01,  3.9071e-01,  6.1178e-01, -1.5928e-01, -1.1891e+00,
         -7.7743e-01, -5.9311e-02,  2.3938e-01, -3.9161e-01, -1.3599e+00,
         -3.0667e-01,  2.9218e-01,  5.5706e-01,  1.1684e-01, -1.8464e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.2252e+00, 1.7712e+00, 2.2822e+00, 3.9132e-01, 1.5735e+00, 8.0050e-01,
         1.1557e+00, 1.1234e+00, 5.2742e-01, 1.7518e+00, 9.2578e-01, 1.3857e+00,
         1.7927e+00, 4.7327e-01, 8.9979e-01, 2.1166e+00, 2.2582e+00, 2.0546e+00,
         1.8298e+00, 1.7472e+00, 1.2847e+00, 2.2426e+00, 2.8645e+00, 1.2120e+00,
         1.3997e+00, 1.9753e+00, 1.3371e+00, 1.6946e+00, 1.9333e+00, 1.5719e+00,
         1.6126e+00, 1.2840e+00, 2.5237e+00, 1.9090e+00, 1.9473e+00, 1.2445e+00,
         3.6057e-01, 1.5399e+00, 1.7934e+00, 2.4677e+00, 1.0888e+00, 1.6608e+00,
         1.9352e+00, 1.8086e+00, 2.7714e+00, 1.5546e+00, 1.4664e+00, 2.1909e+00,
         1.3125e+00, 2.1485e+00, 8.1990e-01, 8.7864e-01, 0.0000e+00, 7.8364e-01,
         7.7265e-01, 1.6110e+00, 1.4659e+00, 1.6235e+00, 1.6405e+00, 7.0170e-01,
         5.9443e-01, 1.0969e+00, 1.3714e+00, 1.7891e+00, 1.2387e+00, 5.6640e-01,
         1.2214e+00, 5.0199e-01, 6.9865e-01, 6.5923e-04, 7.3218e-01, 1.1119e+00,
         1.8411e+00, 1.4885e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7683e-02,
         3.1489e-01, 6.8273e-01, 0.0000e+00, 0.0000e+00, 1.5534e-01, 2.6817e-01,
         6.5091e-01, 6.3596e-01, 8.2396e-01, 8.7206e-01, 9.5820e-01, 8.6836e-01,
         1.3143e+00, 1.7721e+00, 2.0165e+00, 1.8837e+00, 1.0814e+00, 1.0354e+00,
         1.3498e+00, 1.4817e+00, 1.3844e+00, 1.1360e+00],
        [1.5799e+00, 1.8443e+00, 1.2433e+00, 1.5153e+00, 1.9095e+00, 2.0526e+00,
         1.9661e+00, 1.8700e+00, 1.2600e+00, 2.2981e+00, 2.0798e+00, 1.7267e+00,
         2.9258e+00, 1.0005e+00, 8.2799e-01, 2.9352e-01, 2.6548e+00, 2.4202e+00,
         1.5911e+00, 9.7872e-01, 6.6467e-01, 5.7742e-01, 1.3639e+00, 1.4618e+00,
         2.2821e-01, 1.5369e+00, 1.2331e+00, 1.0121e+00, 1.2457e+00, 1.0693e+00,
         1.7521e+00, 1.2190e+00, 1.0890e+00, 2.2152e+00, 1.3948e+00, 1.4355e+00,
         9.9615e-01, 1.8188e+00, 1.4061e+00, 7.0637e-01, 1.2335e+00, 1.5581e+00,
         4.8615e-01, 0.0000e+00, 1.3212e+00, 1.3234e+00, 1.2149e+00, 1.9576e+00,
         2.6028e+00, 1.9523e+00, 1.7621e-01, 0.0000e+00, 1.8360e-01, 3.0031e-01,
         1.0150e+00, 9.2610e-01, 1.0819e+00, 2.8785e-01, 9.4501e-01, 8.0374e-01,
         5.2697e-01, 7.9868e-01, 1.5196e+00, 0.0000e+00, 0.0000e+00, 1.4104e+00,
         1.2196e+00, 1.0230e+00, 6.7213e-01, 1.3302e+00, 4.7485e-01, 1.0918e+00,
         2.5642e-01, 1.6475e+00, 1.2705e+00, 3.0615e-01, 3.3879e-01, 2.0751e-01,
         1.1458e-01, 4.9345e-01, 5.1985e-01, 5.0173e-01, 1.9154e-01, 1.4004e-01,
         7.6025e-01, 5.9870e-01, 9.6888e-01, 1.4673e+00, 1.3378e+00, 1.7274e+00,
         1.0444e+00, 1.8940e+00, 2.7005e+00, 2.6038e+00, 2.5765e+00, 1.2773e+00,
         1.9446e+00, 2.6145e+00, 1.9649e+00, 1.9279e+00],
        [8.9910e-01, 9.3080e-01, 2.1975e+00, 2.1491e+00, 2.3130e+00, 1.3206e+00,
         1.5841e+00, 5.8506e-01, 9.8598e-01, 1.7056e+00, 4.8226e-01, 1.1687e+00,
         1.8728e+00, 1.2488e+00, 1.3762e+00, 1.3868e+00, 2.1666e+00, 1.9521e+00,
         1.9399e+00, 2.6134e+00, 1.2501e+00, 9.2118e-01, 1.7111e+00, 9.4311e-01,
         8.9015e-01, 1.7258e+00, 1.9922e+00, 1.8740e+00, 7.1510e-01, 9.7355e-01,
         1.7991e+00, 1.4088e+00, 7.1158e-01, 6.8501e-01, 1.0472e+00, 2.2281e+00,
         1.2530e+00, 1.1812e+00, 1.5679e+00, 2.3663e-01, 1.6960e+00, 1.6105e+00,
         1.0336e+00, 1.4958e+00, 1.4450e+00, 1.1316e+00, 1.1239e+00, 1.8312e+00,
         1.4118e+00, 1.1981e+00, 4.7977e-01, 7.4807e-01, 8.3551e-01, 1.0884e+00,
         7.1788e-01, 1.5295e-01, 7.8046e-01, 1.8131e-01, 2.1368e-01, 2.2852e-01,
         6.2885e-01, 9.0831e-02, 5.2140e-01, 7.4406e-01, 4.7558e-01, 1.1386e+00,
         7.6244e-01, 1.4936e+00, 1.1509e+00, 1.6864e+00, 1.1761e+00, 6.0024e-01,
         9.2954e-01, 9.8470e-01, 8.1227e-01, 1.0346e+00, 9.5699e-01, 3.0275e-01,
         1.7230e-01, 4.3462e-01, 1.4559e+00, 1.2366e+00, 6.5175e-01, 3.7028e-01,
         2.7820e-01, 1.2513e+00, 9.4944e-01, 2.8904e-01, 5.5857e-02, 0.0000e+00,
         1.2613e+00, 9.6622e-01, 6.3740e-01, 3.7481e-01, 0.0000e+00, 1.6891e+00,
         1.6344e+00, 1.3869e+00, 1.0833e+00, 4.6148e-01],
        [2.4524e+00, 1.9278e+00, 1.2972e+00, 1.5616e+00, 1.6300e+00, 1.5349e+00,
         1.0594e+00, 6.9629e-01, 8.9397e-01, 1.3649e+00, 1.5344e+00, 9.7563e-01,
         1.6357e+00, 2.1989e+00, 1.4936e+00, 2.2639e+00, 1.9702e+00, 1.9445e+00,
         1.0634e+00, 2.0692e+00, 1.2823e+00, 1.3755e+00, 1.4119e+00, 1.9357e+00,
         1.2753e+00, 7.9493e-01, 5.8021e-01, 8.6012e-01, 9.9806e-03, 5.8264e-01,
         2.0213e+00, 1.9913e+00, 2.1727e+00, 1.3349e+00, 1.9921e+00, 1.6944e+00,
         2.0531e+00, 2.2249e+00, 2.2129e+00, 1.3368e+00, 2.3620e+00, 3.0412e+00,
         3.2095e+00, 1.8150e+00, 1.7313e+00, 5.4603e-01, 2.4645e-01, 1.6059e+00,
         1.0061e+00, 6.4142e-01, 0.0000e+00, 0.0000e+00, 8.1447e-01, 1.0383e+00,
         7.6304e-01, 3.9224e-01, 3.8273e-01, 4.0041e-01, 4.7615e-01, 8.6506e-02,
         1.4699e+00, 8.4777e-01, 4.1109e-01, 1.1250e+00, 1.4634e+00, 8.4235e-01,
         1.8901e+00, 6.3282e-01, 8.2184e-01, 1.4086e+00, 1.2251e+00, 1.3551e+00,
         1.4061e+00, 1.6058e+00, 9.8854e-01, 8.6563e-01, 1.0072e+00, 9.5721e-01,
         1.4374e+00, 1.7828e+00, 6.0239e-01, 5.4133e-01, 7.5943e-01, 1.6191e+00,
         2.0008e+00, 9.7570e-01, 9.3028e-01, 1.0285e+00, 1.4950e+00, 1.7849e+00,
         1.3316e+00, 1.0400e+00, 1.1175e+00, 1.6708e+00, 1.9009e+00, 1.6938e+00,
         1.5884e+00, 1.5784e+00, 1.4285e+00, 1.4325e+00],
        [6.0370e-01, 1.3651e+00, 5.5309e-01, 8.1878e-01, 1.8385e+00, 1.5854e+00,
         2.2003e+00, 1.2454e+00, 8.7519e-01, 6.6117e-01, 1.3475e+00, 1.1611e+00,
         1.7448e+00, 1.0043e+00, 6.2599e-01, 1.7123e+00, 1.5300e+00, 2.1764e+00,
         2.8613e+00, 8.0013e-01, 1.6901e+00, 2.0607e+00, 2.2753e+00, 1.4064e+00,
         1.7483e+00, 1.7039e+00, 1.8768e+00, 1.1369e+00, 2.3051e+00, 1.7046e+00,
         1.1050e+00, 1.2575e+00, 1.5214e+00, 2.2512e+00, 1.7377e+00, 8.8999e-01,
         2.1275e+00, 1.2761e+00, 6.8154e-01, 1.6846e+00, 1.6257e+00, 1.7880e+00,
         2.5064e+00, 8.2617e-01, 1.7332e+00, 1.7231e+00, 1.6647e+00, 1.0564e+00,
         1.3197e+00, 1.0697e+00, 7.9171e-01, 1.1339e+00, 1.0697e+00, 2.8765e-01,
         9.4279e-01, 1.1943e+00, 1.8991e+00, 1.4268e+00, 1.9511e+00, 6.2717e-01,
         1.0997e+00, 8.3692e-01, 8.2569e-01, 1.7553e+00, 1.1411e+00, 1.4707e+00,
         1.3281e+00, 1.1632e+00, 1.1157e+00, 1.6637e+00, 7.9833e-01, 1.8451e+00,
         2.1547e+00, 9.8352e-01, 9.6170e-01, 1.5948e+00, 1.3663e+00, 1.3835e+00,
         1.2274e+00, 1.4598e+00, 1.0947e+00, 1.0144e+00, 1.4347e+00, 1.4196e+00,
         1.9858e+00, 8.2006e-01, 1.2253e+00, 1.8922e+00, 2.0956e+00, 2.1858e+00,
         4.3446e-02, 7.4217e-01, 9.5462e-01, 9.2981e-01, 1.1639e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6941e-01],
        [1.5070e+00, 8.5483e-01, 1.5161e+00, 8.2364e-01, 9.0098e-04, 8.2362e-01,
         1.1071e+00, 1.4713e+00, 2.5188e+00, 1.3005e+00, 1.7849e+00, 1.1714e+00,
         8.0875e-01, 9.9792e-01, 4.5316e-01, 1.7572e+00, 2.0235e+00, 1.8353e+00,
         2.1293e+00, 9.3960e-01, 1.8007e+00, 8.8983e-01, 1.7388e+00, 1.0021e+00,
         1.5548e-01, 1.6649e+00, 1.1106e+00, 1.7360e+00, 1.6128e+00, 1.9812e+00,
         1.0729e+00, 6.9773e-01, 1.5220e+00, 1.4055e+00, 1.0361e+00, 1.4284e+00,
         1.2120e+00, 1.7499e+00, 1.2815e+00, 2.0016e+00, 1.6254e+00, 1.8028e+00,
         2.7452e+00, 1.9582e+00, 1.8717e+00, 1.9657e+00, 2.3591e+00, 1.4066e+00,
         2.4398e+00, 1.5272e+00, 1.6133e+00, 5.5819e-01, 0.0000e+00, 1.8896e-01,
         0.0000e+00, 7.1175e-01, 9.2988e-01, 1.0202e+00, 6.2080e-01, 1.7881e+00,
         1.3580e+00, 1.6579e+00, 7.4010e-02, 6.4053e-01, 0.0000e+00, 7.6341e-01,
         5.2122e-01, 7.0539e-01, 1.2293e+00, 1.0793e+00, 1.2458e+00, 2.3815e+00,
         9.2421e-01, 2.0252e-01, 2.5417e-01, 1.4772e+00, 1.6276e+00, 1.6795e+00,
         1.4110e+00, 1.0977e+00, 1.3601e+00, 1.4236e+00, 1.3258e+00, 9.9377e-01,
         7.9898e-01, 1.3778e+00, 1.2251e+00, 8.3184e-01, 4.4468e-01, 4.9638e-01,
         9.5562e-01, 7.5463e-01, 2.1195e-01, 0.0000e+00, 4.8426e-01, 1.1227e+00,
         8.7273e-01, 5.0593e-01, 1.1391e-01, 7.6989e-01],
        [1.5282e+00, 1.3103e+00, 1.2527e+00, 1.0442e+00, 1.2844e+00, 1.9119e+00,
         8.4422e-01, 1.1233e+00, 1.7103e+00, 1.4422e+00, 1.0682e+00, 9.8073e-01,
         1.7451e+00, 1.9878e+00, 1.7800e+00, 2.2109e+00, 1.5575e+00, 2.2438e+00,
         2.6892e+00, 1.1844e+00, 2.9693e+00, 1.3547e+00, 1.8029e+00, 1.8364e+00,
         8.6983e-01, 1.4402e+00, 1.3795e+00, 1.6560e+00, 9.9306e-01, 3.9244e-01,
         1.2703e+00, 4.1740e-01, 1.0694e+00, 1.3146e+00, 1.6820e+00, 1.3397e+00,
         1.5225e+00, 1.2455e+00, 1.2587e+00, 2.5515e+00, 1.9240e+00, 8.0237e-01,
         1.1641e+00, 1.3892e+00, 1.8160e+00, 1.5882e+00, 1.3801e+00, 1.4425e+00,
         1.9780e+00, 1.0232e+00, 0.0000e+00, 1.0390e+00, 8.9120e-01, 6.3443e-01,
         1.0452e+00, 1.3935e-02, 1.3412e+00, 1.2853e+00, 6.3496e-01, 8.1447e-01,
         9.1932e-01, 9.9740e-01, 1.0392e+00, 3.6109e-01, 2.7089e-01, 1.7352e+00,
         1.2541e+00, 7.9911e-01, 8.4798e-01, 1.5076e+00, 1.2234e-01, 1.6876e-01,
         5.3901e-01, 1.3072e+00, 8.8075e-01, 1.2846e+00, 9.9959e-01, 1.2541e+00,
         8.2801e-01, 7.2448e-01, 1.2185e+00, 9.5216e-01, 1.4018e+00, 7.7886e-01,
         6.6343e-01, 5.8629e-01, 7.9370e-02, 6.2148e-01, 6.8695e-01, 6.8484e-01,
         0.0000e+00, 0.0000e+00, 1.5085e-01, 2.9948e-01, 4.6274e-01, 3.7243e-01,
         0.0000e+00, 1.8214e-01, 2.6584e-01, 7.5702e-02],
        [7.2908e-01, 8.7917e-01, 1.0423e+00, 5.7857e-01, 1.5482e+00, 8.6538e-01,
         1.0070e+00, 1.9289e+00, 1.2798e+00, 1.0831e+00, 1.6003e+00, 1.8622e+00,
         1.4540e+00, 3.5770e-01, 1.6111e+00, 2.3076e+00, 2.1993e+00, 2.6115e+00,
         1.5640e+00, 2.4330e+00, 7.8365e-01, 1.8625e+00, 8.2444e-01, 2.1113e+00,
         9.0670e-01, 1.3166e+00, 1.9844e+00, 2.1031e+00, 2.4371e+00, 2.0063e+00,
         2.4150e+00, 1.8358e+00, 1.5486e+00, 1.8493e+00, 8.9732e-01, 2.3892e+00,
         5.8423e-01, 1.3836e+00, 2.0371e+00, 1.3205e+00, 2.3933e+00, 9.9761e-01,
         7.1454e-01, 1.6298e+00, 2.7568e+00, 4.5772e-01, 1.2308e+00, 6.4780e-01,
         1.2838e+00, 2.9003e-01, 1.3012e+00, 1.0511e+00, 5.1582e-01, 7.9451e-01,
         4.6693e-01, 1.2069e+00, 9.2160e-01, 1.4329e+00, 4.6804e-01, 5.7675e-01,
         8.8053e-01, 1.0216e+00, 1.1468e+00, 7.6615e-01, 9.0707e-02, 6.6847e-01,
         1.3322e+00, 7.1671e-01, 1.4131e+00, 4.5907e-01, 1.1353e+00, 6.2941e-01,
         1.7409e+00, 1.5906e+00, 5.4231e-01, 4.0738e-01, 0.0000e+00, 0.0000e+00,
         3.7717e-01, 1.1710e+00, 7.2266e-01, 0.0000e+00, 0.0000e+00, 6.0093e-01,
         1.7131e+00, 1.5958e+00, 6.0929e-01, 3.8822e-01, 1.1593e+00, 2.1891e+00,
         1.7774e+00, 1.0593e+00, 7.6062e-01, 1.3916e+00, 2.3599e+00, 1.3067e+00,
         7.0782e-01, 4.4294e-01, 8.8316e-01, 1.1846e+00]], device='cuda:0',
       grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2030, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.0559, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.0559, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-9.9918e-01, -3.2458e-01, -1.1584e+00, -9.4811e-01, -6.5634e-01,
         -1.0623e+00,  3.6167e-01, -8.7979e-01, -6.9690e-01, -1.0966e+00,
         -1.7344e+00, -1.7128e+00, -1.0413e+00, -7.5052e-01,  1.1356e-01,
         -1.5071e+00, -1.8632e+00,  3.3744e-01, -3.1394e-01, -6.4098e-02,
         -6.6688e-01, -9.6480e-01, -1.6677e-01,  5.0864e-01, -1.4184e+00,
          1.0314e-01,  9.6206e-01, -2.5389e-01,  1.0426e-01, -6.7891e-01,
         -2.8605e-01, -2.1477e-01, -6.3643e-01, -7.4108e-01, -6.9714e-01,
         -6.4297e-01, -1.2038e+00, -1.0078e+00, -2.1976e-01, -7.4230e-01,
         -1.9247e+00,  3.9534e-01,  9.9286e-01,  7.0257e-01,  1.8379e-01,
         -1.0844e+00, -1.7104e-01, -1.4087e+00, -4.5608e-01, -5.5751e-01,
          8.1133e-01,  8.9819e-01,  4.6860e-01,  8.5534e-01, -5.0803e-01,
          7.9749e-01,  1.1067e+00,  4.6748e-01,  6.9645e-01,  7.6159e-01,
         -3.1795e-01, -8.3813e-01, -1.6888e+00, -9.3168e-01, -1.1656e+00,
          1.8856e+00,  2.1644e-01, -4.9006e-01,  2.6652e-02,  1.4111e-01,
          1.1822e+00,  5.8544e-01, -1.4243e-01, -6.8607e-02,  1.2270e-01,
          2.4229e-01,  5.5272e-01,  9.5235e-01,  9.0722e-01,  8.4763e-01,
          4.1291e-01,  5.8239e-01,  6.4388e-01,  5.3216e-01,  6.2205e-01,
          2.8595e-01,  2.6091e-01,  1.4943e-01,  2.1979e-01,  1.3278e-01,
          5.1291e-02, -5.5638e-01, -8.0627e-01, -6.5325e-01, -4.5508e-01,
         -2.1023e-01, -4.2907e-01, -5.6486e-01, -3.8831e-01, -3.0647e-01],
        [-3.4555e-01, -1.0034e+00, -1.2839e+00,  4.8947e-01, -4.0348e-01,
         -1.0284e+00, -3.5597e-01, -3.7703e-01, -8.2308e-01, -6.9824e-01,
         -6.6359e-02,  2.5263e-01, -7.4468e-01, -1.3567e+00, -5.8124e-01,
         -7.9909e-01, -6.5210e-01, -1.4159e+00, -8.7888e-01, -1.4041e+00,
         -1.3339e+00, -4.0631e-01, -8.6493e-01, -1.2030e+00, -1.2136e+00,
          6.9712e-01, -6.1361e-01,  6.7841e-01,  5.4295e-01, -5.4202e-01,
         -7.9060e-01, -8.5670e-01, -1.1349e+00, -1.2308e+00, -5.6222e-01,
         -4.7087e-01, -5.0352e-01,  2.6732e-03, -5.9543e-01,  3.0081e-01,
         -3.5728e-01,  1.1853e-01, -3.9986e-01, -6.9741e-01, -1.5716e-01,
         -2.2301e-01,  1.6221e-01, -3.2828e-03, -1.2807e-01,  6.2664e-01,
          7.2609e-02,  5.9639e-01, -9.6948e-02,  4.3461e-01, -3.8678e-01,
          1.2122e+00,  5.3472e-01,  2.7492e-01, -2.2787e-02,  5.1354e-01,
          3.6111e-01,  8.9982e-01,  1.0100e+00,  9.5551e-02,  1.2146e-01,
          9.8182e-01,  1.1312e+00, -6.0095e-01, -1.4731e+00, -1.8653e+00,
         -6.7329e-01,  2.3184e-01, -6.6197e-01, -5.7857e-01,  1.1927e+00,
          6.2707e-01,  6.9655e-01,  8.9996e-01,  1.2007e+00,  1.4600e+00,
         -4.0462e-02, -1.1939e-01, -6.7208e-02,  4.6318e-01,  1.0273e+00,
         -8.4506e-02, -2.3862e-01, -3.2467e-02,  3.4097e-01,  5.8552e-01,
          2.6498e-01,  5.0908e-01,  5.2661e-01,  2.1673e-01, -1.1373e-01,
          3.2941e-01,  4.4767e-01,  2.7140e-01, -4.2736e-01, -1.7277e-01],
        [-3.9167e-01, -1.0038e+00, -1.9563e-01, -9.2406e-01,  2.4366e-01,
          3.0378e-01,  8.9428e-01, -2.2602e-03, -3.3625e-01, -3.3157e-01,
         -3.7598e-01,  1.6162e-01, -6.0302e-01, -1.1541e+00, -1.5149e-01,
         -1.0308e+00, -9.5580e-01, -1.5915e+00, -7.8431e-01, -1.1956e+00,
         -2.9414e-01, -1.0513e+00, -9.9757e-01, -8.7068e-01, -3.3522e-01,
         -3.5228e-01, -4.0456e-02,  2.2586e-02, -1.1625e+00, -6.2108e-01,
         -1.0502e+00, -6.9315e-01, -1.1215e+00, -2.4797e-01, -1.1951e+00,
         -5.9830e-01, -1.8663e-01, -8.7678e-01,  2.6980e-01, -3.1733e-01,
         -2.0903e-01,  6.4437e-01, -1.1977e+00, -7.1581e-01, -1.1833e+00,
         -9.0145e-01, -5.7705e-01, -2.2296e-01, -6.9765e-01, -5.6556e-01,
         -3.8822e-01,  3.1063e-01,  8.4846e-01,  3.0982e-01,  7.6724e-02,
          1.5239e+00,  1.1511e+00,  1.2831e+00,  1.7740e+00,  8.9342e-01,
          1.1477e-01, -2.4569e-01,  5.9131e-02,  2.6477e-01,  1.9392e-01,
         -2.1497e-01, -1.0674e-02,  2.6893e-01, -1.3633e-02,  2.0300e-01,
          8.9428e-01,  1.1761e+00,  1.1605e+00,  1.5079e+00,  1.4920e+00,
          2.6950e-02, -5.3480e-02, -5.3803e-02,  4.8907e-01,  6.7528e-01,
          7.2880e-02, -9.7408e-03, -2.6125e-02,  5.5543e-01,  3.4269e-01,
         -3.6389e-01, -6.1997e-01, -4.4063e-01,  2.7574e-01, -1.4996e-01,
         -3.5005e-01, -2.1159e-01,  1.6847e-02,  3.4371e-01, -1.6818e-01,
         -9.8499e-02, -1.1057e-01, -9.6160e-03,  1.8889e-01, -2.7093e-01],
        [-1.6248e-01, -3.2165e-01, -1.1254e+00, -1.8402e-01, -5.1442e-01,
         -3.1724e-01,  6.5747e-01,  6.4227e-01,  5.1016e-01, -6.7644e-01,
         -1.7680e+00, -1.5165e+00, -9.7723e-01, -1.0697e+00, -6.3112e-01,
         -9.4236e-01, -6.1285e-01, -1.2835e+00, -1.4802e+00, -6.8529e-01,
         -8.2077e-01, -6.8304e-01, -3.6987e-01, -1.1479e+00,  2.5854e-01,
          6.4153e-01, -1.1059e-01, -9.4812e-01, -1.8321e+00, -1.0784e+00,
         -6.2206e-01, -6.1636e-01,  2.8558e-01, -6.6651e-01, -9.2771e-01,
         -5.0511e-02, -5.9143e-01, -1.0036e+00, -1.3191e+00, -1.2744e+00,
         -5.3325e-01, -6.9776e-01,  6.2631e-01, -1.2159e+00, -6.7159e-01,
         -2.6523e-01, -5.3332e-01, -3.1770e-02, -6.1159e-01, -5.9129e-01,
          1.5455e+00,  4.5969e-01, -3.4990e-02, -1.0999e+00, -2.5237e-01,
          4.4358e-01,  5.2903e-01,  7.8883e-01,  3.8825e-02, -1.7203e-01,
         -5.0171e-01, -6.8081e-01, -4.0523e-01, -8.2680e-01, -5.5543e-01,
         -7.6108e-01, -3.8986e-01, -3.8361e-01, -2.7922e-01, -1.3493e-01,
          5.8030e-02, -7.5762e-01, -7.8343e-02, -7.7164e-01, -5.7830e-01,
         -2.0384e-01,  4.4375e-02, -5.3795e-01, -5.2108e-01, -3.2790e-01,
          1.3682e+00,  1.7273e+00,  9.1436e-01,  7.3517e-02, -7.1430e-01,
          1.0031e+00,  1.1800e+00,  6.0172e-01, -4.1373e-02, -8.4130e-01,
          1.3624e-01,  3.0611e-01, -3.6124e-02, -4.3345e-01, -7.7843e-01,
          1.0737e-02,  1.4416e-01, -3.8685e-01, -8.2957e-01, -9.2224e-01],
        [-4.5827e-01, -2.2531e-01,  2.9618e-01,  8.2532e-03,  5.6897e-01,
          2.2245e-01, -4.1157e-01, -2.0002e-01, -3.7772e-01,  3.0199e-01,
         -1.1473e+00,  5.3806e-02, -3.2650e-01,  1.4065e-01, -2.8197e-01,
         -4.6416e-01, -2.6911e-01, -6.5784e-01, -1.3724e-01, -1.5792e-02,
         -1.4765e+00, -1.1514e+00, -6.4882e-01, -1.5081e+00, -6.4484e-01,
         -9.2551e-01, -1.1997e-02,  3.7622e-02, -7.0421e-01, -2.0962e-01,
         -1.5666e+00, -1.2729e+00, -1.0450e+00, -5.2297e-01, -4.3781e-01,
         -3.1853e-01, -9.7690e-02, -4.7324e-01, -4.3139e-01, -3.5200e-01,
         -2.4520e-01,  4.1357e-02, -5.4981e-01,  1.9270e-01,  2.3759e-01,
          2.9359e-01, -3.7950e-01, -1.7324e-01, -1.2831e+00, -2.3741e-01,
          1.8448e-01,  1.8421e-01, -7.3097e-02, -3.6375e-01, -2.9124e-01,
          8.3405e-01,  6.9819e-01,  1.9620e-01, -9.6206e-02, -4.9061e-02,
          7.3603e-01,  4.4463e-01,  6.1860e-01,  2.4156e-01,  1.2864e+00,
          8.1252e-01, -5.7227e-01,  9.8003e-01,  1.5053e+00,  5.3807e-01,
          1.2902e+00,  4.7388e-01, -1.1781e-01,  4.5437e-01,  1.0250e+00,
          1.0008e-01, -3.9181e-01, -7.2615e-01, -1.1545e+00, -9.5465e-01,
         -2.4260e-01, -5.7335e-01, -1.3319e+00, -1.8415e+00, -1.4034e+00,
         -4.0502e-01, -3.9357e-01, -8.1858e-01, -1.0796e+00, -9.7376e-01,
         -2.0199e-01, -1.5543e-01, -4.4671e-01, -3.9838e-01, -6.5526e-01,
          2.2003e-01,  1.0836e-01, -1.4573e-01,  3.6383e-01,  5.0123e-01],
        [-8.1891e-01, -1.2050e+00, -3.8688e-01,  1.7317e-01, -1.2215e+00,
         -8.2756e-01, -4.9874e-01,  8.6912e-02,  2.0106e-01,  2.4219e-01,
          5.2682e-02, -1.3419e-01, -4.1075e-01, -1.0960e+00, -1.2646e-01,
         -1.4224e-01, -2.7673e-01, -1.4539e+00, -8.4406e-01, -1.1736e+00,
         -1.2344e+00, -3.7258e-01, -6.7549e-01, -1.6718e+00, -6.1827e-01,
         -3.7147e-01,  2.9200e-01, -1.1947e+00, -1.0502e+00, -5.8810e-01,
         -5.5441e-01, -5.4778e-01, -6.3913e-01, -7.0363e-01, -1.0090e+00,
         -6.8653e-01, -2.8589e-01, -1.5464e-01, -6.7032e-01,  1.9754e-01,
          2.9448e-01,  7.4480e-01, -1.5111e+00, -6.4863e-01, -7.0411e-01,
         -2.9890e-01, -8.9032e-01,  2.5526e-02, -9.5842e-02, -6.2389e-01,
         -1.6754e-01, -2.1961e-01, -6.2562e-01, -2.5172e-01,  6.6822e-01,
         -2.5977e-01,  5.7577e-01, -8.7476e-01, -9.5284e-01, -2.1404e-01,
          7.3942e-01, -8.4175e-01, -4.3303e-01, -7.7735e-01,  3.6700e-01,
         -1.1137e-01,  7.1987e-01, -6.5497e-01, -1.4105e-01, -1.2479e-01,
         -7.7741e-02,  8.1830e-01, -7.9776e-02, -1.3980e+00,  7.9522e-01,
         -4.7626e-01, -8.1477e-02,  7.4040e-01,  9.1819e-01,  9.1916e-01,
         -1.9639e-01,  5.5027e-01,  8.2069e-01,  6.5738e-01,  2.6941e-01,
         -1.2271e-01,  3.5760e-01,  6.8171e-01,  5.7104e-01, -1.2882e-01,
         -1.7630e-01,  4.9645e-02,  9.5508e-02, -2.8399e-01, -8.4689e-01,
          8.6527e-01,  6.3206e-01, -2.8877e-02, -6.7807e-01, -1.5050e+00],
        [-7.9732e-01, -7.8425e-01, -9.2052e-02,  2.5666e-01, -6.1658e-01,
         -6.8876e-01, -5.2679e-01, -1.2402e-01, -1.1029e+00, -1.0001e+00,
         -1.0904e+00, -6.9706e-01, -6.7552e-01, -1.0560e+00, -1.5424e-01,
         -9.1978e-01, -1.2297e+00, -6.5595e-02,  2.9346e-01, -7.8937e-01,
         -7.2308e-01, -1.0725e+00, -2.6256e+00, -1.7192e-01,  9.2976e-01,
         -2.4226e-01, -1.1313e+00, -1.1882e+00, -1.0448e-01, -5.2061e-01,
         -1.1347e+00, -5.8327e-01, -5.7465e-01, -4.7449e-01, -9.9923e-01,
         -6.4847e-01, -2.0045e-01,  6.5841e-01, -4.6891e-01, -7.1810e-01,
         -1.1703e+00, -3.0669e-01, -1.6530e+00,  3.7580e-01, -1.3273e-01,
         -7.3463e-01, -1.1727e-01,  1.2508e+00,  8.8510e-01,  1.2230e+00,
          5.9781e-01, -8.4033e-02,  1.3710e-01, -6.1416e-02,  5.9452e-02,
         -4.1780e-01,  4.1099e-02,  4.1144e-01,  1.9637e-01,  1.1038e-01,
         -5.2640e-01, -2.7502e-01, -2.2852e+00, -1.1206e+00, -3.2045e-01,
          9.0562e-01,  1.1631e+00,  4.3224e-02,  6.3632e-01,  8.0040e-01,
          2.7121e-01,  1.4852e-01,  5.5002e-01, -2.0063e-01, -2.8575e-01,
          3.0533e-01,  6.9411e-02,  1.7180e-01,  6.2625e-01,  6.1944e-01,
          1.3295e-01, -4.6798e-01, -4.4495e-01,  2.6397e-01,  1.0365e-01,
          6.3630e-01, -1.7296e-01, -2.9836e-01, -1.7268e-02, -1.5777e-01,
          1.8383e+00,  9.2167e-01,  2.5660e-01,  3.7285e-01,  3.2892e-02,
          1.8156e+00,  1.6062e+00,  8.5996e-01,  6.3793e-01,  1.3225e-01],
        [-5.5075e-01, -1.6555e+00, -1.0210e+00, -5.2063e-01, -1.1572e+00,
         -9.5322e-01, -7.8632e-01, -1.2600e+00, -3.0229e-01,  9.3894e-01,
         -9.4396e-01, -1.5833e+00, -1.9309e+00, -1.6653e-01,  5.9084e-01,
         -1.1513e+00, -3.4399e-01, -1.0849e+00, -2.0276e+00, -4.9843e-01,
         -8.9194e-01, -3.4360e-01, -8.9264e-01,  8.6300e-02, -9.2090e-01,
         -4.9463e-01,  8.2628e-01,  1.9450e-01, -3.9493e-01, -1.0812e+00,
          8.0202e-02,  2.1113e-01, -2.0256e-01, -1.1941e+00, -9.4284e-01,
         -4.1460e-01, -1.4239e-01,  4.2998e-02, -1.2107e+00, -2.9451e-01,
         -3.6053e-01,  5.3447e-01,  4.5030e-01, -3.0892e-01,  2.4725e-01,
          4.2089e-01, -2.3291e-01, -3.7864e-01,  2.8791e-02,  2.5521e-01,
         -6.7677e-02,  9.1304e-01,  4.4638e-01,  5.5652e-01,  8.3178e-01,
          5.3891e-01,  8.2402e-01,  6.7344e-01,  7.6633e-01,  7.6877e-01,
          1.1247e+00,  7.2171e-01,  8.8053e-01,  7.0439e-01,  6.6595e-01,
         -8.3306e-01, -1.1237e-01,  4.8221e-01,  3.0595e-01,  1.4820e-01,
         -5.5439e-01,  8.0277e-01,  7.8812e-01,  4.8274e-01, -4.4079e-01,
         -7.1122e-01, -9.5928e-01, -1.0902e+00, -9.5153e-01, -1.4504e+00,
          1.4686e-02, -3.5557e-01, -4.1330e-01, -1.6452e-01, -6.1549e-01,
          2.8480e-01,  1.4069e-01, -1.8782e-01,  8.0432e-02, -2.1791e-01,
          4.5327e-01,  6.7201e-01,  2.6257e-01,  1.0710e-01, -3.4044e-02,
          4.6613e-01,  4.4120e-01, -5.8821e-02, -6.3118e-01, -5.0879e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.9992, 1.3246, 2.1584, 1.9481, 1.6563, 2.0623, 0.6383, 1.8798, 1.6969,
         2.0966, 2.7344, 2.7128, 2.0413, 1.7505, 0.8864, 2.5071, 2.8632, 0.6626,
         1.3139, 1.0641, 1.6669, 1.9648, 1.1668, 0.4914, 2.4184, 0.8969, 0.0379,
         1.2539, 0.8957, 1.6789, 1.2860, 1.2148, 1.6364, 1.7411, 1.6971, 1.6430,
         2.2038, 2.0078, 1.2198, 1.7423, 2.9247, 0.6047, 0.0071, 0.2974, 0.8162,
         2.0844, 1.1710, 2.4087, 1.4561, 1.5575, 0.1887, 0.1018, 0.5314, 0.1447,
         1.5080, 0.2025, 0.0000, 0.5325, 0.3035, 0.2384, 1.3180, 1.8381, 2.6888,
         1.9317, 2.1656, 0.0000, 0.7836, 1.4901, 0.9733, 0.8589, 0.0000, 0.4146,
         1.1424, 1.0686, 0.8773, 0.7577, 0.4473, 0.0476, 0.0928, 0.1524, 0.5871,
         0.4176, 0.3561, 0.4678, 0.3779, 0.7140, 0.7391, 0.8506, 0.7802, 0.8672,
         0.9487, 1.5564, 1.8063, 1.6533, 1.4551, 1.2102, 1.4291, 1.5649, 1.3883,
         1.3065],
        [1.3456, 2.0034, 2.2839, 0.5105, 1.4035, 2.0284, 1.3560, 1.3770, 1.8231,
         1.6982, 1.0664, 0.7474, 1.7447, 2.3567, 1.5812, 1.7991, 1.6521, 2.4159,
         1.8789, 2.4041, 2.3339, 1.4063, 1.8649, 2.2030, 2.2136, 0.3029, 1.6136,
         0.3216, 0.4570, 1.5420, 1.7906, 1.8567, 2.1349, 2.2308, 1.5622, 1.4709,
         1.5035, 0.9973, 1.5954, 0.6992, 1.3573, 0.8815, 1.3999, 1.6974, 1.1572,
         1.2230, 0.8378, 1.0033, 1.1281, 0.3734, 0.9274, 0.4036, 1.0969, 0.5654,
         1.3868, 0.0000, 0.4653, 0.7251, 1.0228, 0.4865, 0.6389, 0.1002, 0.0000,
         0.9044, 0.8785, 0.0182, 0.0000, 1.6009, 2.4731, 2.8653, 1.6733, 0.7682,
         1.6620, 1.5786, 0.0000, 0.3729, 0.3034, 0.1000, 0.0000, 0.0000, 1.0405,
         1.1194, 1.0672, 0.5368, 0.0000, 1.0845, 1.2386, 1.0325, 0.6590, 0.4145,
         0.7350, 0.4909, 0.4734, 0.7833, 1.1137, 0.6706, 0.5523, 0.7286, 1.4274,
         1.1728],
        [1.3917, 2.0038, 1.1956, 1.9241, 0.7563, 0.6962, 0.1057, 1.0023, 1.3362,
         1.3316, 1.3760, 0.8384, 1.6030, 2.1541, 1.1515, 2.0308, 1.9558, 2.5915,
         1.7843, 2.1956, 1.2941, 2.0513, 1.9976, 1.8707, 1.3352, 1.3523, 1.0405,
         0.9774, 2.1625, 1.6211, 2.0502, 1.6932, 2.1215, 1.2480, 2.1951, 1.5983,
         1.1866, 1.8768, 0.7302, 1.3173, 1.2090, 0.3556, 2.1977, 1.7158, 2.1833,
         1.9015, 1.5771, 1.2230, 1.6977, 1.5656, 1.3882, 0.6894, 0.1515, 0.6902,
         0.9233, 0.0000, 0.0000, 0.0000, 0.0000, 0.1066, 0.8852, 1.2457, 0.9409,
         0.7352, 0.8061, 1.2150, 1.0107, 0.7311, 1.0136, 0.7970, 0.1057, 0.0000,
         0.0000, 0.0000, 0.0000, 0.9731, 1.0535, 1.0538, 0.5109, 0.3247, 0.9271,
         1.0097, 1.0261, 0.4446, 0.6573, 1.3639, 1.6200, 1.4406, 0.7243, 1.1500,
         1.3500, 1.2116, 0.9832, 0.6563, 1.1682, 1.0985, 1.1106, 1.0096, 0.8111,
         1.2709],
        [1.1625, 1.3217, 2.1254, 1.1840, 1.5144, 1.3172, 0.3425, 0.3577, 0.4898,
         1.6764, 2.7680, 2.5165, 1.9772, 2.0697, 1.6311, 1.9424, 1.6129, 2.2835,
         2.4802, 1.6853, 1.8208, 1.6830, 1.3699, 2.1479, 0.7415, 0.3585, 1.1106,
         1.9481, 2.8321, 2.0784, 1.6221, 1.6164, 0.7144, 1.6665, 1.9277, 1.0505,
         1.5914, 2.0036, 2.3191, 2.2744, 1.5333, 1.6978, 0.3737, 2.2159, 1.6716,
         1.2652, 1.5333, 1.0318, 1.6116, 1.5913, 0.0000, 0.5403, 1.0350, 2.0999,
         1.2524, 0.5564, 0.4710, 0.2112, 0.9612, 1.1720, 1.5017, 1.6808, 1.4052,
         1.8268, 1.5554, 1.7611, 1.3899, 1.3836, 1.2792, 1.1349, 0.9420, 1.7576,
         1.0783, 1.7716, 1.5783, 1.2038, 0.9556, 1.5380, 1.5211, 1.3279, 0.0000,
         0.0000, 0.0856, 0.9265, 1.7143, 0.0000, 0.0000, 0.3983, 1.0414, 1.8413,
         0.8638, 0.6939, 1.0361, 1.4334, 1.7784, 0.9893, 0.8558, 1.3868, 1.8296,
         1.9222],
        [1.4583, 1.2253, 0.7038, 0.9917, 0.4310, 0.7775, 1.4116, 1.2000, 1.3777,
         0.6980, 2.1473, 0.9462, 1.3265, 0.8594, 1.2820, 1.4642, 1.2691, 1.6578,
         1.1372, 1.0158, 2.4765, 2.1514, 1.6488, 2.5081, 1.6448, 1.9255, 1.0120,
         0.9624, 1.7042, 1.2096, 2.5666, 2.2729, 2.0450, 1.5230, 1.4378, 1.3185,
         1.0977, 1.4732, 1.4314, 1.3520, 1.2452, 0.9586, 1.5498, 0.8073, 0.7624,
         0.7064, 1.3795, 1.1732, 2.2831, 1.2374, 0.8155, 0.8158, 1.0731, 1.3638,
         1.2912, 0.1659, 0.3018, 0.8038, 1.0962, 1.0491, 0.2640, 0.5554, 0.3814,
         0.7584, 0.0000, 0.1875, 1.5723, 0.0200, 0.0000, 0.4619, 0.0000, 0.5261,
         1.1178, 0.5456, 0.0000, 0.8999, 1.3918, 1.7262, 2.1545, 1.9546, 1.2426,
         1.5733, 2.3319, 2.8415, 2.4034, 1.4050, 1.3936, 1.8186, 2.0796, 1.9738,
         1.2020, 1.1554, 1.4467, 1.3984, 1.6553, 0.7800, 0.8916, 1.1457, 0.6362,
         0.4988],
        [1.8189, 2.2050, 1.3869, 0.8268, 2.2215, 1.8276, 1.4987, 0.9131, 0.7989,
         0.7578, 0.9473, 1.1342, 1.4107, 2.0960, 1.1265, 1.1422, 1.2767, 2.4539,
         1.8441, 2.1736, 2.2344, 1.3726, 1.6755, 2.6718, 1.6183, 1.3715, 0.7080,
         2.1947, 2.0502, 1.5881, 1.5544, 1.5478, 1.6391, 1.7036, 2.0090, 1.6865,
         1.2859, 1.1546, 1.6703, 0.8025, 0.7055, 0.2552, 2.5111, 1.6486, 1.7041,
         1.2989, 1.8903, 0.9745, 1.0958, 1.6239, 1.1675, 1.2196, 1.6256, 1.2517,
         0.3318, 1.2598, 0.4242, 1.8748, 1.9528, 1.2140, 0.2606, 1.8418, 1.4330,
         1.7774, 0.6330, 1.1114, 0.2801, 1.6550, 1.1411, 1.1248, 1.0777, 0.1817,
         1.0798, 2.3980, 0.2048, 1.4763, 1.0815, 0.2596, 0.0818, 0.0808, 1.1964,
         0.4497, 0.1793, 0.3426, 0.7306, 1.1227, 0.6424, 0.3183, 0.4290, 1.1288,
         1.1763, 0.9504, 0.9045, 1.2840, 1.8469, 0.1347, 0.3679, 1.0289, 1.6781,
         2.5050],
        [1.7973, 1.7843, 1.0921, 0.7433, 1.6166, 1.6888, 1.5268, 1.1240, 2.1029,
         2.0001, 2.0904, 1.6971, 1.6755, 2.0560, 1.1542, 1.9198, 2.2297, 1.0656,
         0.7065, 1.7894, 1.7231, 2.0725, 3.6256, 1.1719, 0.0702, 1.2423, 2.1313,
         2.1882, 1.1045, 1.5206, 2.1347, 1.5833, 1.5746, 1.4745, 1.9992, 1.6485,
         1.2004, 0.3416, 1.4689, 1.7181, 2.1703, 1.3067, 2.6530, 0.6242, 1.1327,
         1.7346, 1.1173, 0.0000, 0.1149, 0.0000, 0.4022, 1.0840, 0.8629, 1.0614,
         0.9405, 1.4178, 0.9589, 0.5886, 0.8036, 0.8896, 1.5264, 1.2750, 3.2852,
         2.1206, 1.3205, 0.0944, 0.0000, 0.9568, 0.3637, 0.1996, 0.7288, 0.8515,
         0.4500, 1.2006, 1.2857, 0.6947, 0.9306, 0.8282, 0.3738, 0.3806, 0.8671,
         1.4680, 1.4449, 0.7360, 0.8963, 0.3637, 1.1730, 1.2984, 1.0173, 1.1578,
         0.0000, 0.0783, 0.7434, 0.6271, 0.9671, 0.0000, 0.0000, 0.1400, 0.3621,
         0.8678],
        [1.5508, 2.6555, 2.0210, 1.5206, 2.1572, 1.9532, 1.7863, 2.2600, 1.3023,
         0.0611, 1.9440, 2.5833, 2.9309, 1.1665, 0.4092, 2.1513, 1.3440, 2.0849,
         3.0276, 1.4984, 1.8919, 1.3436, 1.8926, 0.9137, 1.9209, 1.4946, 0.1737,
         0.8055, 1.3949, 2.0812, 0.9198, 0.7889, 1.2026, 2.1941, 1.9428, 1.4146,
         1.1424, 0.9570, 2.2107, 1.2945, 1.3605, 0.4655, 0.5497, 1.3089, 0.7528,
         0.5791, 1.2329, 1.3786, 0.9712, 0.7448, 1.0677, 0.0870, 0.5536, 0.4435,
         0.1682, 0.4611, 0.1760, 0.3266, 0.2337, 0.2312, 0.0000, 0.2783, 0.1195,
         0.2956, 0.3340, 1.8331, 1.1124, 0.5178, 0.6940, 0.8518, 1.5544, 0.1972,
         0.2119, 0.5173, 1.4408, 1.7112, 1.9593, 2.0902, 1.9515, 2.4504, 0.9853,
         1.3556, 1.4133, 1.1645, 1.6155, 0.7152, 0.8593, 1.1878, 0.9196, 1.2179,
         0.5467, 0.3280, 0.7374, 0.8929, 1.0340, 0.5339, 0.5588, 1.0588, 1.6312,
         1.5088]], device='cuda:0', grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2116, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.0807, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.0807, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[ 1.0537e+00, -8.5973e-01, -9.0676e-01, -1.6980e+00, -6.5846e-02,
          4.7698e-01, -1.0942e+00, -8.3746e-01, -8.3220e-01, -9.0593e-01,
          3.1268e-01, -7.9426e-01, -2.7215e-01, -5.7844e-01, -7.7746e-01,
          2.5867e-02, -3.0433e-01, -1.0496e-01, -8.7834e-01,  1.1257e-01,
          1.0228e-01,  2.1861e-02, -2.4783e-02, -2.7300e-01, -5.7881e-01,
         -1.9220e-01, -7.1521e-01, -1.0805e+00, -9.0592e-01, -3.9120e-01,
         -1.2097e+00, -9.5165e-01, -4.7564e-01, -1.1469e+00, -6.2282e-01,
         -8.2742e-02, -1.2489e+00, -2.4753e-01, -2.0810e-01, -1.1430e+00,
         -1.1536e+00, -5.7253e-01, -1.0590e+00, -6.9994e-01, -1.7691e-01,
         -3.7958e-01,  1.4387e-01,  1.5677e-01,  5.2920e-01,  8.2119e-01,
          7.2110e-01, -2.6827e-01,  7.0398e-02, -7.1417e-02, -2.1020e-01,
          1.7107e+00,  1.2526e+00,  4.8175e-01,  1.4040e-01,  1.8011e-01,
          1.1795e+00,  6.7595e-01,  1.3636e-01,  2.2207e-03,  4.3597e-01,
          9.8473e-01,  8.5229e-01,  5.6720e-01, -7.2266e-01,  1.3691e+00,
          7.1638e-01,  1.2107e+00,  8.9978e-01,  2.0184e-01,  1.0970e+00,
          4.1506e-01,  2.8157e-01,  1.3240e-01,  3.3890e-01,  6.9279e-01,
          2.1283e-01,  1.2760e-01, -7.0214e-03,  1.2589e-01,  1.9522e-01,
          9.0834e-02,  3.3903e-01,  2.9354e-01,  1.1542e-01,  1.1006e-01,
          8.6231e-01,  9.7537e-01,  8.9865e-01,  6.9512e-01,  4.1172e-01,
          6.1035e-01,  8.8917e-01,  9.5384e-01,  9.6370e-01,  6.7825e-01],
        [ 4.0662e-01,  1.7654e-01, -7.1504e-01, -1.6773e-01,  4.5957e-01,
         -5.7844e-01, -1.0607e+00, -2.4863e-01,  4.3781e-01, -7.5592e-01,
         -1.3828e+00, -1.3087e+00, -7.6937e-01, -9.5437e-02,  7.5945e-01,
         -2.1156e-01, -1.7388e+00, -1.4761e+00, -7.9179e-01, -1.8130e+00,
         -1.2633e+00, -7.8155e-01, -8.8865e-01, -1.0092e+00, -2.0429e+00,
          4.2923e-01, -1.8507e-01, -1.0684e+00, -6.7181e-01, -3.4413e-01,
         -9.9983e-01, -1.0214e-01, -3.3279e-01, -1.0718e-01, -6.1412e-01,
         -3.2975e-03, -1.0131e+00,  3.0762e-01,  1.6992e-01, -1.8795e-01,
         -3.6068e-01,  5.1454e-01,  1.1104e-01,  8.6386e-01, -3.6721e-01,
         -3.5268e-01, -1.2818e+00, -5.4015e-01, -5.1576e-01, -3.1503e-02,
          5.0830e-01, -3.6247e-01,  1.9827e-01,  9.1163e-02,  3.9297e-01,
          1.8143e-01,  4.7462e-01,  4.7823e-01, -4.5109e-01, -3.6198e-01,
          3.3652e-01,  1.0938e+00,  9.5749e-02,  4.7663e-01,  2.0730e-01,
          6.6215e-01, -3.4295e-01,  1.4323e+00, -3.7436e-01, -1.2872e-01,
          4.1860e-02,  5.2189e-01, -1.0141e+00,  7.7387e-01,  1.4524e+00,
          1.1244e-01,  3.5871e-01,  1.0171e+00,  1.1341e+00,  9.0618e-01,
          4.0074e-01,  2.6246e-01,  5.0864e-01,  6.4746e-01,  6.8424e-01,
          8.2469e-01,  7.6036e-01,  6.1841e-01,  3.9614e-01,  2.7668e-01,
          1.3780e+00,  1.4324e+00,  8.7110e-01,  5.0551e-01,  6.8771e-02,
          1.0495e+00,  8.6873e-01,  1.8629e-01,  3.4934e-01,  5.0256e-01],
        [ 1.8243e-01, -1.7962e-01, -4.1452e-02, -6.7516e-01, -1.2143e-03,
         -8.1816e-01, -6.9225e-01,  1.1301e-01,  6.5760e-01, -8.2641e-01,
         -2.7628e-01, -4.8414e-01,  2.9887e-01, -5.8121e-01, -2.6598e-01,
         -6.5543e-01, -3.1208e-01, -1.2005e+00, -1.1790e-01, -1.0844e+00,
         -1.1832e+00, -1.1199e+00, -1.8198e+00, -9.7977e-01, -7.0479e-01,
         -6.5312e-01,  4.8451e-01, -4.7149e-01, -2.7975e-01, -1.2779e-01,
          1.6235e-01, -9.7652e-01, -6.6914e-01, -3.6544e-01, -1.0821e-01,
          3.2627e-01, -1.6073e-01, -5.4092e-01, -1.4493e+00,  3.4044e-01,
         -6.6111e-01, -7.9926e-02, -7.4027e-01, -5.7480e-01,  3.0079e-01,
         -4.8774e-01,  4.1252e-01, -7.9617e-01,  2.4264e-01,  3.5082e-01,
          9.9431e-01, -2.5645e-01, -3.8996e-01, -2.6564e-01, -3.3366e-01,
         -5.1054e-01,  2.8858e-01,  7.7198e-01,  2.2861e-01, -5.5252e-01,
          7.3841e-01,  1.5819e-01, -1.4677e-01, -2.0352e-01, -1.3881e-01,
          4.8381e-01, -3.5547e-01,  9.9070e-01,  3.3133e-01, -1.4408e+00,
          2.3220e-01,  1.1501e+00,  3.2778e-01,  3.5339e-01, -8.7280e-01,
          1.8271e-01,  6.1931e-01,  9.9480e-01,  9.7865e-01,  6.7381e-01,
          1.2942e+00,  1.7609e+00,  1.8469e+00,  1.6058e+00,  6.4325e-01,
          1.6691e+00,  1.8044e+00,  1.7480e+00,  1.4801e+00,  2.2840e-01,
          1.1108e+00,  9.6061e-01,  8.8012e-01,  7.2591e-01, -3.5072e-01,
          5.7202e-01, -9.4802e-03, -9.9042e-02, -1.3500e-01, -6.9626e-01],
        [-2.3742e-01, -6.3136e-01, -1.5098e+00, -7.2972e-01, -5.9865e-01,
         -8.8783e-01, -8.6147e-01, -9.9422e-01, -1.0303e+00, -8.8146e-01,
         -1.3661e+00, -9.6672e-01, -1.9399e+00, -1.4978e+00, -1.5156e+00,
         -1.1776e+00, -9.1101e-01, -2.0527e+00, -2.0298e+00, -1.4485e+00,
         -9.2313e-01, -5.4627e-01, -4.3388e-01, -3.9223e-01, -5.1056e-01,
         -1.7018e+00,  5.9014e-02,  6.9008e-01,  4.3733e-01,  8.0303e-01,
         -6.3315e-01, -3.2201e-01, -8.0082e-01,  1.2802e+00, -1.9525e-01,
          8.4648e-01, -1.3368e+00, -2.2362e-01,  3.4877e-01,  3.8577e-02,
         -2.1683e-01, -4.3601e-01, -1.9711e+00,  7.3794e-02,  2.4132e-01,
          1.2568e-01,  6.1886e-03, -6.8163e-01, -8.9492e-01,  8.3121e-02,
          1.2168e-01,  2.2845e-01,  4.5663e-01, -1.6812e-01, -2.0541e-01,
         -8.8744e-01, -2.4064e-01, -2.7639e-01, -3.7915e-01, -4.0867e-01,
          1.1023e+00,  1.3227e+00,  1.0697e+00,  1.2510e-02,  3.2845e-01,
          3.8094e-01,  2.9320e-02,  1.8947e-01, -4.7487e-01,  1.7180e-01,
          3.6354e-01,  3.0181e-01,  1.1737e+00,  8.6234e-01,  1.5365e+00,
          2.9445e-02, -2.9911e-02, -1.0218e-01, -7.0364e-02,  7.0941e-02,
          6.3133e-01,  5.3671e-01,  1.3112e-02,  3.7251e-02,  1.4330e-01,
          4.6892e-01,  7.5049e-01,  6.3801e-01,  7.6173e-01,  4.5336e-01,
          3.2480e-02,  5.0222e-01,  8.2515e-01,  1.0261e+00,  8.4370e-01,
         -5.5096e-01,  1.8420e-01,  9.0563e-01,  1.2508e+00,  7.8598e-01],
        [-3.5461e-01, -4.6826e-01, -1.6520e-01, -7.3670e-01, -9.4620e-01,
         -1.1573e+00, -2.7140e-01, -2.3069e-01, -3.9060e-01, -1.1083e+00,
         -5.3543e-01,  8.5413e-02, -1.6930e-01, -5.2634e-01, -1.7100e+00,
         -5.3899e-01, -9.9076e-02, -3.2910e-01, -2.0517e-02, -1.8975e-01,
          2.5092e-01, -8.8595e-01, -1.6324e+00, -2.0725e-01, -5.0703e-01,
         -1.8163e-02, -1.7129e+00, -1.1077e+00,  4.4171e-01,  2.4809e-01,
         -1.7271e-01, -4.1849e-01, -2.9394e-01, -1.1709e-01,  3.5629e-01,
         -1.4250e-01, -4.3272e-01, -6.1920e-01, -3.9726e-01, -6.1292e-01,
         -1.7741e-01,  8.2407e-01, -1.0891e+00,  1.3192e+00, -4.9443e-01,
         -9.3257e-02, -6.9156e-01, -1.3255e+00,  4.0646e-02,  1.7881e-01,
         -1.4107e+00, -1.2157e+00, -3.0590e-01, -2.7468e-01,  1.9752e-01,
         -5.1890e-01,  1.1527e+00,  1.4693e-02,  7.0572e-02,  2.4431e-01,
          8.6355e-01,  6.2140e-02,  1.4724e-01, -1.1281e-01, -4.9359e-01,
         -4.2404e-01,  3.7326e-01, -4.5937e-01, -1.1053e-01, -3.4114e-01,
         -8.4574e-01,  2.8952e-01,  9.7029e-02, -7.3471e-01, -7.2500e-02,
         -1.2707e-01,  1.2515e-01,  3.6416e-01,  2.3295e-01, -1.0617e+00,
         -5.0864e-01, -5.7109e-03,  6.7639e-01,  9.9516e-01,  6.6623e-01,
         -5.8973e-01, -2.2899e-02,  6.9650e-01,  1.0773e+00,  8.6759e-01,
         -4.5451e-01,  1.9050e-01,  3.8582e-01,  4.3365e-01,  2.9132e-01,
         -5.5068e-01,  2.8965e-03,  4.1752e-01,  4.5852e-01,  7.8983e-02],
        [-2.4827e-01, -4.6767e-01, -1.0833e+00,  3.6851e-01,  1.9341e-02,
         -7.4682e-01, -8.3690e-02, -4.5798e-02, -3.3699e-01, -4.9905e-01,
         -4.9330e-01, -9.6914e-01,  2.2282e-01,  1.1543e+00, -2.4817e-01,
         -1.8450e+00, -3.7611e-01, -1.3067e+00, -1.0577e+00, -6.8624e-01,
         -1.8609e+00, -2.7561e-01, -1.5568e+00, -2.5252e+00, -4.9386e-01,
         -1.0414e+00, -1.6986e+00, -6.9757e-01, -1.4437e-01, -3.0400e-01,
         -1.3985e+00, -6.3636e-01, -4.3876e-01, -3.2860e-01, -1.7429e+00,
         -1.8445e-01, -5.7943e-01, -6.0187e-01, -9.7557e-01, -4.7116e-01,
         -6.4254e-01, -9.3217e-02, -6.2729e-02, -8.9145e-01, -7.6562e-01,
         -2.0951e-01, -2.9691e-01,  1.4706e-02,  5.4140e-01, -3.9865e-01,
          9.4201e-01,  3.6434e-01,  7.8895e-01,  8.2287e-02,  2.5433e-01,
          1.8877e-01,  2.8940e-01,  1.0319e+00,  3.7629e-01, -9.1284e-01,
          5.6097e-01, -5.6476e-01,  3.5434e-01,  5.6967e-01,  6.0605e-05,
         -2.7464e-01,  2.4742e-01, -7.9709e-02,  1.3113e+00,  6.4647e-01,
          3.7748e-01,  4.8031e-02, -1.2799e+00, -5.6818e-01, -1.0548e-01,
          4.7193e-01, -6.2446e-02,  1.4494e-01,  4.5576e-01,  4.8361e-01,
         -1.5237e-01, -5.3353e-01, -9.9396e-02,  3.7354e-01,  4.1908e-01,
         -8.9002e-01, -7.6648e-01, -2.6078e-01,  4.4615e-01,  4.7913e-01,
         -7.4918e-01, -3.7332e-01,  7.8713e-02,  6.4065e-01,  5.0858e-01,
         -1.4706e-01,  1.0448e-01,  2.0963e-01,  2.3386e-01, -7.1352e-02],
        [ 6.9098e-01, -2.9023e-01, -4.9311e-01,  4.0822e-02, -3.3722e-01,
         -9.5326e-01, -9.7166e-01, -4.8649e-01,  3.2491e-01, -7.1664e-03,
         -1.6098e-01, -1.6245e-01, -1.3266e+00, -8.9644e-01, -1.0851e-01,
         -1.0075e+00, -5.0593e-01, -8.0318e-01, -1.4139e+00, -3.3665e-01,
          1.4112e-01,  2.4399e-02, -6.4563e-01, -9.6616e-01,  3.5755e-01,
         -1.1404e+00, -1.0577e+00, -2.3255e-01, -7.9332e-01, -1.6290e+00,
         -1.2203e+00, -9.5615e-01, -7.7187e-01, -1.2948e+00, -1.8222e+00,
         -6.5825e-01,  8.2333e-02, -3.0302e-01, -2.7249e-01, -1.9930e+00,
         -1.1813e-01, -2.9731e-01, -8.7992e-02, -2.9244e-03, -1.1697e+00,
         -7.3135e-01, -4.7369e-01,  2.6614e-01,  5.3332e-02, -8.1896e-01,
          1.1094e+00,  4.0732e-01, -9.9215e-02, -1.0207e-01, -3.8914e-02,
          1.0061e+00,  3.1795e-01,  4.5473e-01,  3.7067e-01,  5.5239e-01,
          1.1770e+00,  7.8966e-01,  1.0847e+00,  2.6762e-02,  5.0694e-01,
         -8.9191e-02,  6.2423e-01,  3.6535e-01,  5.1854e-01, -4.8456e-01,
         -2.2283e+00, -5.3413e-01, -8.2528e-01, -7.3025e-01, -5.8986e-01,
          4.8281e-01,  3.6257e-01,  2.9883e-01,  2.0235e-01, -2.8821e-01,
          2.9355e-01,  6.6473e-02,  1.4375e-02, -1.5552e-01, -5.0683e-01,
         -7.3170e-03, -2.3060e-01, -4.0101e-01, -6.3368e-01, -7.9989e-01,
         -6.4849e-01, -3.5539e-01, -3.6389e-01, -5.6472e-01, -3.6998e-01,
         -1.4841e-01,  2.6302e-01,  1.8831e-01,  1.2668e-01,  2.2829e-01],
        [-6.9000e-01, -3.6364e-01, -8.8858e-02, -4.3063e-01, -2.3880e-02,
         -1.6418e+00, -4.8593e-01, -6.2326e-01,  2.5532e-01, -2.1937e-01,
         -1.1352e+00, -3.3128e-01,  2.9880e-01,  1.1544e+00,  2.2251e-01,
         -1.0149e+00, -2.9350e-01, -8.9728e-01, -5.1268e-01, -1.2287e+00,
         -2.1475e+00, -1.0703e+00, -2.0163e+00, -2.3417e-01, -9.0665e-01,
         -3.9030e-01, -4.0538e-01, -7.8312e-01,  3.3812e-02, -5.3019e-01,
         -4.0543e-01,  4.1049e-01, -5.7862e-01, -7.9118e-01, -4.9662e-01,
         -5.1881e-01,  1.8043e-01,  7.5337e-01,  3.5202e-02,  5.2211e-01,
         -9.9829e-02,  5.8627e-01, -5.9950e-01, -3.9457e-01, -4.0430e-02,
          8.7611e-01, -3.6233e-01,  3.6229e-02, -1.9650e-01, -6.9178e-02,
          7.6342e-01,  1.0445e+00,  8.2865e-01,  5.6059e-01, -3.3435e-01,
          9.8300e-01, -1.3046e-01, -1.2516e+00, -7.7741e-01, -9.2052e-01,
          4.6134e-01,  1.5383e+00,  4.7496e-01, -7.5820e-02,  5.6691e-01,
         -3.9433e-01, -4.4767e-01, -4.7520e-01,  2.1085e-01,  7.7004e-01,
         -7.5207e-01,  2.9740e-02, -1.3578e+00,  2.0919e-01,  1.0546e-01,
          4.8000e-01,  1.2517e-01,  4.5606e-02,  8.3802e-02,  6.8221e-02,
         -1.3400e-02, -2.6276e-01, -6.9718e-02,  3.7750e-02, -2.1284e-01,
         -1.0845e-01,  1.7138e-01,  1.8377e-01,  3.9294e-01,  9.0994e-02,
          3.6027e-01,  7.7933e-01,  1.0571e+00,  1.0450e+00,  6.9552e-01,
          3.0229e-01,  7.3559e-01,  1.0093e+00,  9.3295e-01,  4.8898e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[0.0000, 1.8597, 1.9068, 2.6980, 1.0658, 0.5230, 2.0942, 1.8375, 1.8322,
         1.9059, 0.6873, 1.7943, 1.2721, 1.5784, 1.7775, 0.9741, 1.3043, 1.1050,
         1.8783, 0.8874, 0.8977, 0.9781, 1.0248, 1.2730, 1.5788, 1.1922, 1.7152,
         2.0805, 1.9059, 1.3912, 2.2097, 1.9516, 1.4756, 2.1469, 1.6228, 1.0827,
         2.2489, 1.2475, 1.2081, 2.1430, 2.1536, 1.5725, 2.0590, 1.6999, 1.1769,
         1.3796, 0.8561, 0.8432, 0.4708, 0.1788, 0.2789, 1.2683, 0.9296, 1.0714,
         1.2102, 0.0000, 0.0000, 0.5183, 0.8596, 0.8199, 0.0000, 0.3240, 0.8636,
         0.9978, 0.5640, 0.0153, 0.1477, 0.4328, 1.7227, 0.0000, 0.2836, 0.0000,
         0.1002, 0.7982, 0.0000, 0.5849, 0.7184, 0.8676, 0.6611, 0.3072, 0.7872,
         0.8724, 1.0070, 0.8741, 0.8048, 0.9092, 0.6610, 0.7065, 0.8846, 0.8899,
         0.1377, 0.0246, 0.1014, 0.3049, 0.5883, 0.3897, 0.1108, 0.0462, 0.0363,
         0.3217],
        [0.5934, 0.8235, 1.7150, 1.1677, 0.5404, 1.5784, 2.0607, 1.2486, 0.5622,
         1.7559, 2.3828, 2.3087, 1.7694, 1.0954, 0.2405, 1.2116, 2.7388, 2.4761,
         1.7918, 2.8130, 2.2633, 1.7816, 1.8886, 2.0092, 3.0429, 0.5708, 1.1851,
         2.0684, 1.6718, 1.3441, 1.9998, 1.1021, 1.3328, 1.1072, 1.6141, 1.0033,
         2.0131, 0.6924, 0.8301, 1.1879, 1.3607, 0.4855, 0.8890, 0.1361, 1.3672,
         1.3527, 2.2818, 1.5401, 1.5158, 1.0315, 0.4917, 1.3625, 0.8017, 0.9088,
         0.6070, 0.8186, 0.5254, 0.5218, 1.4511, 1.3620, 0.6635, 0.0000, 0.9043,
         0.5234, 0.7927, 0.3379, 1.3429, 0.0000, 1.3744, 1.1287, 0.9581, 0.4781,
         2.0141, 0.2261, 0.0000, 0.8876, 0.6413, 0.0000, 0.0000, 0.0938, 0.5993,
         0.7375, 0.4914, 0.3525, 0.3158, 0.1753, 0.2396, 0.3816, 0.6039, 0.7233,
         0.0000, 0.0000, 0.1289, 0.4945, 0.9312, 0.0000, 0.1313, 0.8137, 0.6507,
         0.4974],
        [0.8176, 1.1796, 1.0415, 1.6752, 1.0012, 1.8182, 1.6923, 0.8870, 0.3424,
         1.8264, 1.2763, 1.4841, 0.7011, 1.5812, 1.2660, 1.6554, 1.3121, 2.2005,
         1.1179, 2.0844, 2.1832, 2.1199, 2.8198, 1.9798, 1.7048, 1.6531, 0.5155,
         1.4715, 1.2798, 1.1278, 0.8377, 1.9765, 1.6691, 1.3654, 1.1082, 0.6737,
         1.1607, 1.5409, 2.4493, 0.6596, 1.6611, 1.0799, 1.7403, 1.5748, 0.6992,
         1.4877, 0.5875, 1.7962, 0.7574, 0.6492, 0.0057, 1.2565, 1.3900, 1.2656,
         1.3337, 1.5105, 0.7114, 0.2280, 0.7714, 1.5525, 0.2616, 0.8418, 1.1468,
         1.2035, 1.1388, 0.5162, 1.3555, 0.0093, 0.6687, 2.4408, 0.7678, 0.0000,
         0.6722, 0.6466, 1.8728, 0.8173, 0.3807, 0.0052, 0.0213, 0.3262, 0.0000,
         0.0000, 0.0000, 0.0000, 0.3567, 0.0000, 0.0000, 0.0000, 0.0000, 0.7716,
         0.0000, 0.0394, 0.1199, 0.2741, 1.3507, 0.4280, 1.0095, 1.0990, 1.1350,
         1.6963],
        [1.2374, 1.6314, 2.5098, 1.7297, 1.5986, 1.8878, 1.8615, 1.9942, 2.0303,
         1.8815, 2.3661, 1.9667, 2.9399, 2.4978, 2.5156, 2.1776, 1.9110, 3.0527,
         3.0298, 2.4485, 1.9231, 1.5463, 1.4339, 1.3922, 1.5106, 2.7018, 0.9410,
         0.3099, 0.5627, 0.1970, 1.6331, 1.3220, 1.8008, 0.0000, 1.1953, 0.1535,
         2.3368, 1.2236, 0.6512, 0.9614, 1.2168, 1.4360, 2.9711, 0.9262, 0.7587,
         0.8743, 0.9938, 1.6816, 1.8949, 0.9169, 0.8783, 0.7715, 0.5434, 1.1681,
         1.2054, 1.8874, 1.2406, 1.2764, 1.3791, 1.4087, 0.0000, 0.0000, 0.0000,
         0.9875, 0.6715, 0.6191, 0.9707, 0.8105, 1.4749, 0.8282, 0.6365, 0.6982,
         0.0000, 0.1377, 0.0000, 0.9706, 1.0299, 1.1022, 1.0704, 0.9291, 0.3687,
         0.4633, 0.9869, 0.9627, 0.8567, 0.5311, 0.2495, 0.3620, 0.2383, 0.5466,
         0.9675, 0.4978, 0.1749, 0.0000, 0.1563, 1.5510, 0.8158, 0.0944, 0.0000,
         0.2140],
        [1.3546, 1.4683, 1.1652, 1.7367, 1.9462, 2.1573, 1.2714, 1.2307, 1.3906,
         2.1083, 1.5354, 0.9146, 1.1693, 1.5263, 2.7100, 1.5390, 1.0991, 1.3291,
         1.0205, 1.1897, 0.7491, 1.8859, 2.6324, 1.2073, 1.5070, 1.0182, 2.7129,
         2.1077, 0.5583, 0.7519, 1.1727, 1.4185, 1.2939, 1.1171, 0.6437, 1.1425,
         1.4327, 1.6192, 1.3973, 1.6129, 1.1774, 0.1759, 2.0891, 0.0000, 1.4944,
         1.0933, 1.6916, 2.3255, 0.9594, 0.8212, 2.4107, 2.2157, 1.3059, 1.2747,
         0.8025, 1.5189, 0.0000, 0.9853, 0.9294, 0.7557, 0.1365, 0.9379, 0.8528,
         1.1128, 1.4936, 1.4240, 0.6267, 1.4594, 1.1105, 1.3411, 1.8457, 0.7105,
         0.9030, 1.7347, 1.0725, 1.1271, 0.8749, 0.6358, 0.7670, 2.0617, 1.5086,
         1.0057, 0.3236, 0.0048, 0.3338, 1.5897, 1.0229, 0.3035, 0.0000, 0.1324,
         1.4545, 0.8095, 0.6142, 0.5664, 0.7087, 1.5507, 0.9971, 0.5825, 0.5415,
         0.9210],
        [1.2483, 1.4677, 2.0833, 0.6315, 0.9807, 1.7468, 1.0837, 1.0458, 1.3370,
         1.4991, 1.4933, 1.9691, 0.7772, 0.0000, 1.2482, 2.8450, 1.3761, 2.3067,
         2.0577, 1.6862, 2.8609, 1.2756, 2.5568, 3.5252, 1.4939, 2.0414, 2.6986,
         1.6976, 1.1444, 1.3040, 2.3985, 1.6364, 1.4388, 1.3286, 2.7429, 1.1845,
         1.5794, 1.6019, 1.9756, 1.4712, 1.6425, 1.0932, 1.0627, 1.8915, 1.7656,
         1.2095, 1.2969, 0.9853, 0.4586, 1.3986, 0.0580, 0.6357, 0.2110, 0.9177,
         0.7457, 0.8112, 0.7106, 0.0000, 0.6237, 1.9128, 0.4390, 1.5648, 0.6457,
         0.4303, 0.9999, 1.2746, 0.7526, 1.0797, 0.0000, 0.3535, 0.6225, 0.9520,
         2.2799, 1.5682, 1.1055, 0.5281, 1.0624, 0.8551, 0.5442, 0.5164, 1.1524,
         1.5335, 1.0994, 0.6265, 0.5809, 1.8900, 1.7665, 1.2608, 0.5538, 0.5209,
         1.7492, 1.3733, 0.9213, 0.3594, 0.4914, 1.1471, 0.8955, 0.7904, 0.7661,
         1.0714],
        [0.3090, 1.2902, 1.4931, 0.9592, 1.3372, 1.9533, 1.9717, 1.4865, 0.6751,
         1.0072, 1.1610, 1.1624, 2.3266, 1.8964, 1.1085, 2.0075, 1.5059, 1.8032,
         2.4139, 1.3367, 0.8589, 0.9756, 1.6456, 1.9662, 0.6424, 2.1404, 2.0577,
         1.2325, 1.7933, 2.6290, 2.2203, 1.9562, 1.7719, 2.2948, 2.8222, 1.6583,
         0.9177, 1.3030, 1.2725, 2.9930, 1.1181, 1.2973, 1.0880, 1.0029, 2.1697,
         1.7314, 1.4737, 0.7339, 0.9467, 1.8190, 0.0000, 0.5927, 1.0992, 1.1021,
         1.0389, 0.0000, 0.6820, 0.5453, 0.6293, 0.4476, 0.0000, 0.2103, 0.0000,
         0.9732, 0.4931, 1.0892, 0.3758, 0.6346, 0.4815, 1.4846, 3.2283, 1.5341,
         1.8253, 1.7302, 1.5899, 0.5172, 0.6374, 0.7012, 0.7977, 1.2882, 0.7065,
         0.9335, 0.9856, 1.1555, 1.5068, 1.0073, 1.2306, 1.4010, 1.6337, 1.7999,
         1.6485, 1.3554, 1.3639, 1.5647, 1.3700, 1.1484, 0.7370, 0.8117, 0.8733,
         0.7717],
        [1.6900, 1.3636, 1.0889, 1.4306, 1.0239, 2.6418, 1.4859, 1.6233, 0.7447,
         1.2194, 2.1352, 1.3313, 0.7012, 0.0000, 0.7775, 2.0149, 1.2935, 1.8973,
         1.5127, 2.2287, 3.1475, 2.0703, 3.0163, 1.2342, 1.9067, 1.3903, 1.4054,
         1.7831, 0.9662, 1.5302, 1.4054, 0.5895, 1.5786, 1.7912, 1.4966, 1.5188,
         0.8196, 0.2466, 0.9648, 0.4779, 1.0998, 0.4137, 1.5995, 1.3946, 1.0404,
         0.1239, 1.3623, 0.9638, 1.1965, 1.0692, 0.2366, 0.0000, 0.1713, 0.4394,
         1.3344, 0.0170, 1.1305, 2.2516, 1.7774, 1.9205, 0.5387, 0.0000, 0.5250,
         1.0758, 0.4331, 1.3943, 1.4477, 1.4752, 0.7891, 0.2300, 1.7521, 0.9703,
         2.3578, 0.7908, 0.8945, 0.5200, 0.8748, 0.9544, 0.9162, 0.9318, 1.0134,
         1.2628, 1.0697, 0.9623, 1.2128, 1.1084, 0.8286, 0.8162, 0.6071, 0.9090,
         0.6397, 0.2207, 0.0000, 0.0000, 0.3045, 0.6977, 0.2644, 0.0000, 0.0671,
         0.5110]], device='cuda:0', grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1252, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.0548, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.0548, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-1.4339e-01, -1.0224e-01, -1.2864e+00, -3.7163e-01,  5.8603e-01,
         -5.4123e-01, -1.7962e+00,  6.7378e-01, -8.1419e-01, -7.7434e-01,
         -5.8485e-01, -5.3968e-01, -8.0802e-01, -5.5594e-01,  4.9340e-01,
          7.9912e-02, -1.5028e+00, -2.5558e-01, -5.0435e-01, -2.6350e-01,
         -1.3149e+00, -1.3042e+00, -8.9641e-01, -5.9962e-01, -8.8845e-01,
          5.1097e-01, -9.4892e-02, -1.1894e+00,  7.5017e-02, -8.9358e-01,
         -1.4984e-01,  1.3542e-01, -1.3832e-01, -5.6227e-01, -2.8419e-01,
          3.7594e-01, -2.0991e-01, -7.3622e-01, -7.5787e-01, -9.3992e-01,
          2.8970e-02, -6.3619e-01,  4.4152e-01, -1.0756e+00,  5.2203e-02,
          3.7158e-01, -7.1032e-01, -1.1075e+00, -4.7363e-01, -5.9272e-01,
          1.1764e-01,  4.3219e-01, -3.1086e-02,  5.9300e-01,  6.1266e-01,
         -3.2493e-01, -7.1376e-02, -2.1097e-01,  2.5374e-01,  4.9274e-01,
         -4.0290e-02,  1.2991e-01,  4.5751e-01,  2.6842e-02, -4.1272e-01,
         -1.8747e-01, -4.3682e-01,  9.5836e-04, -1.7061e-01,  5.0537e-01,
         -1.8245e-01,  4.0191e-02,  9.6378e-02, -1.3242e+00, -3.9690e-01,
          1.4976e-01, -1.9285e-02, -1.2555e-01,  3.1939e-01,  2.3703e-01,
         -2.1319e-01, -1.9985e-01,  7.0089e-02,  7.3893e-01,  9.9664e-01,
         -5.9543e-01, -4.1556e-01,  1.5898e-01,  1.3823e+00,  2.0307e+00,
         -1.3075e-01, -9.8856e-02,  2.7855e-01,  1.0372e+00,  1.6135e+00,
         -4.6477e-01, -3.9535e-01,  9.0550e-02,  5.1322e-01,  5.5424e-01],
        [-3.6758e-01, -1.1034e+00, -3.4783e-01, -2.5212e-01, -3.8545e-01,
         -4.8542e-01, -3.6970e-01, -2.2197e-01,  2.3142e-01, -4.4184e-01,
         -4.4054e-01, -5.0464e-01, -6.2311e-01, -3.2852e-01, -5.0037e-01,
         -6.6396e-01, -6.1283e-01, -1.9563e-01, -1.1970e+00, -3.1071e-01,
         -6.8641e-01,  6.4993e-01, -6.9807e-01, -1.6969e+00, -1.5341e+00,
         -2.8202e-01, -1.9880e-01,  9.4018e-03,  5.2013e-01,  2.3688e-01,
         -9.7076e-02, -8.8449e-01, -9.8749e-01,  2.2122e-01, -2.8715e-01,
         -1.5549e-01, -2.1864e-01,  1.5925e-03, -1.3645e-01,  6.1086e-01,
         -6.1150e-02, -7.9816e-01,  3.1656e-01, -4.0396e-01,  4.2535e-01,
         -4.6201e-01,  8.9231e-01,  3.5310e-01, -4.7426e-01,  1.2452e+00,
         -2.1641e-01, -1.1944e-01, -9.8731e-01, -8.2503e-01,  1.9471e-01,
         -4.1661e-01, -2.0421e-01, -4.6008e-01, -7.4792e-01,  3.6131e-02,
         -2.1247e-01,  9.7506e-02, -6.4729e-01, -5.3115e-01, -2.6125e-01,
         -5.6581e-01, -1.7926e-01, -4.2838e-01,  7.7274e-02,  2.3017e-01,
         -7.6127e-01, -2.6042e-01,  6.1807e-01, -1.7549e-01,  2.9556e-01,
         -7.9825e-01, -5.7292e-01, -6.0205e-01, -3.3280e-01, -7.2892e-02,
         -7.3664e-01, -4.0716e-01, -6.1538e-01, -3.5050e-01, -1.7494e-01,
         -8.8346e-01, -5.5149e-01, -6.1641e-01, -7.4969e-01, -7.9438e-01,
         -7.4509e-01, -4.2426e-01, -6.7090e-01, -7.6576e-01, -9.7104e-01,
         -6.5321e-01, -3.2026e-01, -6.1369e-01, -6.8104e-01, -8.1863e-01],
        [-1.0933e+00, -4.7641e-01,  3.1151e-01, -8.7354e-01, -9.6130e-02,
          8.3851e-02, -1.0417e+00, -7.8303e-01, -6.9431e-01, -2.5043e-01,
          1.3264e-01, -8.5748e-02, -6.5571e-01, -5.0387e-01,  2.8916e-01,
          4.3661e-01, -7.7626e-02, -9.5259e-01, -2.8423e-01,  1.1355e+00,
         -2.3544e-01, -1.1254e+00, -4.1143e-01, -6.8789e-01, -7.2826e-01,
         -3.2218e-01, -3.1556e-01, -1.0799e+00, -5.2675e-01, -1.6820e+00,
          1.0029e-01, -1.9347e+00, -1.8531e+00, -1.6933e+00, -1.0798e+00,
         -5.8793e-01, -1.8070e+00, -1.1872e+00, -1.7394e-01, -7.5429e-01,
          2.0341e-01, -5.9896e-01, -3.7155e-01, -9.9015e-01, -2.8397e-02,
          1.5598e-02, -2.1206e-01, -1.2171e-01, -1.3144e-01, -3.9358e-01,
         -7.4111e-01, -3.8293e-01,  1.8897e-01,  4.8679e-01,  4.9165e-01,
          8.3067e-01,  5.9947e-01,  2.3976e-01,  5.6721e-01,  7.3251e-01,
          4.1460e-01, -1.1329e+00,  1.0150e-01, -8.2801e-02,  2.2815e-01,
         -4.5132e-01,  2.4367e-01,  8.1195e-01,  5.8256e-01,  9.6554e-01,
          6.3114e-01,  3.6984e-01,  4.9636e-01,  3.0630e-01,  6.7630e-01,
         -1.1960e-01,  6.0486e-02,  3.5547e-01,  4.3974e-01,  5.6527e-01,
         -5.6252e-02,  1.3879e-01,  5.4319e-01,  6.0997e-01,  6.1411e-01,
          8.5818e-01,  7.1266e-01,  6.6688e-01,  4.8464e-01,  5.7555e-01,
          1.3313e+00,  1.0019e+00,  8.8028e-01,  5.3347e-01,  5.9269e-01,
          9.2567e-01,  5.8629e-01,  2.1562e-01, -8.2310e-02,  2.6401e-01],
        [ 4.0447e-01,  1.1552e-01,  6.3048e-01,  1.2333e-01, -8.9908e-02,
          8.0710e-02, -4.2813e-01,  3.3710e-01, -4.7399e-02, -2.0458e-01,
          1.4173e-01,  1.6174e-01, -1.5217e-01,  3.1773e-01, -8.9887e-01,
         -4.6148e-02,  9.4224e-02, -1.1981e+00, -1.6386e+00, -1.2123e+00,
         -4.5213e-01, -1.9670e-01, -1.1544e+00, -2.3328e+00, -2.1522e+00,
         -8.6971e-01, -1.1642e+00, -1.1255e+00, -1.2038e+00, -7.5377e-01,
         -1.8785e+00, -1.5130e+00, -1.2965e+00, -1.2508e+00, -8.5434e-01,
         -2.7896e-01, -1.0978e+00, -4.9693e-02, -4.7861e-01,  3.6464e-01,
         -1.1105e+00, -6.0478e-01, -8.0195e-01, -4.0436e-01,  6.0587e-01,
         -6.0333e-01, -5.0230e-01,  4.2964e-01, -6.0754e-01,  5.0331e-01,
          5.8577e-01,  1.0588e+00,  7.8785e-01,  8.9352e-01,  9.3168e-01,
          7.9348e-01,  6.3834e-02,  1.7200e-01,  9.1594e-01,  5.9942e-01,
          1.5892e+00,  6.0797e-01,  1.3587e-01, -6.2885e-01, -1.7519e-02,
          1.3829e+00,  2.6791e-01, -6.8666e-03,  6.0182e-01,  8.4002e-01,
          6.4862e-01,  1.4304e-01, -4.0680e-02, -7.9409e-01, -3.0675e-01,
          4.8323e-01,  4.8287e-01,  4.0581e-01,  6.1994e-01,  7.2152e-01,
          5.0871e-01,  3.0411e-01,  2.4156e-01,  4.8751e-01,  5.0395e-01,
          7.6830e-01,  5.2064e-01,  3.4215e-01,  4.5925e-01,  3.6016e-01,
          4.2500e-01,  6.1419e-02, -3.3889e-01, -1.6955e-01, -9.0346e-02,
         -6.0291e-01, -9.3162e-01, -1.3123e+00, -1.1208e+00, -1.0822e+00],
        [-6.1641e-01, -1.0232e+00,  6.1919e-02, -4.1305e-01, -1.0666e+00,
         -1.3833e+00,  4.8409e-01, -6.0474e-01,  4.2504e-02, -1.5896e-01,
         -1.3413e+00, -6.8062e-01,  2.6934e-01, -9.1447e-01, -1.5843e-01,
         -1.0719e+00, -3.3513e-01, -1.7483e-01, -4.7285e-01,  4.0761e-02,
         -1.8685e+00,  6.4113e-01, -3.8402e-01, -7.6173e-01, -8.0019e-01,
         -6.4759e-01,  2.5381e-01, -4.1218e-01, -2.2413e-01,  2.7169e-01,
         -6.2150e-01, -8.7931e-03,  5.4372e-02,  6.7218e-02,  5.9256e-01,
         -2.2955e-02, -5.8898e-01, -4.0307e-01, -1.8058e+00, -4.2378e-01,
         -3.9414e-01,  1.7647e-01,  8.7534e-01, -7.6736e-01, -9.9378e-02,
         -1.5244e-01, -1.7086e+00,  4.0536e-01,  6.1497e-02,  4.7085e-01,
         -3.8407e-01,  1.0637e+00,  1.7122e-01, -3.9321e-01,  1.5387e-01,
          5.9173e-01,  1.6210e+00,  3.9494e-01,  9.7787e-02,  7.3397e-01,
          2.2371e-01,  1.0609e-01,  1.6313e-01,  4.7486e-01,  3.6709e-03,
          5.4574e-01, -3.4189e-01, -2.0896e-01, -2.6497e-01, -3.9548e-02,
         -4.3976e-01, -5.5056e-01, -3.1415e-01,  4.0159e-01, -1.5771e-01,
         -4.1990e-01, -6.7424e-01, -2.0629e-01,  2.7768e-01,  4.2842e-01,
         -7.6027e-01, -9.1983e-01, -4.8257e-01, -1.3324e-01, -9.3728e-02,
         -5.9682e-01, -4.0325e-01, -9.6146e-02, -7.9974e-02, -1.6386e-01,
          2.7970e-01,  7.6340e-01,  8.1089e-01,  7.0768e-01,  4.0874e-01,
          8.3405e-01,  9.7199e-01,  1.1044e+00,  8.4862e-01,  4.5756e-02],
        [-3.0044e-01, -3.1539e-01,  3.4482e-02, -5.4746e-01, -5.0905e-01,
         -3.2944e-02, -2.4261e-01,  1.2946e-01, -4.5431e-01,  2.4893e-02,
         -5.4686e-01, -8.7777e-01,  6.4212e-01, -7.2820e-01, -4.0809e-01,
         -7.9507e-01, -1.3391e+00, -5.5282e-02, -9.1865e-01, -4.0208e-01,
         -3.9686e-01, -9.0981e-01, -7.6604e-01, -3.1414e-01, -4.6439e-01,
         -1.0366e+00, -1.8381e-01,  2.5378e-01, -2.3391e-01, -4.9176e-01,
         -6.7610e-01, -4.8589e-01, -2.2095e-01, -9.8546e-01, -8.3468e-01,
         -7.6966e-01, -1.5316e+00, -5.2979e-01,  8.1867e-02, -7.6230e-01,
          5.3682e-01,  3.7717e-01, -1.5343e-01,  5.2686e-01, -2.5830e-01,
         -1.3278e+00,  2.4536e-01, -8.9528e-01, -1.5552e+00, -1.1875e-01,
          6.1351e-01,  7.7478e-02,  2.2776e-01,  1.4366e-01,  9.2254e-02,
         -2.9916e-01, -4.7918e-01, -1.0097e+00, -1.0198e+00, -9.3426e-01,
         -1.0357e+00, -2.3281e-01, -1.0778e+00, -6.7192e-01,  1.1176e+00,
         -1.7578e-01,  5.2086e-01, -2.3544e-01, -1.4007e-01,  5.0408e-01,
         -5.1720e-01, -4.1990e-01, -9.6488e-01, -1.2657e-01,  7.7584e-01,
          7.3178e-02,  2.2982e-01,  7.8740e-01,  8.1730e-01,  4.4918e-01,
          1.4477e-01,  1.7875e-01,  4.4905e-01,  3.3033e-01,  1.0168e-01,
         -2.6076e-01, -4.0847e-01, -2.2879e-01, -1.4795e-01, -1.7941e-01,
         -2.6550e-01, -2.1258e-01, -4.0139e-01, -5.0834e-01, -5.8157e-01,
         -7.8902e-03,  1.8129e-01, -2.0875e-01, -6.2548e-01, -6.2334e-01],
        [-4.9193e-01, -1.3152e+00, -4.7613e-01, -2.5971e-01, -4.8668e-01,
         -1.2032e+00, -2.1378e-01, -5.5789e-01, -8.9442e-01, -1.3226e+00,
         -1.4436e+00, -1.0215e+00, -1.2848e+00, -5.9613e-01,  2.0254e-01,
         -2.1941e-02,  1.9992e-01,  4.3642e-01, -4.4803e-01, -9.8033e-01,
         -1.5970e+00, -1.6939e+00, -9.5075e-03, -1.0724e+00, -2.3094e-01,
         -9.9431e-01, -4.5475e-01, -5.4072e-01, -3.1454e-01, -4.2579e-01,
         -5.3178e-01, -8.2671e-01, -1.0742e+00, -1.3275e+00, -7.5348e-01,
         -9.1258e-01,  1.1903e-01,  1.9662e-01, -5.6064e-01, -8.5157e-01,
         -9.9994e-01, -6.6017e-01, -1.2945e+00, -9.3976e-01, -8.6634e-01,
          5.1827e-01,  1.5556e-01,  3.2031e-02,  2.8512e-02, -7.0501e-01,
          1.4262e+00,  3.6318e-01,  1.3660e+00, -3.2411e-01,  9.1727e-01,
          3.6706e-01,  8.3506e-01,  6.0249e-01,  9.2758e-02,  2.4731e-01,
         -3.9073e-01, -1.3357e+00,  9.5558e-01,  1.3596e-01,  2.7618e-01,
         -1.3302e+00, -2.4066e-01,  3.5466e-01, -1.0909e-01, -3.9565e-01,
         -5.5390e-01,  2.6932e-02,  8.1281e-01,  4.8578e-01, -3.6797e-01,
          7.1450e-01,  7.1206e-01,  3.4789e-01, -1.1064e-01, -1.1086e-01,
          1.5240e+00,  1.6520e+00,  1.2721e+00,  7.5043e-01,  6.6722e-01,
          1.2343e+00,  1.7815e+00,  1.4179e+00,  9.1683e-01,  8.8580e-01,
          1.9895e-01,  6.1280e-01,  5.0642e-01,  2.3712e-01,  2.5304e-01,
          2.8464e-01,  3.5193e-01,  2.5601e-01,  3.8539e-01,  5.0329e-02],
        [-7.7368e-01,  3.3714e-01,  4.9088e-01, -6.3357e-01, -3.1539e-01,
         -1.3515e+00, -7.2922e-01,  9.6034e-02,  3.9988e-01, -8.5305e-01,
         -9.1000e-01, -1.2020e+00, -4.0675e-01, -9.4112e-01, -1.4996e+00,
         -1.1798e+00,  7.5322e-01, -4.6739e-01, -1.0298e+00,  5.3063e-01,
         -8.2603e-01, -2.5858e-01, -7.0506e-01, -1.9810e+00, -3.7761e-01,
          9.3577e-02, -4.9498e-01, -7.6817e-01, -1.3967e+00,  3.4252e-02,
          2.1727e-01, -1.5448e-01, -6.6532e-01, -9.4959e-01, -2.0856e-01,
         -6.4867e-01,  7.4327e-01, -5.0256e-01, -7.1082e-01, -1.3084e+00,
         -2.7606e-01, -3.1079e-01,  5.8522e-01, -2.0113e+00, -3.8355e-01,
         -5.1921e-01,  9.2954e-01,  4.3114e-02, -5.0542e-02, -3.6610e-01,
          1.4372e-01,  1.8922e-01, -1.2763e+00, -1.5170e-01, -5.7159e-01,
          1.3567e-01, -8.6118e-01, -5.8327e-01, -1.0078e+00,  1.0476e+00,
          5.0070e-01,  1.1454e-01, -3.0948e-01, -4.7211e-01,  2.5136e-01,
          3.0273e-02,  6.7500e-01, -3.6334e-01, -5.0931e-02, -3.3641e-01,
          1.5065e-01,  2.7437e-01,  3.1020e-01,  1.5884e-04, -1.2487e+00,
         -6.7479e-01, -4.4119e-01, -1.4471e-01, -3.5210e-01,  2.1924e-02,
          6.0833e-02, -5.5232e-02, -3.3130e-02,  1.1487e-01,  6.3221e-01,
          4.6235e-01,  4.7901e-01,  3.3622e-01,  5.1746e-01,  7.6235e-01,
          4.1776e-01,  4.9823e-01,  3.3406e-01,  6.4493e-02,  8.8113e-03,
          4.2597e-01,  7.8151e-01,  7.1231e-01,  2.0666e-01, -1.4405e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.1434, 1.1022, 2.2864, 1.3716, 0.4140, 1.5412, 2.7962, 0.3262, 1.8142,
         1.7743, 1.5849, 1.5397, 1.8080, 1.5559, 0.5066, 0.9201, 2.5028, 1.2556,
         1.5043, 1.2635, 2.3149, 2.3042, 1.8964, 1.5996, 1.8884, 0.4890, 1.0949,
         2.1894, 0.9250, 1.8936, 1.1498, 0.8646, 1.1383, 1.5623, 1.2842, 0.6241,
         1.2099, 1.7362, 1.7579, 1.9399, 0.9710, 1.6362, 0.5585, 2.0756, 0.9478,
         0.6284, 1.7103, 2.1075, 1.4736, 1.5927, 0.8824, 0.5678, 1.0311, 0.4070,
         0.3873, 1.3249, 1.0714, 1.2110, 0.7463, 0.5073, 1.0403, 0.8701, 0.5425,
         0.9732, 1.4127, 1.1875, 1.4368, 0.9990, 1.1706, 0.4946, 1.1825, 0.9598,
         0.9036, 2.3242, 1.3969, 0.8502, 1.0193, 1.1255, 0.6806, 0.7630, 1.2132,
         1.1999, 0.9299, 0.2611, 0.0034, 1.5954, 1.4156, 0.8410, 0.0000, 0.0000,
         1.1307, 1.0989, 0.7215, 0.0000, 0.0000, 1.4648, 1.3953, 0.9095, 0.4868,
         0.4458],
        [1.3676, 2.1034, 1.3478, 1.2521, 1.3854, 1.4854, 1.3697, 1.2220, 0.7686,
         1.4418, 1.4405, 1.5046, 1.6231, 1.3285, 1.5004, 1.6640, 1.6128, 1.1956,
         2.1970, 1.3107, 1.6864, 0.3501, 1.6981, 2.6969, 2.5341, 1.2820, 1.1988,
         0.9906, 0.4799, 0.7631, 1.0971, 1.8845, 1.9875, 0.7788, 1.2872, 1.1555,
         1.2186, 0.9984, 1.1365, 0.3891, 1.0612, 1.7982, 0.6834, 1.4040, 0.5747,
         1.4620, 0.1077, 0.6469, 1.4743, 0.0000, 1.2164, 1.1194, 1.9873, 1.8250,
         0.8053, 1.4166, 1.2042, 1.4601, 1.7479, 0.9639, 1.2125, 0.9025, 1.6473,
         1.5311, 1.2612, 1.5658, 1.1793, 1.4284, 0.9227, 0.7698, 1.7613, 1.2604,
         0.3819, 1.1755, 0.7044, 1.7982, 1.5729, 1.6020, 1.3328, 1.0729, 1.7366,
         1.4072, 1.6154, 1.3505, 1.1749, 1.8835, 1.5515, 1.6164, 1.7497, 1.7944,
         1.7451, 1.4243, 1.6709, 1.7658, 1.9710, 1.6532, 1.3203, 1.6137, 1.6810,
         1.8186],
        [2.0933, 1.4764, 0.6885, 1.8735, 1.0961, 0.9161, 2.0417, 1.7830, 1.6943,
         1.2504, 0.8674, 1.0857, 1.6557, 1.5039, 0.7108, 0.5634, 1.0776, 1.9526,
         1.2842, 0.0000, 1.2354, 2.1254, 1.4114, 1.6879, 1.7283, 1.3222, 1.3156,
         2.0799, 1.5267, 2.6820, 0.8997, 2.9347, 2.8531, 2.6933, 2.0798, 1.5879,
         2.8070, 2.1872, 1.1739, 1.7543, 0.7966, 1.5990, 1.3716, 1.9901, 1.0284,
         0.9844, 1.2121, 1.1217, 1.1314, 1.3936, 1.7411, 1.3829, 0.8110, 0.5132,
         0.5084, 0.1693, 0.4005, 0.7602, 0.4328, 0.2675, 0.5854, 2.1329, 0.8985,
         1.0828, 0.7718, 1.4513, 0.7563, 0.1880, 0.4174, 0.0345, 0.3689, 0.6302,
         0.5036, 0.6937, 0.3237, 1.1196, 0.9395, 0.6445, 0.5603, 0.4347, 1.0563,
         0.8612, 0.4568, 0.3900, 0.3859, 0.1418, 0.2873, 0.3331, 0.5154, 0.4245,
         0.0000, 0.0000, 0.1197, 0.4665, 0.4073, 0.0743, 0.4137, 0.7844, 1.0823,
         0.7360],
        [0.5955, 0.8845, 0.3695, 0.8767, 1.0899, 0.9193, 1.4281, 0.6629, 1.0474,
         1.2046, 0.8583, 0.8383, 1.1522, 0.6823, 1.8989, 1.0461, 0.9058, 2.1981,
         2.6386, 2.2123, 1.4521, 1.1967, 2.1544, 3.3328, 3.1522, 1.8697, 2.1642,
         2.1255, 2.2038, 1.7538, 2.8785, 2.5130, 2.2965, 2.2508, 1.8543, 1.2790,
         2.0978, 1.0497, 1.4786, 0.6354, 2.1105, 1.6048, 1.8019, 1.4044, 0.3941,
         1.6033, 1.5023, 0.5704, 1.6075, 0.4967, 0.4142, 0.0000, 0.2122, 0.1065,
         0.0683, 0.2065, 0.9362, 0.8280, 0.0841, 0.4006, 0.0000, 0.3920, 0.8641,
         1.6289, 1.0175, 0.0000, 0.7321, 1.0069, 0.3982, 0.1600, 0.3514, 0.8570,
         1.0407, 1.7941, 1.3067, 0.5168, 0.5171, 0.5942, 0.3801, 0.2785, 0.4913,
         0.6959, 0.7584, 0.5125, 0.4961, 0.2317, 0.4794, 0.6578, 0.5408, 0.6398,
         0.5750, 0.9386, 1.3389, 1.1695, 1.0903, 1.6029, 1.9316, 2.3123, 2.1208,
         2.0822],
        [1.6164, 2.0232, 0.9381, 1.4130, 2.0666, 2.3833, 0.5159, 1.6047, 0.9575,
         1.1590, 2.3413, 1.6806, 0.7307, 1.9145, 1.1584, 2.0719, 1.3351, 1.1748,
         1.4728, 0.9592, 2.8685, 0.3589, 1.3840, 1.7617, 1.8002, 1.6476, 0.7462,
         1.4122, 1.2241, 0.7283, 1.6215, 1.0088, 0.9456, 0.9328, 0.4074, 1.0230,
         1.5890, 1.4031, 2.8058, 1.4238, 1.3941, 0.8235, 0.1247, 1.7674, 1.0994,
         1.1524, 2.7086, 0.5946, 0.9385, 0.5291, 1.3841, 0.0000, 0.8288, 1.3932,
         0.8461, 0.4083, 0.0000, 0.6051, 0.9022, 0.2660, 0.7763, 0.8939, 0.8369,
         0.5251, 0.9963, 0.4543, 1.3419, 1.2090, 1.2650, 1.0395, 1.4398, 1.5506,
         1.3142, 0.5984, 1.1577, 1.4199, 1.6742, 1.2063, 0.7223, 0.5716, 1.7603,
         1.9198, 1.4826, 1.1332, 1.0937, 1.5968, 1.4033, 1.0961, 1.0800, 1.1639,
         0.7203, 0.2366, 0.1891, 0.2923, 0.5913, 0.1660, 0.0280, 0.0000, 0.1514,
         0.9542],
        [1.3004, 1.3154, 0.9655, 1.5475, 1.5091, 1.0329, 1.2426, 0.8705, 1.4543,
         0.9751, 1.5469, 1.8778, 0.3579, 1.7282, 1.4081, 1.7951, 2.3391, 1.0553,
         1.9186, 1.4021, 1.3969, 1.9098, 1.7660, 1.3141, 1.4644, 2.0366, 1.1838,
         0.7462, 1.2339, 1.4918, 1.6761, 1.4859, 1.2209, 1.9855, 1.8347, 1.7697,
         2.5316, 1.5298, 0.9181, 1.7623, 0.4632, 0.6228, 1.1534, 0.4731, 1.2583,
         2.3278, 0.7546, 1.8953, 2.5552, 1.1187, 0.3865, 0.9225, 0.7722, 0.8563,
         0.9077, 1.2992, 1.4792, 2.0097, 2.0198, 1.9343, 2.0357, 1.2328, 2.0778,
         1.6719, 0.0000, 1.1758, 0.4791, 1.2354, 1.1401, 0.4959, 1.5172, 1.4199,
         1.9649, 1.1266, 0.2242, 0.9268, 0.7702, 0.2126, 0.1827, 0.5508, 0.8552,
         0.8213, 0.5510, 0.6697, 0.8983, 1.2608, 1.4085, 1.2288, 1.1479, 1.1794,
         1.2655, 1.2126, 1.4014, 1.5083, 1.5816, 1.0079, 0.8187, 1.2087, 1.6255,
         1.6233],
        [1.4919, 2.3152, 1.4761, 1.2597, 1.4867, 2.2032, 1.2138, 1.5579, 1.8944,
         2.3226, 2.4436, 2.0215, 2.2848, 1.5961, 0.7975, 1.0219, 0.8001, 0.5636,
         1.4480, 1.9803, 2.5970, 2.6939, 1.0095, 2.0724, 1.2309, 1.9943, 1.4547,
         1.5407, 1.3145, 1.4258, 1.5318, 1.8267, 2.0742, 2.3275, 1.7535, 1.9126,
         0.8810, 0.8034, 1.5606, 1.8516, 1.9999, 1.6602, 2.2945, 1.9398, 1.8663,
         0.4817, 0.8444, 0.9680, 0.9715, 1.7050, 0.0000, 0.6368, 0.0000, 1.3241,
         0.0827, 0.6329, 0.1649, 0.3975, 0.9072, 0.7527, 1.3907, 2.3357, 0.0444,
         0.8640, 0.7238, 2.3302, 1.2407, 0.6453, 1.1091, 1.3956, 1.5539, 0.9731,
         0.1872, 0.5142, 1.3680, 0.2855, 0.2879, 0.6521, 1.1106, 1.1109, 0.0000,
         0.0000, 0.0000, 0.2496, 0.3328, 0.0000, 0.0000, 0.0000, 0.0832, 0.1142,
         0.8010, 0.3872, 0.4936, 0.7629, 0.7470, 0.7154, 0.6481, 0.7440, 0.6146,
         0.9497],
        [1.7737, 0.6629, 0.5091, 1.6336, 1.3154, 2.3515, 1.7292, 0.9040, 0.6001,
         1.8531, 1.9100, 2.2020, 1.4067, 1.9411, 2.4996, 2.1798, 0.2468, 1.4674,
         2.0298, 0.4694, 1.8260, 1.2586, 1.7051, 2.9810, 1.3776, 0.9064, 1.4950,
         1.7682, 2.3967, 0.9657, 0.7827, 1.1545, 1.6653, 1.9496, 1.2086, 1.6487,
         0.2567, 1.5026, 1.7108, 2.3084, 1.2761, 1.3108, 0.4148, 3.0113, 1.3836,
         1.5192, 0.0705, 0.9569, 1.0505, 1.3661, 0.8563, 0.8108, 2.2763, 1.1517,
         1.5716, 0.8643, 1.8612, 1.5833, 2.0078, 0.0000, 0.4993, 0.8855, 1.3095,
         1.4721, 0.7486, 0.9697, 0.3250, 1.3633, 1.0509, 1.3364, 0.8494, 0.7256,
         0.6898, 0.9998, 2.2487, 1.6748, 1.4412, 1.1447, 1.3521, 0.9781, 0.9392,
         1.0552, 1.0331, 0.8851, 0.3678, 0.5376, 0.5210, 0.6638, 0.4825, 0.2376,
         0.5822, 0.5018, 0.6659, 0.9355, 0.9912, 0.5740, 0.2185, 0.2877, 0.7933,
         1.1440]], device='cuda:0', grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1855, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1354, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1354, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++real_logits+++++++++++++++++++++++++++++++++
tensor([[-6.2340e-01, -9.4831e-01, -6.4425e-01,  1.5122e-02,  2.3535e-01,
          6.5990e-01, -2.5231e-01, -4.0969e-01, -6.1548e-01, -2.1343e-01,
         -3.6124e-01, -4.6047e-01, -4.6130e-01, -9.4526e-01, -5.0106e-01,
          5.5418e-01, -2.6076e-01, -7.6670e-01, -1.2960e+00, -7.3275e-01,
         -6.4847e-01,  3.2154e-01,  4.9392e-02, -3.3289e-01, -8.1237e-01,
         -9.4629e-01, -8.1710e-01,  1.6907e-01, -3.3258e-01,  7.7281e-03,
         -7.6435e-01, -7.7274e-01, -5.4506e-01, -1.3879e+00,  8.6695e-02,
         -1.3031e+00, -7.1912e-01, -7.4063e-01, -9.4348e-01, -1.5982e-01,
          1.1821e-02, -1.2115e+00, -8.7836e-02,  1.3506e+00, -8.6311e-01,
         -1.4192e+00, -1.2660e+00, -5.2666e-02,  7.3791e-02, -2.0497e+00,
          9.4300e-01,  2.9720e-02,  4.5345e-01, -2.2071e-01,  2.1167e-01,
         -1.3247e-01, -4.1921e-01,  7.6047e-02,  6.2575e-01,  7.0445e-01,
          3.0614e-02,  5.9036e-01,  9.6348e-01,  6.6063e-01,  1.1694e+00,
          6.6084e-01,  2.1359e-01,  5.6274e-01,  3.1071e-01,  6.3248e-01,
         -1.0619e-01, -6.5520e-01, -3.8220e-01, -2.6650e-01,  4.7614e-01,
          5.6760e-01,  3.4625e-01,  6.2001e-01,  7.9797e-01,  1.0691e+00,
          7.4960e-01,  5.5501e-01,  4.7941e-01,  6.2512e-01,  9.3263e-01,
         -1.2184e-01,  1.2708e-01,  1.7524e-01,  5.1815e-01,  5.0047e-01,
         -1.0894e+00, -5.6256e-01, -6.0849e-02,  1.8088e-01,  6.8551e-01,
         -1.3383e+00, -5.8046e-01, -8.0313e-02,  1.7916e-01,  7.3429e-01],
        [-2.1941e-01, -1.5581e-01,  1.2081e-01,  1.0347e+00, -3.8497e-01,
         -1.7631e+00, -2.3969e+00, -1.4900e+00, -1.5509e+00, -1.5165e+00,
         -1.1555e+00, -6.5077e-02,  5.5552e-01, -5.2852e-01, -1.1288e+00,
         -1.7184e+00, -4.5337e-01, -4.0346e-01,  3.3396e-01, -3.3197e-02,
         -1.0929e+00,  7.5251e-01, -9.9544e-01, -6.1816e-01,  1.0318e-01,
          1.1732e-01,  6.6230e-01,  3.1156e-01,  5.1058e-01, -5.9427e-01,
         -6.5085e-01, -2.5631e-01, -1.5543e+00, -2.9336e-01, -4.9799e-01,
         -4.3606e-01, -4.0240e-01, -3.1652e-01, -6.4720e-02, -4.5929e-01,
         -7.8552e-01, -1.3195e-01, -5.4091e-01, -1.2119e+00, -6.0930e-01,
         -8.3111e-01,  1.0307e+00,  2.2570e-01,  1.1104e-01,  2.8758e-01,
          1.3534e+00,  6.7179e-01,  1.4547e-01, -1.9465e-01,  6.6077e-01,
          1.3476e+00,  1.2275e+00,  7.1853e-01,  4.2806e-01,  3.8154e-01,
          7.8762e-02, -8.5135e-01, -8.9271e-01,  9.7238e-01,  1.6734e+00,
         -3.0810e-01, -3.4517e-01,  6.8731e-02,  1.7158e-02, -4.3136e-01,
         -3.8838e-01,  9.0532e-03, -9.5873e-01, -5.5171e-01,  7.7918e-01,
         -1.1954e-01, -6.0231e-02,  2.8451e-01,  5.3606e-01,  5.5394e-01,
         -2.5828e-01, -1.7165e-01,  4.4830e-02,  5.2075e-01,  7.6896e-01,
         -5.2872e-01, -5.3055e-01, -3.7045e-01,  8.7105e-02,  2.4594e-01,
         -1.1211e+00, -7.9773e-01, -6.4393e-01, -4.3583e-01, -4.8809e-01,
         -1.2954e+00, -1.3480e+00, -1.2559e+00, -1.2588e+00, -1.2745e+00],
        [-1.2588e+00, -2.8396e-01,  7.0099e-01, -6.5049e-01, -2.3394e-01,
         -8.0292e-01, -1.6747e+00, -9.7939e-01,  5.6722e-01,  6.0171e-01,
         -1.5254e+00, -2.3982e+00, -1.6764e+00, -1.0529e+00, -7.3330e-01,
         -1.6038e+00, -1.7549e+00, -5.9857e-01, -4.5645e-01, -1.0203e+00,
         -9.8920e-01, -8.4677e-02, -2.9332e-01, -1.2571e+00, -3.7654e-01,
         -1.6537e-01, -6.5797e-01, -9.9770e-01, -8.4368e-01,  2.9839e-02,
         -1.3434e-01,  2.9673e-02,  5.4010e-01,  5.5424e-01,  4.7249e-01,
          5.7725e-01,  1.7562e-01,  2.2008e-01, -4.5961e-01,  4.6159e-01,
         -7.3080e-01,  6.9511e-01, -2.4883e-01, -1.7324e-01,  1.1544e-01,
         -1.5108e+00, -5.7708e-01, -6.2245e-01, -2.6006e-01,  8.5018e-01,
          4.2564e-01, -8.7054e-02, -2.0664e-01,  2.6275e-01,  4.0615e-01,
          2.1491e-01,  5.9053e-01,  3.5806e-01,  2.1861e-02,  6.5451e-01,
         -4.2509e-01, -3.8109e-01, -3.4372e-01, -1.0211e+00,  1.3592e-01,
          1.2521e+00,  7.8686e-01,  9.1614e-02, -4.1910e-01,  4.8493e-01,
          2.8083e-01,  5.8310e-01, -6.0562e-01, -3.1492e-01,  1.3499e-01,
         -8.1556e-01, -1.2173e+00, -1.0310e+00, -1.2204e+00, -1.2666e+00,
         -6.7874e-01, -7.1514e-01, -3.6167e-01, -3.9718e-01, -3.0914e-01,
         -7.3048e-01, -3.7552e-01,  3.2447e-01,  5.0795e-01,  3.0914e-01,
          4.8385e-02,  6.7015e-01,  1.0436e+00,  8.7003e-01,  4.7261e-01,
          3.8688e-01,  8.2184e-01,  1.1058e+00,  1.0966e+00,  9.9411e-01],
        [-7.2799e-01, -3.3997e-01, -2.2348e+00, -9.8890e-01, -1.1590e+00,
         -5.3108e-01, -1.5271e-01, -9.6146e-02,  2.1128e-01, -1.6502e-01,
         -3.2173e-01, -2.5784e-01, -1.7528e+00, -1.4655e+00,  1.5928e-01,
         -2.3214e-01, -4.9840e-02, -4.3109e-01, -2.6309e-01, -1.4291e+00,
         -2.3902e-01, -5.2010e-01, -1.3086e-01, -8.0048e-01, -9.4500e-01,
         -7.3354e-01, -5.6704e-01, -7.8504e-01, -2.5411e-01, -7.6012e-01,
         -1.2290e+00, -7.2722e-01,  4.6598e-01, -4.7271e-01, -4.3660e-01,
         -9.7432e-01, -3.3144e-01, -2.1573e-01, -3.5981e-01, -5.6624e-02,
         -9.8007e-01, -4.6057e-02,  2.1962e-01,  1.8687e-02,  6.5552e-01,
         -3.9950e-01, -1.5755e-01, -1.0011e+00, -2.6002e-01, -8.6484e-01,
          2.3290e+00,  6.3228e-01, -2.7668e-01,  6.8967e-01,  4.4729e-01,
          3.0021e-01, -3.9985e-01,  1.5881e-01, -4.2089e-01,  3.0270e-01,
          5.7344e-01,  7.2370e-01,  7.1943e-01,  5.2715e-01,  3.0473e-01,
         -1.2701e-01,  1.0877e-01, -9.6053e-02,  7.3097e-02, -2.2187e-01,
         -4.5925e-01, -1.3245e-01, -5.1499e-01, -1.6506e-01,  5.4302e-01,
          7.4700e-01,  8.8319e-01,  8.2138e-01,  6.5059e-01,  5.3406e-01,
          7.7332e-01,  7.1562e-01,  4.2252e-01,  2.5885e-01,  1.3851e-03,
          4.4385e-01, -3.0677e-02, -3.7607e-01, -4.5572e-01, -4.3122e-01,
         -2.6547e-01, -5.6709e-01, -6.6325e-01, -5.9581e-01, -2.6206e-01,
         -5.2651e-01, -4.5340e-01, -2.3988e-01, -1.7062e-01, -6.6257e-02],
        [-1.1477e+00, -5.7469e-01, -4.9800e-01,  3.3347e-01, -2.1700e-01,
         -6.2451e-01, -1.0772e-01,  1.9655e+00,  1.2393e-01,  2.1580e-01,
         -4.5050e-01,  1.0024e-01,  6.8143e-01, -3.1108e-01,  2.1270e-01,
         -3.4454e-01, -8.7427e-01, -5.9515e-01, -1.7006e+00, -1.9411e-01,
         -1.9579e+00, -1.0452e+00,  6.1909e-02, -6.8786e-01, -2.7740e-01,
          1.0169e-01,  5.6800e-02, -6.1215e-01, -1.6199e-01, -2.8324e-01,
         -4.0852e-01, -1.1186e-01, -1.1774e+00, -7.3049e-01, -2.0141e-01,
          3.1281e-01, -2.4404e-01, -8.3293e-02,  3.8258e-01,  2.1702e-01,
         -9.2538e-01,  2.9108e-01,  5.9279e-01, -2.2779e-01,  7.4188e-02,
         -9.7403e-01,  2.7408e-01, -7.1726e-01, -7.0950e-01, -4.5474e-01,
          1.1789e+00,  6.9350e-01,  5.8263e-01,  2.0799e-01, -5.4962e-02,
          6.4843e-01,  1.2289e-01, -5.0266e-01,  1.3367e-01,  5.7331e-02,
          6.7581e-01, -2.3849e-01, -9.8451e-01, -2.3917e-01, -6.2195e-02,
          4.6041e-01,  7.9710e-01, -5.0851e-01, -5.0521e-01, -3.6900e-01,
         -5.9304e-02,  1.5087e-01,  4.1238e-02, -4.6698e-01, -7.7235e-01,
          5.6448e-02, -8.4338e-02,  4.7954e-02, -6.9971e-02,  3.4294e-01,
         -1.0042e+00, -8.3746e-01, -5.6278e-01, -2.1563e-01,  5.4213e-02,
         -6.1302e-01, -8.3552e-01, -9.2205e-01, -5.7827e-01, -4.5547e-01,
          5.2065e-02, -1.1510e-02, -3.3882e-01, -3.9014e-01, -2.5928e-01,
          7.0722e-01,  5.6297e-01,  7.0093e-02, -5.7494e-02,  1.6909e-01],
        [ 1.1611e-01,  1.5736e-01, -8.5065e-01, -6.0629e-01,  9.9338e-02,
         -4.5547e-01, -9.3438e-01,  1.3897e-01,  6.1059e-02,  6.9245e-01,
         -8.3784e-01, -1.2108e+00, -9.6052e-01, -3.1783e-01,  5.7085e-02,
          1.5994e-01, -4.5996e-01, -1.5785e-02,  6.6584e-02, -6.1150e-01,
         -9.7824e-01, -1.1863e+00, -8.0471e-01, -7.4940e-01,  3.7146e-01,
         -9.5810e-01, -5.2122e-02,  1.3174e-02, -1.5287e+00, -6.1026e-01,
         -3.6549e-01, -1.0461e+00, -2.2033e-01, -5.6455e-01, -1.4389e+00,
         -1.0150e+00, -1.5198e+00, -4.8911e-01, -8.4554e-02, -1.3431e+00,
         -4.1890e-01, -7.3153e-01, -4.5392e-01, -4.6819e-01, -2.5729e-01,
          4.0058e-01,  6.7961e-01,  2.5541e-02, -1.0637e+00,  1.6698e-01,
         -2.1899e-01, -6.8910e-02, -1.4746e-01,  6.0507e-02, -2.5985e-01,
          7.0658e-01,  2.6670e-01,  4.1050e-01,  8.5732e-01,  2.5596e-01,
          2.4383e-01, -4.0588e-01, -1.1845e+00, -3.8642e-01,  4.0442e-01,
         -2.6611e-01, -5.4226e-01,  2.5924e-01, -1.0944e+00, -2.9986e-01,
         -3.3117e-01, -2.7676e-01, -8.7562e-03, -8.7578e-01, -1.5358e+00,
          2.8408e-01,  2.0443e-01,  4.5408e-01,  6.1965e-01,  6.6181e-01,
          9.0512e-02,  2.2897e-01,  7.3929e-01,  6.0498e-01,  2.2250e-01,
         -2.9282e-03, -1.4675e-01,  2.3210e-01,  1.3784e-01, -3.0559e-01,
          1.7250e-01, -2.7135e-01,  2.3727e-01,  1.7454e-01, -2.1977e-01,
         -1.2184e-01, -6.6427e-01, -2.3222e-01,  2.0866e-01,  1.3324e-02],
        [-5.5089e-01, -3.0869e-03, -5.7551e-01,  9.6388e-02, -4.8194e-01,
         -1.1410e+00,  3.7152e-01, -7.8667e-01, -3.1300e-01, -5.9692e-01,
         -1.3965e+00, -1.1772e-02, -5.1511e-01, -7.7209e-01, -6.4879e-01,
         -1.1179e+00, -9.3720e-01, -4.3852e-01, -7.6524e-01, -7.7295e-01,
         -1.9074e+00, -5.3032e-01,  1.2311e+00,  1.9577e-01,  2.6544e-01,
         -7.4252e-01,  3.5642e-02, -3.3810e-02, -5.6237e-01, -6.5274e-02,
         -3.3542e-01,  8.2622e-03, -4.6386e-01, -7.8648e-01, -3.8778e-01,
         -5.6987e-01,  9.0736e-02, -4.6974e-01,  1.0644e+00,  5.6465e-01,
         -8.8780e-01, -5.5699e-01, -1.4696e-01, -1.2104e-01, -6.5634e-01,
         -6.4318e-01,  4.9044e-02, -9.8158e-01, -7.1736e-01,  1.1562e-01,
          1.3137e+00,  1.3200e+00,  7.0150e-01,  9.7898e-02,  2.3541e-01,
          1.1428e+00,  9.8398e-01,  5.0846e-01,  3.3695e-01,  3.0296e-01,
          1.5136e+00, -4.4552e-01,  2.7745e-01,  7.9490e-02, -8.2121e-02,
          6.5795e-01,  3.2993e-01,  4.8919e-01,  5.7439e-02, -4.3763e-01,
          8.5200e-02, -2.1322e-01,  5.1959e-01, -3.5347e-01,  5.6171e-01,
          3.8944e-01,  1.2762e-01, -6.0629e-02,  7.3679e-02,  1.8266e-01,
          3.6968e-01,  2.3355e-01, -1.5162e-02, -1.7297e-01, -3.9211e-01,
         -1.1433e-01,  4.0424e-01,  2.9101e-01, -2.1676e-01, -6.7621e-01,
         -5.2503e-01,  2.0742e-01,  4.7364e-01,  3.2224e-01, -3.1516e-01,
         -9.8885e-01, -1.5497e-01,  8.2239e-01,  8.9014e-01,  9.3647e-01],
        [-1.0175e+00, -2.5455e-02,  2.0276e-01, -3.6536e-02, -1.9830e-01,
         -8.6733e-01, -3.9614e-01, -3.5952e-01, -2.9388e-01,  1.2238e-01,
         -6.5844e-01,  4.1294e-02,  7.3197e-02, -1.0735e+00, -1.1027e+00,
         -1.1897e+00, -1.7397e+00,  9.4003e-02, -1.2565e-02, -4.2533e-01,
         -2.3496e+00, -8.6203e-01, -1.3750e+00, -5.7587e-01, -5.3719e-01,
         -9.5655e-01, -7.2396e-01, -7.6134e-01, -1.0751e+00, -9.4405e-01,
         -6.1001e-01,  9.6067e-02, -5.2931e-01, -9.4452e-01, -1.1355e+00,
         -9.4049e-02,  4.2366e-01, -6.7540e-01, -3.0124e-01, -6.1612e-01,
         -1.4162e+00, -4.8459e-01, -1.5395e-01, -4.5905e-01, -8.8453e-01,
         -1.4041e-01,  3.4997e-01, -7.9541e-01, -5.1690e-01, -5.1893e-01,
          5.2424e-01,  8.0510e-01,  9.9340e-01, -2.4600e-01,  3.6853e-01,
          1.2568e+00,  4.1468e-01,  8.9091e-01,  1.2285e+00,  1.6731e+00,
          6.6740e-01, -5.3486e-02,  1.0842e-01, -2.3278e-01,  1.2014e+00,
          1.2358e-01, -1.0198e+00, -7.5464e-01, -3.8368e-03,  3.0398e-01,
          8.5683e-02, -1.9298e-01, -4.1744e-01, -8.2793e-01, -1.6689e-01,
         -8.2302e-02,  1.6962e-01,  2.1715e-01,  4.4684e-01,  4.2540e-01,
         -1.0881e-01,  6.5453e-02,  2.0337e-01,  5.9600e-01,  4.2173e-01,
         -1.8888e-01, -4.6160e-02,  6.7825e-02,  6.4771e-01,  7.6076e-01,
         -2.0548e-01, -6.3409e-01, -3.4786e-01,  6.3182e-01,  1.1923e+00,
         -3.1329e-01, -8.9453e-01, -8.8075e-01, -2.1557e-01,  6.1823e-01]],
       device='cuda:0', grad_fn=<CatBackward0>)
++++++++++++++++++++++loss1+++++++++++++++++++++++++++++++
tensor([[1.6234, 1.9483, 1.6442, 0.9849, 0.7647, 0.3401, 1.2523, 1.4097, 1.6155,
         1.2134, 1.3612, 1.4605, 1.4613, 1.9453, 1.5011, 0.4458, 1.2608, 1.7667,
         2.2960, 1.7328, 1.6485, 0.6785, 0.9506, 1.3329, 1.8124, 1.9463, 1.8171,
         0.8309, 1.3326, 0.9923, 1.7644, 1.7727, 1.5451, 2.3879, 0.9133, 2.3031,
         1.7191, 1.7406, 1.9435, 1.1598, 0.9882, 2.2115, 1.0878, 0.0000, 1.8631,
         2.4192, 2.2660, 1.0527, 0.9262, 3.0497, 0.0570, 0.9703, 0.5466, 1.2207,
         0.7883, 1.1325, 1.4192, 0.9240, 0.3743, 0.2955, 0.9694, 0.4096, 0.0365,
         0.3394, 0.0000, 0.3392, 0.7864, 0.4373, 0.6893, 0.3675, 1.1062, 1.6552,
         1.3822, 1.2665, 0.5239, 0.4324, 0.6538, 0.3800, 0.2020, 0.0000, 0.2504,
         0.4450, 0.5206, 0.3749, 0.0674, 1.1218, 0.8729, 0.8248, 0.4819, 0.4995,
         2.0894, 1.5626, 1.0608, 0.8191, 0.3145, 2.3383, 1.5805, 1.0803, 0.8208,
         0.2657],
        [1.2194, 1.1558, 0.8792, 0.0000, 1.3850, 2.7631, 3.3969, 2.4900, 2.5509,
         2.5165, 2.1555, 1.0651, 0.4445, 1.5285, 2.1288, 2.7184, 1.4534, 1.4035,
         0.6660, 1.0332, 2.0929, 0.2475, 1.9954, 1.6182, 0.8968, 0.8827, 0.3377,
         0.6884, 0.4894, 1.5943, 1.6509, 1.2563, 2.5543, 1.2934, 1.4980, 1.4361,
         1.4024, 1.3165, 1.0647, 1.4593, 1.7855, 1.1320, 1.5409, 2.2119, 1.6093,
         1.8311, 0.0000, 0.7743, 0.8890, 0.7124, 0.0000, 0.3282, 0.8545, 1.1947,
         0.3392, 0.0000, 0.0000, 0.2815, 0.5719, 0.6185, 0.9212, 1.8514, 1.8927,
         0.0276, 0.0000, 1.3081, 1.3452, 0.9313, 0.9828, 1.4314, 1.3884, 0.9909,
         1.9587, 1.5517, 0.2208, 1.1195, 1.0602, 0.7155, 0.4639, 0.4461, 1.2583,
         1.1716, 0.9552, 0.4792, 0.2310, 1.5287, 1.5305, 1.3704, 0.9129, 0.7541,
         2.1211, 1.7977, 1.6439, 1.4358, 1.4881, 2.2954, 2.3480, 2.2559, 2.2588,
         2.2745],
        [2.2588, 1.2840, 0.2990, 1.6505, 1.2339, 1.8029, 2.6747, 1.9794, 0.4328,
         0.3983, 2.5254, 3.3982, 2.6764, 2.0529, 1.7333, 2.6038, 2.7549, 1.5986,
         1.4565, 2.0203, 1.9892, 1.0847, 1.2933, 2.2571, 1.3765, 1.1654, 1.6580,
         1.9977, 1.8437, 0.9702, 1.1343, 0.9703, 0.4599, 0.4458, 0.5275, 0.4227,
         0.8244, 0.7799, 1.4596, 0.5384, 1.7308, 0.3049, 1.2488, 1.1732, 0.8846,
         2.5108, 1.5771, 1.6224, 1.2601, 0.1498, 0.5744, 1.0871, 1.2066, 0.7373,
         0.5938, 0.7851, 0.4095, 0.6419, 0.9781, 0.3455, 1.4251, 1.3811, 1.3437,
         2.0211, 0.8641, 0.0000, 0.2131, 0.9084, 1.4191, 0.5151, 0.7192, 0.4169,
         1.6056, 1.3149, 0.8650, 1.8156, 2.2173, 2.0310, 2.2204, 2.2666, 1.6787,
         1.7151, 1.3617, 1.3972, 1.3091, 1.7305, 1.3755, 0.6755, 0.4921, 0.6909,
         0.9516, 0.3299, 0.0000, 0.1300, 0.5274, 0.6131, 0.1782, 0.0000, 0.0000,
         0.0059],
        [1.7280, 1.3400, 3.2348, 1.9889, 2.1590, 1.5311, 1.1527, 1.0961, 0.7887,
         1.1650, 1.3217, 1.2578, 2.7528, 2.4655, 0.8407, 1.2321, 1.0498, 1.4311,
         1.2631, 2.4291, 1.2390, 1.5201, 1.1309, 1.8005, 1.9450, 1.7335, 1.5670,
         1.7850, 1.2541, 1.7601, 2.2290, 1.7272, 0.5340, 1.4727, 1.4366, 1.9743,
         1.3314, 1.2157, 1.3598, 1.0566, 1.9801, 1.0461, 0.7804, 0.9813, 0.3445,
         1.3995, 1.1576, 2.0011, 1.2600, 1.8648, 0.0000, 0.3677, 1.2767, 0.3103,
         0.5527, 0.6998, 1.3998, 0.8412, 1.4209, 0.6973, 0.4266, 0.2763, 0.2806,
         0.4728, 0.6953, 1.1270, 0.8912, 1.0961, 0.9269, 1.2219, 1.4592, 1.1325,
         1.5150, 1.1651, 0.4570, 0.2530, 0.1168, 0.1786, 0.3494, 0.4659, 0.2267,
         0.2844, 0.5775, 0.7411, 0.9986, 0.5561, 1.0307, 1.3761, 1.4557, 1.4312,
         1.2655, 1.5671, 1.6632, 1.5958, 1.2621, 1.5265, 1.4534, 1.2399, 1.1706,
         1.0663],
        [2.1477, 1.5747, 1.4980, 0.6665, 1.2170, 1.6245, 1.1077, 0.0000, 0.8761,
         0.7842, 1.4505, 0.8998, 0.3186, 1.3111, 0.7873, 1.3445, 1.8743, 1.5951,
         2.7006, 1.1941, 2.9579, 2.0452, 0.9381, 1.6879, 1.2774, 0.8983, 0.9432,
         1.6121, 1.1620, 1.2832, 1.4085, 1.1119, 2.1774, 1.7305, 1.2014, 0.6872,
         1.2440, 1.0833, 0.6174, 0.7830, 1.9254, 0.7089, 0.4072, 1.2278, 0.9258,
         1.9740, 0.7259, 1.7173, 1.7095, 1.4547, 0.0000, 0.3065, 0.4174, 0.7920,
         1.0550, 0.3516, 0.8771, 1.5027, 0.8663, 0.9427, 0.3242, 1.2385, 1.9845,
         1.2392, 1.0622, 0.5396, 0.2029, 1.5085, 1.5052, 1.3690, 1.0593, 0.8491,
         0.9588, 1.4670, 1.7724, 0.9436, 1.0843, 0.9520, 1.0700, 0.6571, 2.0042,
         1.8375, 1.5628, 1.2156, 0.9458, 1.6130, 1.8355, 1.9221, 1.5783, 1.4555,
         0.9479, 1.0115, 1.3388, 1.3901, 1.2593, 0.2928, 0.4370, 0.9299, 1.0575,
         0.8309],
        [0.8839, 0.8426, 1.8507, 1.6063, 0.9007, 1.4555, 1.9344, 0.8610, 0.9389,
         0.3076, 1.8378, 2.2108, 1.9605, 1.3178, 0.9429, 0.8401, 1.4600, 1.0158,
         0.9334, 1.6115, 1.9782, 2.1863, 1.8047, 1.7494, 0.6285, 1.9581, 1.0521,
         0.9868, 2.5287, 1.6103, 1.3655, 2.0461, 1.2203, 1.5645, 2.4389, 2.0150,
         2.5198, 1.4891, 1.0846, 2.3431, 1.4189, 1.7315, 1.4539, 1.4682, 1.2573,
         0.5994, 0.3204, 0.9745, 2.0637, 0.8330, 1.2190, 1.0689, 1.1475, 0.9395,
         1.2599, 0.2934, 0.7333, 0.5895, 0.1427, 0.7440, 0.7562, 1.4059, 2.1845,
         1.3864, 0.5956, 1.2661, 1.5423, 0.7408, 2.0944, 1.2999, 1.3312, 1.2768,
         1.0088, 1.8758, 2.5358, 0.7159, 0.7956, 0.5459, 0.3803, 0.3382, 0.9095,
         0.7710, 0.2607, 0.3950, 0.7775, 1.0029, 1.1467, 0.7679, 0.8622, 1.3056,
         0.8275, 1.2714, 0.7627, 0.8255, 1.2198, 1.1218, 1.6643, 1.2322, 0.7913,
         0.9867],
        [1.5509, 1.0031, 1.5755, 0.9036, 1.4819, 2.1410, 0.6285, 1.7867, 1.3130,
         1.5969, 2.3965, 1.0118, 1.5151, 1.7721, 1.6488, 2.1179, 1.9372, 1.4385,
         1.7652, 1.7730, 2.9074, 1.5303, 0.0000, 0.8042, 0.7346, 1.7425, 0.9644,
         1.0338, 1.5624, 1.0653, 1.3354, 0.9917, 1.4639, 1.7865, 1.3878, 1.5699,
         0.9093, 1.4697, 0.0000, 0.4354, 1.8878, 1.5570, 1.1470, 1.1210, 1.6563,
         1.6432, 0.9510, 1.9816, 1.7174, 0.8844, 0.0000, 0.0000, 0.2985, 0.9021,
         0.7646, 0.0000, 0.0160, 0.4915, 0.6631, 0.6970, 0.0000, 1.4455, 0.7226,
         0.9205, 1.0821, 0.3420, 0.6701, 0.5108, 0.9426, 1.4376, 0.9148, 1.2132,
         0.4804, 1.3535, 0.4383, 0.6106, 0.8724, 1.0606, 0.9263, 0.8173, 0.6303,
         0.7665, 1.0152, 1.1730, 1.3921, 1.1143, 0.5958, 0.7090, 1.2168, 1.6762,
         1.5250, 0.7926, 0.5264, 0.6778, 1.3152, 1.9889, 1.1550, 0.1776, 0.1099,
         0.0635],
        [2.0175, 1.0255, 0.7972, 1.0365, 1.1983, 1.8673, 1.3961, 1.3595, 1.2939,
         0.8776, 1.6584, 0.9587, 0.9268, 2.0735, 2.1027, 2.1897, 2.7397, 0.9060,
         1.0126, 1.4253, 3.3496, 1.8620, 2.3750, 1.5759, 1.5372, 1.9566, 1.7240,
         1.7613, 2.0751, 1.9441, 1.6100, 0.9039, 1.5293, 1.9445, 2.1355, 1.0940,
         0.5763, 1.6754, 1.3012, 1.6161, 2.4162, 1.4846, 1.1539, 1.4591, 1.8845,
         1.1404, 0.6500, 1.7954, 1.5169, 1.5189, 0.4758, 0.1949, 0.0066, 1.2460,
         0.6315, 0.0000, 0.5853, 0.1091, 0.0000, 0.0000, 0.3326, 1.0535, 0.8916,
         1.2328, 0.0000, 0.8764, 2.0198, 1.7546, 1.0038, 0.6960, 0.9143, 1.1930,
         1.4174, 1.8279, 1.1669, 1.0823, 0.8304, 0.7829, 0.5532, 0.5746, 1.1088,
         0.9345, 0.7966, 0.4040, 0.5783, 1.1889, 1.0462, 0.9322, 0.3523, 0.2392,
         1.2055, 1.6341, 1.3479, 0.3682, 0.0000, 1.3133, 1.8945, 1.8807, 1.2156,
         0.3818]], device='cuda:0', grad_fn=<ReluBackward0>)
++++++++++++++++++++++loss1++++++++++++++++++++++++++++++
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1875, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Dstep()---------------------5
self.params============================ []
grad_samples(self) -> List[torch.Tensor]:
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 461, in training_loop
    opt_1.step()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 521, in step
    if self.pre_step():
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 502, in pre_step
    self.clip_and_accumulate()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 403, in clip_and_accumulate
    if len(self.grad_samples[0]) == 0:
IndexError: list index out of range
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:146: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1438, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1125, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1125, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2200, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1087, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1087, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2466, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1034, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1034, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2030, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.0559, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.0559, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.2116, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.0807, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.0807, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1252, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.0547, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.0547, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1855, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1355, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1355, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#loss:accumulate_gradients in Dmain
++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++++
tensor(1.1875, device='cuda:0', grad_fn=<MeanBackward0>)
++++++++++++++++++++++loss_Dreal+++++++++++++++++++++++++++++++
module.training== True
module.training== True
module.training== True
module.training== True
loss:endDmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Dstep()---------------------5
self.params============================ []
grad_samples(self) -> List[torch.Tensor]:
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 461, in training_loop
    opt_1.step()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 521, in step
    if self.pre_step():
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 502, in pre_step
    self.clip_and_accumulate()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/optimizers/optimizer.py", line 403, in clip_and_accumulate
    if len(self.grad_samples[0]) == 0:
IndexError: list index out of range
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:146: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 74, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:146: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1225, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 442, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 74, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
Loading training set...

Num images:  8638
Image shape: [3, 256, 256]
Label shape: [0]

Constructing networks...

Generator              Parameters  Buffers  Output shape        Datatype
---                    ---         ---      ---                 ---     
mapping                -           -        [8, 1, 256]         float32 
synthesis.init.init    16785408    8192     [8, 2048, 4, 4]     float32 
synthesis.feat_8.0     -           -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.1     37748736    20480    [8, 2048, 8, 8]     float32 
synthesis.feat_8.2     4096        -        [8, 2048, 8, 8]     float32 
synthesis.feat_8.3     -           -        [8, 1024, 8, 8]     float32 
synthesis.feat_16.0    -           -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.1    9437184     10240    [8, 1024, 16, 16]   float32 
synthesis.feat_16.2    2048        -        [8, 1024, 16, 16]   float32 
synthesis.feat_16.3    -           -        [8, 512, 16, 16]    float32 
synthesis.feat_32.0    -           -        [8, 512, 32, 32]    float32 
synthesis.feat_32.1    2359296     5120     [8, 512, 32, 32]    float32 
synthesis.feat_32.2    1024        -        [8, 512, 32, 32]    float32 
synthesis.feat_32.3    -           -        [8, 256, 32, 32]    float32 
synthesis.feat_64.0    -           -        [8, 256, 64, 64]    float32 
synthesis.feat_64.1    1179648     2816     [8, 512, 64, 64]    float32 
synthesis.feat_64.2    1024        -        [8, 512, 64, 64]    float32 
synthesis.feat_64.3    -           -        [8, 256, 64, 64]    float32 
synthesis.se_64.main   8454144     33536    [8, 256, 1, 1]      float32 
synthesis.se_64        -           -        [8, 256, 64, 64]    float32 
synthesis.feat_128.0   -           -        [8, 256, 128, 128]  float32 
synthesis.feat_128.1   589824      2560     [8, 256, 128, 128]  float32 
synthesis.feat_128.2   512         -        [8, 256, 128, 128]  float32 
synthesis.feat_128.3   -           -        [8, 128, 128, 128]  float32 
synthesis.se_128.main  2113536     16768    [8, 128, 1, 1]      float32 
synthesis.se_128       -           -        [8, 128, 128, 128]  float32 
synthesis.feat_256.0   -           -        [8, 128, 256, 256]  float32 
synthesis.feat_256.1   147456      1280     [8, 128, 256, 256]  float32 
synthesis.feat_256.2   256         -        [8, 128, 256, 256]  float32 
synthesis.feat_256.3   -           -        [8, 64, 256, 256]   float32 
synthesis.se_256.main  528384      8384     [8, 64, 1, 1]       float32 
synthesis.se_256       -           -        [8, 64, 256, 256]   float32 
synthesis.to_big       1731        579      [8, 3, 256, 256]    float32 
---                    ---         ---      ---                 ---     
Total                  79354307    109955   -                   -       

/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

ProjectedDiscriminator                                               Parameters  Buffers  Output shape       Datatype
---                                                                  ---         ---      ---                ---     
feature_network.pretrained.layer0.0                                  864         -        [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.1                                  64          65       [8, 32, 128, 128]  float32 
feature_network.pretrained.layer0.3                                  896         98       [8, 16, 128, 128]  float32 
feature_network.pretrained.layer0.4                                  13968       1062     [8, 24, 64, 64]    float32 
feature_network.pretrained.layer1.0                                  39712       1702     [8, 40, 32, 32]    float32 
feature_network.pretrained.layer2.0                                  198480      5289     [8, 80, 16, 16]    float32 
feature_network.pretrained.layer2.1                                  446784      7977     [8, 112, 16, 16]   float32 
feature_network.pretrained.layer3.0                                  1652640     18060    [8, 192, 8, 8]     float32 
feature_network.pretrained.layer3.1                                  605440      5251     [8, 320, 8, 8]     float32 
feature_network.scratch.layer0_ccm                                   1600        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer1_ccm                                   5248        -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer2_ccm                                   28928       -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer3_ccm                                   164352      -        [8, 512, 8, 8]     float32 
feature_network.scratch.layer3_csm.out_conv                          131328      -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.skip_add.activation_post_process  -           -        [8, 256, 16, 16]   float32 
feature_network.scratch.layer2_csm.out_conv                          32896       -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.skip_add.activation_post_process  -           -        [8, 128, 32, 32]   float32 
feature_network.scratch.layer1_csm.out_conv                          8256        -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.skip_add.activation_post_process  -           -        [8, 64, 64, 64]    float32 
feature_network.scratch.layer0_csm.out_conv                          4160        -        [8, 64, 128, 128]  float32 
discriminator.mini_discs.0.main                                      2829120     17345    [8, 1, 5, 5]       float32 
discriminator.mini_discs.1.main                                      2763392     16257    [8, 1, 5, 5]       float32 
discriminator.mini_discs.2.main                                      2631936     15105    [8, 1, 5, 5]       float32 
discriminator.mini_discs.3.main                                      2106880     12801    [8, 1, 5, 5]       float32 
discriminator                                                        -           -        [8, 100]           float32 
---                                                                  ---         ---      ---                ---     
Total                                                                13666944    101012   -                  -       

Distributing across 1 GPUs...
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py:146: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
提前注册opt
Setting up training phases...
batch 外部
privacy->D1
batch 外部
privacy->D2
batch 外部
privacy->D3
batch 外部
privacy->D4
type==2 <class 'opacus.data_loader.DPDataLoader'>
<class 'opacus.optimizers.optimizer.DPOptimizer'> opt_0的type（）
Exporting sample images...
Initializing logs...
Training for 600 kimg...

phase_real_img tensor([[[[ 57,  57,  57,  ...,   3,   5,   6],
          [ 61,  60,  61,  ...,   3,   5,   6],
          [ 64,  64,  64,  ...,   4,   5,   6],
          ...,
          [ 57,  59,  69,  ...,  37,  40,  37],
          [ 60,  66,  69,  ...,  31,  31,  35],
          [ 60,  59,  66,  ...,  30,  34,  28]],

         [[120, 120, 120,  ...,  64,  66,  67],
          [122, 121, 122,  ...,  64,  66,  67],
          [123, 123, 123,  ...,  65,  66,  67],
          ...,
          [ 79,  79,  88,  ...,  58,  61,  58],
          [ 82,  86,  87,  ...,  51,  51,  55],
          [ 81,  78,  85,  ...,  50,  54,  48]],

         [[129, 129, 129,  ...,  65,  67,  68],
          [132, 131, 131,  ...,  65,  67,  68],
          [133, 133, 133,  ...,  66,  67,  68],
          ...,
          [  1,   1,   3,  ...,   2,   5,   2],
          [  5,   5,   5,  ...,   0,   1,   2],
          [  6,   3,   4,  ...,   3,   5,   1]]],


        [[[178, 181, 184,  ..., 171, 169, 167],
          [182, 184, 186,  ..., 168, 166, 165],
          [184, 184, 189,  ..., 164, 163, 163],
          ...,
          [ 56,  59,  61,  ...,  76,  78,  85],
          [ 55,  59,  60,  ..., 100,  94,  83],
          [ 55,  59,  60,  ...,  76,  74,  74]],

         [[171, 173, 175,  ..., 164, 164, 165],
          [175, 177, 178,  ..., 162, 161, 163],
          [177, 177, 181,  ..., 159, 158, 161],
          ...,
          [ 58,  60,  63,  ...,  99, 102, 100],
          [ 57,  59,  61,  ..., 124, 116,  97],
          [ 57,  59,  61,  ..., 101,  96,  88]],

         [[226, 226, 228,  ..., 240, 241, 245],
          [228, 228, 229,  ..., 241, 242, 245],
          [229, 228, 232,  ..., 244, 244, 245],
          ...,
          [ 53,  56,  58,  ...,   5,   4,   8],
          [ 52,  55,  56,  ...,  17,  10,   6],
          [ 52,  55,  56,  ...,   5,   1,   7]]],


        [[[243, 243, 243,  ..., 190, 190, 189],
          [245, 245, 245,  ..., 190, 189, 187],
          [247, 248, 247,  ..., 190, 189, 189],
          ...,
          [136, 145, 178,  ...,  72,  72,  74],
          [124, 152, 137,  ...,  66,  73,  75],
          [136, 126,  95,  ...,  67,  78,  85]],

         [[232, 232, 232,  ..., 174, 174, 173],
          [234, 234, 234,  ..., 174, 173, 171],
          [236, 237, 236,  ..., 174, 173, 173],
          ...,
          [ 88,  95, 127,  ...,  35,  31,  32],
          [ 76, 104,  90,  ...,  28,  32,  33],
          [ 93,  85,  57,  ...,  30,  37,  44]],

         [[228, 228, 228,  ..., 184, 184, 183],
          [230, 230, 230,  ..., 184, 183, 181],
          [232, 233, 232,  ..., 184, 183, 183],
          ...,
          [ 60,  74, 107,  ...,  10,   5,   8],
          [ 45,  80,  70,  ...,  10,  13,  10],
          [ 58,  58,  35,  ...,  10,  16,  15]]],


        ...,


        [[[103, 101, 101,  ...,  59,  58,  57],
          [101, 102, 102,  ...,  58,  57,  56],
          [101, 103, 103,  ...,  57,  56,  55],
          ...,
          [228, 226, 228,  ..., 216, 229, 211],
          [226, 230, 226,  ..., 222, 224, 223],
          [226, 225, 219,  ..., 217, 232, 230]],

         [[125, 123, 123,  ...,  85,  84,  83],
          [124, 124, 125,  ...,  85,  84,  83],
          [125, 126, 126,  ...,  85,  84,  83],
          ...,
          [228, 226, 228,  ..., 216, 227, 207],
          [226, 230, 226,  ..., 222, 222, 218],
          [226, 225, 219,  ..., 217, 230, 225]],

         [[164, 161, 162,  ..., 127, 126, 125],
          [164, 165, 165,  ..., 128, 127, 127],
          [165, 167, 168,  ..., 129, 128, 127],
          ...,
          [230, 228, 230,  ..., 216, 232, 216],
          [228, 232, 228,  ..., 222, 226, 227],
          [228, 227, 221,  ..., 216, 235, 234]]],


        [[[119, 152, 142,  ...,  53,  32,  33],
          [ 95,  93, 133,  ...,  59,  49,  32],
          [153,  93, 103,  ...,  52,  47,  30],
          ...,
          [163, 122, 125,  ...,  53,  69,  75],
          [132, 180, 138,  ...,  32,  43,  48],
          [120, 138, 160,  ...,  19,  21,  24]],

         [[139, 173, 163,  ...,  53,  29,  28],
          [120, 115, 151,  ...,  62,  52,  33],
          [179, 114, 118,  ...,  61,  54,  37],
          ...,
          [181, 140, 143,  ...,  53,  68,  74],
          [150, 199, 157,  ...,  32,  43,  48],
          [138, 156, 179,  ...,  19,  21,  24]],

         [[ 91, 129, 125,  ...,  48,  25,  24],
          [ 79,  80, 124,  ...,  56,  47,  30],
          [142,  85,  97,  ...,  53,  47,  32],
          ...,
          [203, 162, 165,  ...,  57,  73,  79],
          [166, 215, 174,  ...,  34,  45,  50],
          [149, 167, 190,  ...,  21,  23,  26]]],


        [[[160, 193, 205,  ..., 178, 235, 243],
          [178, 170, 222,  ..., 220, 216, 228],
          [188, 169, 188,  ..., 246, 184, 233],
          ...,
          [151, 146,  98,  ...,  72,  57,  48],
          [151, 152, 138,  ...,  82,  90,  77],
          [152, 149, 145,  ...,  67,  70,  86]],

         [[144, 181, 201,  ..., 189, 250, 254],
          [173, 168, 223,  ..., 229, 229, 238],
          [197, 180, 198,  ..., 255, 196, 241],
          ...,
          [178, 176, 133,  ...,  77,  60,  51],
          [174, 179, 170,  ...,  86,  91,  79],
          [174, 173, 175,  ...,  69,  69,  86]],

         [[132, 171, 182,  ..., 207, 245, 251],
          [175, 165, 207,  ..., 240, 237, 250],
          [195, 167, 173,  ..., 255, 207, 249],
          ...,
          [188, 171, 105,  ...,  73,  52,  40],
          [197, 187, 155,  ...,  92,  94,  78],
          [201, 186, 167,  ...,  85,  84,  97]]]], dtype=torch.uint8)
# Accumulate gradients.-------1
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Gboth
/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 2 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 3 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 4 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 5 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 6 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 7 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
time : loss.accumulate_gradients 8 ----------2.0
#loss:accumulate_gradients in Gboth
module.training== True
module.training== True
module.training== True
module.training== True
loss:endGmain---------2.5
#phase.module.requires_grad_(False)
# Update weights.------------------------3
#Gstep()----------------------5
# Accumulate gradients.-------1
zero_grad(set_to_none=True)
time : loss.accumulate_gradients 1 ----------2.0
#loss:accumulate_gradients in Dmain
loss_Dgen==1 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
loss_Dgen==2 tensor(1.1230, device='cuda:0', grad_fn=<MeanBackward0>)
module.training== True
Traceback (most recent call last):
  File "train.py", line 267, in <module>
    main() # pylint: disable=no-value-for-parameter
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1128, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1053, in main
    rv = self.invoke(ctx)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/click/core.py", line 754, in invoke
    return __callback(*args, **kwargs)
  File "train.py", line 253, in main
    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)
  File "train.py", line 101, in launch_training
    subprocess_fn(rank=0, c=c, temp_dir=temp_dir)
  File "train.py", line 47, in subprocess_fn
    training_loop.training_loop(rank=rank, **c)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/training_loop.py", line 443, in training_loop
    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, gain=phase.interval, cur_nimg=cur_nimg)
  File "/mnt/LJH/wd/projected-fixed/projected_gan-main/training/loss.py", line 91, in accumulate_gradients
    loss_Dgen.backward()
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py", line 62, in __call__
    return self.hook(module, *args, **kwargs)
  File "/mnt/LJH/wd/.conda/envs/pg/lib/python3.8/site-packages/opacus/privacy_engine.py", line 74, in forbid_accumulation_hook
    if p.grad_sample is not None:
AttributeError: 'Parameter' object has no attribute 'grad_sample'
